import logging
import re
import time
from typing import List

from .env import EXA_API_KEY, TAVILY_API_KEY
from .utils import _normalize_published_date

logger = logging.getLogger(__name__)

# Search dependencies (optional)
try:
    from ddgs import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    try:
        from duckduckgo_search import DDGS
        DDGS_AVAILABLE = True
        logger.info("[Warning] Prefer 'pip install ddgs' over 'duckduckgo_search'")
    except ImportError:
        DDGS = None
        DDGS_AVAILABLE = False
        logger.info("[Warning] DuckDuckGo search unavailable: install ddgs or duckduckgo_search")

try:
    from tavily import TavilyClient
    TAVILY_AVAILABLE = True
except ImportError:
    TavilyClient = None
    TAVILY_AVAILABLE = False
    logger.info("[Warning] Tavily search unavailable: install tavily-python")

try:
    from exa_py import Exa
    EXA_AVAILABLE = True
except ImportError:
    Exa = None
    EXA_AVAILABLE = False
    logger.info("[Warning] Exa search unavailable: install exa-py")

try:
    import wikipedia
    wikipedia.set_lang("zh")
    WIKIPEDIA_AVAILABLE = True
except ImportError:
    wikipedia = None
    WIKIPEDIA_AVAILABLE = False
    logger.info("[Warning] Wikipedia search unavailable: install wikipedia")

def search(query: str) -> str:
    """
    ä½¿ç”¨å¤šæ•°æ®æºç­–ç•¥æ‰§è¡Œç½‘é¡µæœç´¢å¹¶åˆå¹¶ç»“æœã€‚
    ç­–ç•¥Aï¼šä¸²è¡Œæœç´¢ + æ™ºèƒ½æ£€æµ‹
    ä¼˜å…ˆçº§ï¼šExa > Tavily > Wikipedia > DuckDuckGo

    Args:
        query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²

    Returns:
        æ ¼å¼åŒ–çš„åˆå¹¶æœç´¢ç»“æœ
    """
    all_results = []
    sources_used = []

    # 0. å°è¯• Exa Search (è¯­ä¹‰æœç´¢ï¼Œä¼˜å…ˆçº§æœ€é«˜)
    if EXA_API_KEY and EXA_AVAILABLE:
        try:
            exa_result = _search_with_exa(query)
            if exa_result and len(exa_result) > 200:  # ç¡®ä¿ç»“æœè¶³å¤Ÿé•¿
                logger.info(f"[Search] âœ… Exa æœç´¢æˆåŠŸ: {query[:50]}...")
                # æ£€æŸ¥ä¿¡æ¯å……è¶³æ€§ (ç®€å•å¯å‘å¼)
                # å¦‚æœæ˜¯æ·±åº¦æŸ¥è¯¢ï¼Œä¸” Exa è¿”å›äº†ä¸°å¯Œå†…å®¹ï¼Œç›´æ¥è¿”å›
                if len(exa_result) > 1000:
                    logger.info("[Search] ğŸš€ Exa ç»“æœå……è¶³ï¼Œè·³è¿‡å…¶ä»–æœç´¢æº")
                    return f"""ğŸ” ç»¼åˆæœç´¢ç»“æœ (æ¥è‡ª Exa):
{'='*60}

{exa_result}

{'='*60}
"""

                all_results.append({
                    'source': 'Exa',
                    'content': exa_result
                })
                sources_used.append('Exa')
        except Exception as e:
            error_msg = str(e) if e else "æœªçŸ¥é”™è¯¯"
            logger.info(f"[Search] Exa æœç´¢å¤±è´¥: {error_msg}")

    # 1.å°è¯• Tavily Search (AIæœç´¢)
    # å¦‚æœ Exa å¤±è´¥æˆ–ç»“æœä¸è¶³ï¼Œå°è¯• Tavily
    if TAVILY_API_KEY and TAVILY_AVAILABLE:
        try:
            tavily_result = _search_with_tavily(query)
            if tavily_result and len(tavily_result) > 50:
                all_results.append({
                    'source': 'Tavily',
                    'content': tavily_result
                })
                sources_used.append('Tavily')
                logger.info(f"[Search] âœ… Tavily æœç´¢æˆåŠŸ: {query[:50]}...")

                # å¦‚æœå·²æœ‰ä¸¤ä¸ªé«˜è´¨é‡æºï¼Œåœæ­¢æœç´¢
                if len(sources_used) >= 2:
                    logger.info("[Search] ğŸš€ å·²æœ‰ä¸¤ä¸ªé«˜è´¨é‡æºï¼Œè·³è¿‡åç»­æœç´¢")
                    return _merge_search_results(all_results, query)

        except Exception as e:
            error_msg = str(e) if e else "æœªçŸ¥é”™è¯¯"
            # å¿½ç•¥ Tavily é”™è¯¯ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæº
            logger.info(f"[Search] Tavily æœç´¢å¤±è´¥: {error_msg}")

    # 2. å°è¯•ç»´åŸºç™¾ç§‘ï¼ˆä»…ç”¨äºéé‡‘èæŸ¥è¯¢ï¼‰
    query_lower = query.lower()
    is_financial_query = any(kw in query_lower for kw in [
        'stock', 'price', 'market', 'trading', 'aapl', 'msft', 'googl', 'tsla', 'nvda',
        'nasdaq', 's&p', 'dow', 'sentiment', 'news', 'headline', 'earnings', 'revenue',
        'risk', 'trend', 'analysis', 'investment', 'portfolio', '^', '$'
    ])
    if WIKIPEDIA_AVAILABLE and not is_financial_query:
        try:
            wiki_result = _search_with_wikipedia(query)
            if wiki_result and len(wiki_result) > 100:
                all_results.append({
                    'source': 'Wikipedia',
                    'content': wiki_result
                })
                sources_used.append('Wikipedia')
                logger.info(f"[Search] âœ… ç»´åŸºç™¾ç§‘è·å–ä¿¡æ¯æˆåŠŸ: {query[:50]}...")
        except Exception as e:
            logger.info(f"[Search] ç»´åŸºç™¾ç§‘æœç´¢å¤±è´¥: {e}")

    # 3. å°è¯• DuckDuckGo (æœ€åå…œåº•)
    # å¦‚æœä¹‹å‰æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œæˆ–è€…ç»“æœå¤ªå°‘
    if (not all_results) and DDGS_AVAILABLE and DDGS is not None:
        try:
            ddgs_result = _search_with_duckduckgo(query)
            if ddgs_result and len(ddgs_result) > 50:
                all_results.append({
                    'source': 'DuckDuckGo',
                    'content': ddgs_result
                })
                sources_used.append('DuckDuckGo')
                logger.info(f"[Search] âœ… DuckDuckGo æœç´¢æˆåŠŸ: {query[:50]}...")
        except Exception as e:
            logger.info(f"[Search] DuckDuckGo æœç´¢å¤±è´¥: {e}")

    # 4. åˆå¹¶æ‰€æœ‰ç»“æœ
    if not all_results:
        return "Search error: æ‰€æœ‰æœç´¢æºå‡å¤±è´¥ï¼Œæ— æ³•è·å–æœç´¢ç»“æœã€‚"

    # åˆå¹¶ç»“æœ
    combined_result = _merge_search_results(all_results, query)

    logger.info(f"[Search] âœ… æœ€ç»ˆä½¿ç”¨ {len(sources_used)} ä¸ªæœç´¢æº: {', '.join(sources_used)}")
    return combined_result



def _search_with_duckduckgo(query: str) -> str:
    """ä½¿ç”¨ DuckDuckGo æœç´¢"""
    if not DDGS_AVAILABLE or DDGS is None:
        raise Exception("DuckDuckGo ä¸å¯ç”¨")
    
    for attempt in range(3):  # å¢åŠ é‡è¯•æ¬¡æ•°
        try:
            ddgs = DDGS()
            
            try:
                results = list(ddgs.text(query, max_results=10, safesearch='moderate'))
            except TypeError:
                results = list(ddgs.text(query, max_results=10))
            
            if not results:
                if attempt < 2:
                    time.sleep(2)
                    continue
                return None
            
            # éªŒè¯ç»“æœç›¸å…³æ€§
            query_lower = query.lower()
            relevant_results = []
            for res in results:
                title = res.get('title', '')
                body = res.get('body', '')
                title_lower = title.lower()
                body_lower = body.lower()
                
                query_words = [w for w in query_lower.split() if len(w) > 2 and w not in ['the', 'and', 'or', 'for', 'with', 'from']]
                is_relevant = any(word in title_lower or word in body_lower for word in query_words) if query_words else True
                
                if is_relevant or len(relevant_results) < 3:
                    relevant_results.append(res)
            
            if not relevant_results:
                if attempt < 2:
                    time.sleep(2)
                    continue
                relevant_results = results[:3]
            
            formatted = []
            for i, res in enumerate(relevant_results[:10], 1):
                title = res.get('title', 'No title')
                body = res.get('body', 'No summary')
                href = res.get('href', 'No link')
                
                title = title.encode('utf-8', 'ignore').decode('utf-8').strip()
                body = body.encode('utf-8', 'ignore').decode('utf-8').strip()
                
                if not title or not body:
                    continue
                    
                formatted.append(f"{i}. {title}\n   {body[:200]}...\n   {href}")
            
            if formatted:
                return "Search Results (DuckDuckGo):\n" + "\n\n".join(formatted)
            else:
                return None
                
        except Exception as e:
            if attempt < 2:
                time.sleep(2)
            else:
                raise e
    
    return None



def _merge_search_results(results: list, query: str) -> str:
    """
    åˆå¹¶å¤šä¸ªæœç´¢æºçš„ç»“æœ
    
    Args:
        results: æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'source' å’Œ 'content'
        query: åŸå§‹æŸ¥è¯¢
        
    Returns:
        åˆå¹¶åçš„æœç´¢ç»“æœæ–‡æœ¬
    """
    if not results:
        return "No search results found."
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªç»“æœï¼Œç›´æ¥è¿”å›
    if len(results) == 1:
        return results[0]['content']
    
    # åˆå¹¶å¤šä¸ªç»“æœ
    merged_parts = []
    merged_parts.append(f"ğŸ” ç»¼åˆæœç´¢ç»“æœ (æ¥è‡ª {len(results)} ä¸ªæ•°æ®æº):\n")
    merged_parts.append("=" * 60 + "\n\n")
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šExa > Wikipedia > Tavily > DuckDuckGo
    source_priority = {'Exa': 0, 'Wikipedia': 1, 'Tavily': 2, 'DuckDuckGo': 3}
    results_sorted = sorted(results, key=lambda x: source_priority.get(x['source'], 99))
    
    for i, result in enumerate(results_sorted, 1):
        source = result['source']
        content = result['content']
        
        merged_parts.append(f"ã€æ•°æ®æº {i}: {source}ã€‘\n")
        merged_parts.append("-" * 60 + "\n")
        
        # æå–ä¸»è¦å†…å®¹ï¼ˆå»é™¤æ ‡é¢˜å’Œæ ¼å¼ï¼‰
        if source == 'Wikipedia':
            # ç»´åŸºç™¾ç§‘ç»“æœå·²ç»æ ¼å¼åŒ–å¥½äº†
            merged_parts.append(content)
        elif source == 'Tavily':
            # Tavily ç»“æœä¹Ÿæ ¼å¼åŒ–å¥½äº†
            merged_parts.append(content)
        else:
            # DuckDuckGo ç»“æœ
            merged_parts.append(content)
        
        merged_parts.append("\n\n")
    
    merged_parts.append("=" * 60 + "\n")
    merged_parts.append(f"ğŸ’¡ æç¤º: ä»¥ä¸Šç»“æœæ¥è‡ªå¤šä¸ªæœç´¢æºï¼Œè¯·ç»¼åˆå‚è€ƒä»¥è·å¾—æœ€å‡†ç¡®çš„ä¿¡æ¯ã€‚\n")
    
    return "".join(merged_parts)



def _search_with_wikipedia(query: str) -> str:
    """
    ä½¿ç”¨ç»´åŸºç™¾ç§‘æœç´¢ï¼ˆå…è´¹ï¼Œä¸éœ€è¦API keyï¼‰
    
    ä¼˜å…ˆä½¿ç”¨ç»´åŸºç™¾ç§‘ï¼Œå› ä¸ºï¼š
    - å†…å®¹å‡†ç¡®ã€æƒå¨
    - ç»“æ„åŒ–ä¿¡æ¯
    - å…è´¹ï¼Œæ— é™åˆ¶
    - ç‰¹åˆ«é€‚åˆæŸ¥è¯¢æŒ‡æ•°æˆåˆ†è‚¡ã€å…¬å¸ä¿¡æ¯ç­‰
    """
    if not WIKIPEDIA_AVAILABLE or wikipedia is None:
        raise Exception("ç»´åŸºç™¾ç§‘ä¸å¯ç”¨ï¼ˆæœªå®‰è£… wikipediaï¼‰")
    
    try:
        # å°è¯•æœç´¢é¡µé¢ï¼ˆå¢åŠ æœç´¢ç»“æœæ•°é‡ï¼‰
        search_results = wikipedia.search(query, results=5)
        
        if not search_results:
            return None
        
        # å°è¯•å¤šä¸ªæœç´¢ç»“æœï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„
        best_result = None
        for page_title in search_results:
            try:
                page = wikipedia.page(page_title, auto_suggest=False)
                
                # è·å–é¡µé¢æ‘˜è¦å’Œä¸»è¦å†…å®¹
                summary = page.summary
                content = page.content[:5000]  # å¢åŠ å†…å®¹é•¿åº¦
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦ç›¸å…³ï¼ˆåŒ…å«æŸ¥è¯¢å…³é”®è¯ï¼‰
                query_lower = query.lower()
                content_lower = (summary + content).lower()
                
                # å¦‚æœå†…å®¹åŒ…å«æŸ¥è¯¢å…³é”®è¯ï¼Œè®¤ä¸ºæ˜¯ç›¸å…³ç»“æœ
                if any(keyword in content_lower for keyword in query_lower.split() if len(keyword) > 2):
                    best_result = {
                        'title': page_title,
                        'summary': summary,
                        'content': content,
                        'url': page.url
                    }
                    break
                    
            except wikipedia.exceptions.DisambiguationError as e:
                # å¦‚æœæœ‰æ­§ä¹‰ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªé€‰é¡¹
                if e.options:
                    try:
                        page = wikipedia.page(e.options[0], auto_suggest=False)
                        summary = page.summary
                        content = page.content[:5000]
                        best_result = {
                            'title': e.options[0],
                            'summary': summary,
                            'content': content,
                            'url': page.url
                        }
                        break
                    except:
                        continue
                        
            except wikipedia.exceptions.PageError:
                continue
            except Exception as e:
                logger.info(f"[Search] ç»´åŸºç™¾ç§‘è·å–é¡µé¢ {page_title} å¤±è´¥: {e}")
                continue
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç›¸å…³ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
        if not best_result and search_results:
            try:
                page = wikipedia.page(search_results[0], auto_suggest=False)
                best_result = {
                    'title': search_results[0],
                    'summary': page.summary,
                    'content': page.content[:5000],
                    'url': page.url
                }
            except:
                return None
        
        if best_result:
            # æ ¼å¼åŒ–ç»“æœ
            result = f"""Wikipedia Results for "{best_result['title']}":

Summary:
{best_result['summary']}

Detailed Information:
{best_result['content']}

URL: {best_result['url']}"""
            return result
        
        return None
            
    except Exception as e:
        logger.info(f"[Search] ç»´åŸºç™¾ç§‘æœç´¢å‡ºé”™: {e}")
        return None



def _search_with_tavily(query: str) -> str:
    """
    ä½¿ç”¨ Tavily Search API è¿›è¡ŒAIæœç´¢

    Tavily æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºAIåº”ç”¨è®¾è®¡çš„æœç´¢APIï¼Œæä¾›ï¼š
    - æ›´å‡†ç¡®çš„æœç´¢ç»“æœ
    - ç»“æ„åŒ–çš„æ•°æ®æ ¼å¼
    - æ›´å¥½çš„ä¸Šä¸‹æ–‡ç†è§£
    """
    if not TAVILY_API_KEY:
        raise Exception("Tavily API key not configured")

    if not TAVILY_AVAILABLE or TavilyClient is None:
        raise Exception("Tavily å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼ˆæœªå®‰è£… tavily-pythonï¼‰")

    try:
        client = TavilyClient(api_key=TAVILY_API_KEY)

        # æ‰§è¡Œæœç´¢
        response = client.search(
            query=query,
            search_depth="advanced",  # basic æˆ– advanced
            max_results=10,
            include_answer=True,  # åŒ…å«AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦
            include_raw_content=False,  # ä¸åŒ…å«åŸå§‹å†…å®¹ï¼ˆèŠ‚çœtokenï¼‰
        )

        # æ ¼å¼åŒ–ç»“æœ
        formatted = []

        # å¦‚æœæœ‰AIç”Ÿæˆçš„ç­”æ¡ˆï¼Œä¼˜å…ˆæ˜¾ç¤º
        if response.get('answer'):
            formatted.append(f"ğŸ“Š AIæ‘˜è¦:\n{response['answer']}\n")

        # æ˜¾ç¤ºæœç´¢ç»“æœ
        results = response.get('results', [])
        if results:
            formatted.append("æœç´¢ç»“æœ:")
            for i, res in enumerate(results, 1):
                title = res.get('title', 'No title')
                content = res.get('content', 'No content')
                url = res.get('url', 'No link')
                score = res.get('score', 0)

                formatted.append(
                    f"{i}. {title} (ç›¸å…³æ€§: {score:.2f})\n"
                    f"   {content[:200]}...\n"
                    f"   {url}"
                )
        else:
            formatted.append("æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœã€‚")

        return "\n\n".join(formatted)

    except Exception as e:
        error_msg = str(e) if e else "æœªçŸ¥é”™è¯¯"
        error_type = type(e).__name__
        logger.info(f"[Search] Tavily API é”™è¯¯ ({error_type}): {error_msg}")

        # å¦‚æœæ˜¯ API key ç›¸å…³é”™è¯¯ï¼Œç»™å‡ºæ›´æ˜ç¡®çš„æç¤º
        if "api" in error_msg.lower() or "key" in error_msg.lower() or "auth" in error_msg.lower():
            logger.info(f"[Search] æç¤º: è¯·æ£€æŸ¥ TAVILY_API_KEY æ˜¯å¦æ­£ç¡®é…ç½®")

        raise Exception(f"Tavily API é”™è¯¯: {error_msg}")



def _search_with_exa(query: str) -> str:
    """
    ä½¿ç”¨ Exa Search API è¿›è¡Œè¯­ä¹‰æœç´¢

    Exa æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºAIåº”ç”¨è®¾è®¡çš„è¯­ä¹‰æœç´¢APIï¼Œæä¾›ï¼š
    - ç¥ç»ç½‘ç»œé©±åŠ¨çš„è¯­ä¹‰æœç´¢
    - é«˜è´¨é‡çš„å†…å®¹æå–
    - æ›´å¥½çš„ä¸Šä¸‹æ–‡ç†è§£
    """
    if not EXA_API_KEY:
        raise Exception("Exa API key not configured")

    if not EXA_AVAILABLE or Exa is None:
        raise Exception("Exa å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼ˆæœªå®‰è£… exa-pyï¼‰")

    try:
        exa = Exa(api_key=EXA_API_KEY)

        # æ‰§è¡Œæœç´¢
        response = exa.search_and_contents(
            query=query,
            type="neural",  # neural æˆ– keyword
            num_results=10,
            text=True,  # åŒ…å«æ–‡æœ¬å†…å®¹
            highlights=True,  # åŒ…å«é«˜äº®ç‰‡æ®µ
        )

        # æ ¼å¼åŒ–ç»“æœ
        formatted = []
        formatted.append("Search Results (Exa):")

        if response.results:
            for i, res in enumerate(response.results, 1):
                title = res.title or 'No title'
                url = res.url or 'No link'

                # è·å–é«˜äº®æˆ–æ–‡æœ¬å†…å®¹
                content = ""
                if hasattr(res, 'highlights') and res.highlights:
                    content = " ".join(res.highlights[:2])
                elif hasattr(res, 'text') and res.text:
                    content = res.text[:300]

                published = (
                    getattr(res, "published_date", None)
                    or getattr(res, "published_at", None)
                    or getattr(res, "date", None)
                    or getattr(res, "created_at", None)
                )
                date_str = _normalize_published_date(published)
                if date_str:
                    content = f"{date_str} {content}".strip()

                formatted.append(
                    f"{i}. {title}\n"
                    f"   {content}...\n"
                    f"   {url}"
                )

            return "\n\n".join(formatted)
        else:
            return None

    except Exception as e:
        raise Exception(f"Exa search failed: {str(e)}")

# ============================================
# è‚¡ä»·è·å– - å¤šæ•°æ®æºç­–ç•¥
# ============================================
