import logging
import os
import re
import time
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, date
from email.utils import parsedate_to_datetime
from typing import Optional, List, Dict, Any
from urllib.parse import urlparse

import yfinance as yf

from .env import ALPHA_VANTAGE_API_KEY, finnhub_client
from .http import _http_get
from .search import search
from .utils import _normalize_published_date

logger = logging.getLogger(__name__)

NEWS_TAG_RULES = [
    ("ç§‘æŠ€", ["tech", "technology", "software", "hardware", "cloud", "cyber", "ç§‘æŠ€", "è½¯ä»¶", "ç¡¬ä»¶", "äº‘", "æ•°æ®ä¸­å¿ƒ", "äº’è”ç½‘"]),
    ("AI", ["ai", "artificial intelligence", "genai", "å¤§æ¨¡å‹", "ç”Ÿæˆå¼", "äººå·¥æ™ºèƒ½", "AIGC"]),
    ("åŠå¯¼ä½“", ["semiconductor", "chip", "foundry", "tsmc", "asml", "nvidia", "åŠå¯¼ä½“", "èŠ¯ç‰‡", "æ™¶åœ†", "å…‰åˆ»"]),
    ("å†›äº‹", ["military", "defense", "missile", "army", "navy", "weapon", "drone", "å†›äº‹", "å›½é˜²", "å¯¼å¼¹", "æˆ˜æœº", "æ— äººæœº", "æ­¦å™¨"]),
    ("èƒ½æº", ["oil", "crude", "gas", "lng", "opec", "èƒ½æº", "çŸ³æ²¹", "åŸæ²¹", "å¤©ç„¶æ°”", "ç…¤ç‚­", "ç”µåŠ›"]),
    ("å®è§‚", ["cpi", "ppi", "gdp", "pmi", "fomc", "inflation", "jobs", "payroll", "å®è§‚", "ç»æµ", "åˆ©ç‡", "é€šèƒ€", "å°±ä¸š", "éå†œ", "å¤®è¡Œ"]),
    ("é‡‘è", ["bank", "banking", "credit", "bond", "yield", "é‡‘è", "é“¶è¡Œ", "å€ºåˆ¸", "æ”¶ç›Šç‡", "ä¿¡è´·"]),
    ("ç›‘ç®¡", ["regulator", "regulation", "antitrust", "sec", "doj", "ç›‘ç®¡", "åå„æ–­", "åˆ¶è£", "ç½šæ¬¾"]),
    ("å¹¶è´­", ["merger", "acquisition", "buyout", "deal", "å¹¶è´­", "æ”¶è´­", "åˆå¹¶", "äº¤æ˜“", "è¦çº¦"]),
    ("è´¢æŠ¥", ["earnings", "guidance", "revenue", "profit", "ä¸šç»©", "è´¢æŠ¥", "è¥æ”¶", "åˆ©æ¶¦", "æŒ‡å¼•"]),
    ("åŠ å¯†", ["crypto", "bitcoin", "ethereum", "blockchain", "åŠ å¯†", "æ¯”ç‰¹å¸", "ä»¥å¤ªåŠ", "åŒºå—é“¾"]),
    ("æ±½è½¦", ["ev", "electric vehicle", "automotive", "auto", "æ±½è½¦", "ç”µåŠ¨è½¦", "æ–°èƒ½æºè½¦"]),
    ("æ¶ˆè´¹", ["consumer", "retail", "e-commerce", "æ¶ˆè´¹", "é›¶å”®", "ç”µå•†"]),
    ("åŒ»è¯", ["pharma", "biotech", "drug", "åŒ»ç–—", "åŒ»è¯", "ç”Ÿç‰©", "ç–«è‹—"]),
    ("åœ°äº§", ["real estate", "property", "housing", "åœ°äº§", "æ¥¼å¸‚"]),
    ("åœ°ç¼˜", ["geopolitical", "geopolitics", "war", "conflict", "sanction", "åœ°ç¼˜", "å†²çª", "æˆ˜äº‰"]),
    ("ä¸­å›½", ["china", "chinese", "ä¸­å›½", "å¤§é™†"]),
    ("ç¾å›½", ["united states", "u.s.", "ç¾å›½", "ç™½å®«", "åç››é¡¿"]),
]

MARKET_INDICES = {
    "^GSPC": "S&P 500 index",
    "^IXIC": "Nasdaq Composite index", 
    "^DJI": "Dow Jones Industrial Average",
    "^RUT": "Russell 2000 index",
    "^VIX": "VIX volatility index",
    "^NYA": "NYSE Composite index",
    "^FTSE": "FTSE 100 index",
    "^N225": "Nikkei 225 index",
    "^HSI": "Hang Seng index"
}

def _is_reasonable_headline(text: str, window: str = "") -> bool:
    """ç®€å•è¿‡æ»¤ï¼šéœ€è¦æ—¥æœŸ/æ—¶é—´çº¿ç´¢ï¼Œé¿å…ç™¾ç§‘/ä»‹ç»ç±»æ¡ç›®ã€‚"""
    combined = (window or "") + " " + text
    has_date = re.search(
        r"(\d{4}-\d{2}-\d{2}|\b20\d{2}\b|\b\d{1,2}\s+(hours?|days?)\s+ago\b)",
        combined,
        re.IGNORECASE,
    )
    if not has_date:
        return False
    lowered = combined.lower()
    if "wall street journal" in lowered:
        return False
    return True



def _get_env_int(key: str, default: int) -> int:
    try:
        return int(os.getenv(key, str(default)))
    except Exception:
        return default



def _contains_cjk(text: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", text))



def _headline_is_useful(title: str, snippet: str = "") -> bool:
    combined = f"{title} {snippet}".strip()
    if not combined:
        return False
    min_chars = _get_env_int("NEWS_MIN_TITLE_CHARS", 10)
    min_words = _get_env_int("NEWS_MIN_TITLE_WORDS", 4)
    if min_chars <= 0 and min_words <= 0:
        return True

    compact = re.sub(r"\s+", "", combined)
    if _contains_cjk(combined):
        if min_chars <= 0:
            return True
        return len(compact) >= min_chars

    word_count = len(re.findall(r"[A-Za-z0-9]+", combined))
    if min_words > 0 and min_chars > 0:
        return not (word_count < min_words and len(compact) < min_chars)
    if min_words > 0:
        return word_count >= min_words
    if min_chars > 0:
        return len(compact) >= min_chars
    return True


NEWS_TAG_RULES = [
    ("ç§‘æŠ€", ["tech", "technology", "software", "hardware", "cloud", "cyber", "ç§‘æŠ€", "è½¯ä»¶", "ç¡¬ä»¶", "äº‘", "æ•°æ®ä¸­å¿ƒ", "äº’è”ç½‘"]),
    ("AI", ["ai", "artificial intelligence", "genai", "å¤§æ¨¡å‹", "ç”Ÿæˆå¼", "äººå·¥æ™ºèƒ½", "AIGC"]),
    ("åŠå¯¼ä½“", ["semiconductor", "chip", "foundry", "tsmc", "asml", "nvidia", "åŠå¯¼ä½“", "èŠ¯ç‰‡", "æ™¶åœ†", "å…‰åˆ»"]),
    ("å†›äº‹", ["military", "defense", "missile", "army", "navy", "weapon", "drone", "å†›äº‹", "å›½é˜²", "å¯¼å¼¹", "æˆ˜æœº", "æ— äººæœº", "æ­¦å™¨"]),
    ("èƒ½æº", ["oil", "crude", "gas", "lng", "opec", "èƒ½æº", "çŸ³æ²¹", "åŸæ²¹", "å¤©ç„¶æ°”", "ç…¤ç‚­", "ç”µåŠ›"]),
    ("å®è§‚", ["cpi", "ppi", "gdp", "pmi", "fomc", "inflation", "jobs", "payroll", "å®è§‚", "ç»æµ", "åˆ©ç‡", "é€šèƒ€", "å°±ä¸š", "éå†œ", "å¤®è¡Œ"]),
    ("é‡‘è", ["bank", "banking", "credit", "bond", "yield", "é‡‘è", "é“¶è¡Œ", "å€ºåˆ¸", "æ”¶ç›Šç‡", "ä¿¡è´·"]),
    ("ç›‘ç®¡", ["regulator", "regulation", "antitrust", "sec", "doj", "ç›‘ç®¡", "åå„æ–­", "åˆ¶è£", "ç½šæ¬¾"]),
    ("å¹¶è´­", ["merger", "acquisition", "buyout", "deal", "å¹¶è´­", "æ”¶è´­", "åˆå¹¶", "äº¤æ˜“", "è¦çº¦"]),
    ("è´¢æŠ¥", ["earnings", "guidance", "revenue", "profit", "ä¸šç»©", "è´¢æŠ¥", "è¥æ”¶", "åˆ©æ¶¦", "æŒ‡å¼•"]),
    ("åŠ å¯†", ["crypto", "bitcoin", "ethereum", "blockchain", "åŠ å¯†", "æ¯”ç‰¹å¸", "ä»¥å¤ªåŠ", "åŒºå—é“¾"]),
    ("æ±½è½¦", ["ev", "electric vehicle", "automotive", "auto", "æ±½è½¦", "ç”µåŠ¨è½¦", "æ–°èƒ½æºè½¦"]),
    ("æ¶ˆè´¹", ["consumer", "retail", "e-commerce", "æ¶ˆè´¹", "é›¶å”®", "ç”µå•†"]),
    ("åŒ»è¯", ["pharma", "biotech", "drug", "åŒ»ç–—", "åŒ»è¯", "ç”Ÿç‰©", "ç–«è‹—"]),
    ("åœ°äº§", ["real estate", "property", "housing", "åœ°äº§", "æ¥¼å¸‚"]),
    ("åœ°ç¼˜", ["geopolitical", "geopolitics", "war", "conflict", "sanction", "åœ°ç¼˜", "å†²çª", "æˆ˜äº‰"]),
    ("ä¸­å›½", ["china", "chinese", "ä¸­å›½", "å¤§é™†"]),
    ("ç¾å›½", ["united states", "u.s.", "ç¾å›½", "ç™½å®«", "åç››é¡¿"]),
]



def _keyword_match(text: str, keyword: str) -> bool:
    if not keyword:
        return False
    kw = keyword.lower()
    if _contains_cjk(kw):
        return kw in text
    if len(kw) <= 3 and kw.isalpha():
        return re.search(rf"\b{re.escape(kw)}\b", text) is not None
    return kw in text



def _headline_tags(text: str) -> List[str]:
    if not text:
        return []
    text_lower = text.lower()
    max_tags = max(1, _get_env_int("NEWS_TAG_MAX", 3))
    tags: List[str] = []
    for tag, keywords in NEWS_TAG_RULES:
        if any(_keyword_match(text_lower, kw) for kw in keywords):
            tags.append(tag)
            if len(tags) >= max_tags:
                break
    return tags



def _format_headline_line(
    date_str: str,
    title: str,
    source: str,
    url: str = "",
    snippet: str = "",
) -> str:
    tags = _headline_tags(f"{title} {snippet}".strip())
    tag_text = f"[{'/'.join(tags)}] " if tags else ""
    clean_title = (title or "").strip() or "Untitled"
    display_title = f"[{clean_title}]({url})" if url else clean_title
    clean_source = (source or "").strip()
    source_text = f"({clean_source})" if clean_source else ""
    clean_snippet = (snippet or "").strip()
    if len(clean_snippet) > 160:
        clean_snippet = clean_snippet[:157] + "..."
    snippet_text = f" - {clean_snippet}" if clean_snippet else ""
    return f"[{date_str}] {tag_text}{display_title} {source_text}{snippet_text}".strip()


def _domain_from_url(url: str) -> str:
    try:
        parsed = urlparse(url)
        host = parsed.netloc or ""
        return host.replace("www.", "")
    except Exception:
        return ""



def _extract_datetime_from_text(text: str, now: datetime) -> Optional[datetime]:
    if not text:
        return None
    lowered = text.lower()

    # Relative English (e.g., "3 hours ago", "2 days ago")
    m = re.search(r"(\d{1,2})\s*(hours?|days?)\s+ago", lowered)
    if m:
        value = int(m.group(1))
        unit = m.group(2)
        if "hour" in unit:
            return now - timedelta(hours=value)
        return now - timedelta(days=value)

    # Relative Chinese (e.g., "3å°æ—¶å‰", "2å¤©å‰", "10åˆ†é’Ÿå‰")
    m = re.search(r"(\d{1,2})\s*(å°æ—¶|å¤©|åˆ†é’Ÿ)å‰", text)
    if m:
        value = int(m.group(1))
        unit = m.group(2)
        if unit == "å°æ—¶":
            return now - timedelta(hours=value)
        if unit == "åˆ†é’Ÿ":
            return now - timedelta(minutes=value)
        return now - timedelta(days=value)

    # Absolute date: YYYY-MM-DD or YYYY/MM/DD
    m = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})", text)
    if m:
        try:
            return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
        except Exception:
            return None

    # Absolute date: Month DD, YYYY
    m = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+(\d{1,2}),?\s+(\d{4})",
        text,
        re.IGNORECASE,
    )
    if m:
        try:
            month = m.group(1).title()
            day = int(m.group(2))
            year = int(m.group(3))
            month_map = {
                "Jan": 1,
                "Feb": 2,
                "Mar": 3,
                "Apr": 4,
                "May": 5,
                "Jun": 6,
                "Jul": 7,
                "Aug": 8,
                "Sep": 9,
                "Oct": 10,
                "Nov": 11,
                "Dec": 12,
            }
            return datetime(year, month_map[month], day)
        except Exception:
            return None

    return None



def _extract_datetime_from_url(url: str) -> Optional[datetime]:
    if not url:
        return None

    # Patterns like /2025/07/23/ or 2025-07-23
    m = re.search(r"(20\d{2})[-/](\d{1,2})[-/](\d{1,2})", url)
    if m:
        try:
            return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
        except Exception:
            return None

    # Pattern like 20250723 (avoid matching long ids by requiring separators nearby)
    m = re.search(r"(20\d{2})(\d{2})(\d{2})", url)
    if m:
        try:
            return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
        except Exception:
            return None

    return None



def _build_news_item(
    title: str,
    source: str,
    url: str = "",
    published_at: Any = None,
    snippet: str = "",
    ticker: Optional[str] = None,
    confidence: float = 0.7,
) -> Dict[str, Any]:
    if not title:
        return {}
    published_date = _normalize_published_date(published_at)
    return {
        "headline": title,
        "title": title,
        "url": url or "",
        "source": source or "Unknown",
        "snippet": snippet or "",
        "published_at": published_date,
        "datetime": published_date,
        "ticker": ticker,
        "confidence": confidence,
    }



def format_news_items(items: List[Dict[str, Any]], title: str = "Latest News") -> str:
    if not items:
        return "No recent news available."
    lines: List[str] = []
    for idx, item in enumerate(items, 1):
        headline = item.get("headline") or item.get("title") or "No title"
        source = item.get("source") or "Unknown"
        url = item.get("url") or ""
        snippet = item.get("snippet") or ""
        date_str = item.get("published_at") or item.get("datetime") or "Recent"
        line = _format_headline_line(date_str, headline, source, url, snippet)
        lines.append(f"{idx}. {line}")
    return f"{title}:\n" + "\n".join(lines)



def _extract_search_items(text: str) -> List[Dict[str, str]]:
    items: List[Dict[str, str]] = []
    current: Dict[str, str] | None = None

    for line in text.splitlines():
        stripped = line.strip()
        if re.match(r"^\d+\.", stripped):
            if current:
                items.append(current)
            title = re.sub(r"^\d+\.\s*", "", stripped).strip()
            current = {"title": title, "snippet": "", "url": ""}
            continue
        if stripped.startswith("http"):
            if current and not current.get("url"):
                current["url"] = stripped
            continue
        if stripped and current and not current.get("snippet"):
            current["snippet"] = stripped

    if current:
        items.append(current)

    return items



def _format_search_news_items(
    text: str,
    limit: int = 5,
    max_age_days: int = 7,
    now: Optional[datetime] = None,
) -> tuple[list[str], bool]:
    now = now or datetime.utcnow()
    items = _extract_search_items(text)
    enriched = []

    for item in items:
        title = item.get("title", "")
        snippet = item.get("snippet", "")
        if not _headline_is_useful(title, snippet):
            continue
        candidate_text = f"{title} {snippet}"
        dt = _extract_datetime_from_text(candidate_text, now)
        if not dt and item.get("url"):
            dt = _extract_datetime_from_url(item["url"])
        age_days = (now - dt).days if dt else None
        url = item.get("url", "")
        source = _domain_from_url(url)
        enriched.append(
            {
                "title": title,
                "snippet": snippet,
                "url": url,
                "source": source,
                "date": dt,
                "age_days": age_days,
            }
        )

    recent = [
        item
        for item in enriched
        if item["date"] and (now - item["date"]) <= timedelta(days=max_age_days)
    ]
    use_items = recent if recent else enriched

    lines: List[str] = []
    for item in use_items[:limit]:
        date_str = item["date"].strftime("%Y-%m-%d") if item["date"] else "æœªçŸ¥æ—¥æœŸ"
        source = item["source"] or "source"
        url = item["url"] or ""
        lines.append(
            _format_headline_line(
                date_str,
                item["title"],
                source,
                url,
                item.get("snippet", ""),
            )
        )

    return lines, bool(recent)



def _build_search_news_items(
    text: str,
    limit: int = 5,
    max_age_days: int = 7,
    now: Optional[datetime] = None,
) -> List[Dict[str, Any]]:
    now = now or datetime.utcnow()
    items = _extract_search_items(text)
    enriched = []

    for item in items:
        title = item.get("title", "")
        snippet = item.get("snippet", "")
        if not _headline_is_useful(title, snippet):
            continue
        candidate_text = f"{title} {snippet}"
        dt = _extract_datetime_from_text(candidate_text, now)
        if not dt and item.get("url"):
            dt = _extract_datetime_from_url(item["url"])
        age_days = (now - dt).days if dt else None
        url = item.get("url", "")
        source = _domain_from_url(url)
        enriched.append(
            {
                "title": title,
                "snippet": snippet,
                "url": url,
                "source": source,
                "date": dt,
                "age_days": age_days,
            }
        )

    recent = [
        item
        for item in enriched
        if item["date"] and (now - item["date"]) <= timedelta(days=max_age_days)
    ]
    use_items = recent if recent else enriched

    results: List[Dict[str, Any]] = []
    for item in use_items[:limit]:
        published_at = item["date"].strftime("%Y-%m-%d") if item["date"] else None
        results.append(
            _build_news_item(
                title=item["title"],
                source=item["source"] or "search",
                url=item["url"],
                published_at=published_at,
                snippet=item.get("snippet", ""),
                confidence=0.4,
            )
        )
    return [item for item in results if item]



def _parse_rss_items(
    xml_text: str,
    limit: int = 5,
    max_age_days: int = 2,
    now: Optional[datetime] = None,
) -> tuple[list[str], bool]:
    now = now or datetime.utcnow()
    lines: List[str] = []
    try:
        root = ET.fromstring(xml_text)
    except Exception:
        return [], False

    items = root.findall(".//item")
    for item in items:
        title = (item.findtext("title") or "").strip()
        link = (item.findtext("link") or "").strip()
        pub_date = (item.findtext("pubDate") or "").strip()
        if not title or not pub_date:
            continue
        if not _headline_is_useful(title, ""):
            continue

        try:
            dt = parsedate_to_datetime(pub_date)
        except Exception:
            dt = None
        if not dt:
            continue
        if dt.tzinfo:
            dt = dt.astimezone(tz=None).replace(tzinfo=None)

        if (now - dt) > timedelta(days=max_age_days):
            continue

        source = _domain_from_url(link)
        date_str = dt.strftime("%Y-%m-%d")
        lines.append(_format_headline_line(date_str, title, source, link))
        if len(lines) >= limit:
            break

    return lines, bool(lines)



def _fetch_rss_headlines(
    feed_urls: List[str],
    limit: int = 5,
    max_age_days: int = 2,
) -> tuple[list[str], bool]:
    all_lines: List[str] = []
    for url in feed_urls:
        try:
            resp = _http_get(url, timeout=8)
            if resp.status_code != 200:
                continue
            lines, ok = _parse_rss_items(resp.text, limit=limit, max_age_days=max_age_days)
            if ok:
                all_lines.extend(lines)
        except Exception:
            continue
        if len(all_lines) >= limit:
            break
    return all_lines[:limit], bool(all_lines)



def _fetch_finnhub_market_news(limit: int = 5, max_age_hours: int = 48) -> tuple[list[str], bool]:
    if not finnhub_client:
        return [], False

    now = datetime.utcnow()
    try:
        items = finnhub_client.general_news("general")
    except Exception:
        return [], False

    lines = []
    for item in items or []:
        ts = item.get("datetime")
        if not ts:
            continue
        try:
            dt = datetime.utcfromtimestamp(ts)
        except Exception:
            continue
        if (now - dt) > timedelta(hours=max_age_hours):
            continue
        title = item.get("headline") or item.get("summary") or "No title"
        snippet = item.get("summary") or ""
        if not _headline_is_useful(title, snippet):
            continue
        source = item.get("source") or "finnhub"
        url = item.get("url") or ""
        lines.append(
            _format_headline_line(
                dt.strftime("%Y-%m-%d"),
                title,
                source,
                url,
                snippet,
            )
        )
        if len(lines) >= limit:
            break
    return lines, bool(lines)

MARKET_INDICES = {
    "^GSPC": "S&P 500 index",
    "^IXIC": "Nasdaq Composite index", 
    "^DJI": "Dow Jones Industrial Average",
    "^RUT": "Russell 2000 index",
    "^VIX": "VIX volatility index",
    "^NYA": "NYSE Composite index",
    "^FTSE": "FTSE 100 index",
    "^N225": "Nikkei 225 index",
    "^HSI": "Hang Seng index"
}


def _is_market_index(ticker: str) -> bool:
    """åˆ¤æ–­tickeræ˜¯å¦ä¸ºå¸‚åœºæŒ‡æ•°"""
    # æ–¹æ³•1: æ£€æŸ¥æ˜¯å¦åœ¨å·²çŸ¥æŒ‡æ•°åˆ—è¡¨ä¸­
    if ticker in MARKET_INDICES:
        return True
    
    # æ–¹æ³•2: æ£€æŸ¥å¸¸è§æŒ‡æ•°å‘½åæ¨¡å¼
    index_patterns = [
        r'^\^',      # ä»¥ ^ å¼€å¤´ï¼ˆYahoo FinanceæŒ‡æ•°æ ‡è®°ï¼‰
        r'SPX$',     # S&P 500 çš„å¦ä¸€ç§å†™æ³•
        r'NDX$',     # Nasdaq 100
        r'DJI$',     # Dow Jones
    ]
    
    for pattern in index_patterns:
        if re.match(pattern, ticker):
            return True
    
    return False


def _get_index_news(ticker: str) -> List[Dict[str, Any]]:
    """
    ä¸“é—¨ä¸ºå¸‚åœºæŒ‡æ•°è·å–æ–°é—»çš„æ–¹æ³•ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰ã€‚
    ç­–ç•¥ï¼šé€šè¿‡æœç´¢è·å–å®è§‚å¸‚åœºæ–°é—»å’ŒæŒ‡æ•°åˆ†æã€‚
    """
    friendly_name = MARKET_INDICES.get(ticker, ticker.replace('^', ''))
    
    logger.info(f"  â†’ Detected market index: {friendly_name}")
    logger.info(f"  â†’ Using specialized search strategy for index news...")
    
    # ç­–ç•¥1: æœç´¢æŒ‡æ•°æœ€è¿‘è¡¨ç°å’Œåˆ†æ
    current_date = datetime.now().strftime('%B %Y')
    search_queries = [
        f"{friendly_name} recent performance analysis {current_date}",
        f"{friendly_name} market news today",
        f"What's driving {friendly_name} this week"
    ]
    
    all_results = []
    for query in search_queries[:2]:  # åªç”¨å‰2ä¸ªæŸ¥è¯¢ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
        try:
            results = search(query)
            if results and "No search results" not in results:
                all_results.append(results)
            time.sleep(1)
        except Exception as e:
            logger.info(f"  â†’ Search failed for '{query}': {e}")
            continue
    
    if not all_results:
        return []
    
    # è§£æå¹¶æ ¼å¼åŒ–æœç´¢ç»“æœ
    combined_results = "\n\n".join(all_results)
    
    # å°è¯•ä»æœç´¢ç»“æœä¸­æå–æ–°é—»æ ‡é¢˜å’Œæ—¥æœŸ
    news_items: List[Dict[str, Any]] = []
    lines = combined_results.split('\n')
    
    for i, line in enumerate(lines):
        # å¯»æ‰¾æ ‡é¢˜æ¨¡å¼ï¼ˆé€šå¸¸ä»¥æ•°å­—å¼€å¤´ï¼‰
        if re.match(r'^\d+\.', line.strip()):
            raw_title = line.strip()
            title = re.sub(r'^\d+\.\s*', '', raw_title).strip()
            window = ' '.join(lines[i:i+3])
            # å°è¯•æ‰¾åˆ°æ—¥æœŸä¿¡æ¯
            date_match = re.search(r'(\d{1,2}\s+\w+\s+ago|\d{4}-\d{2}-\d{2}|\w+\s+\d{1,2},?\s+\d{4})', 
                                  window, re.IGNORECASE)
            if not _is_reasonable_headline(title, window):
                continue
            if not _headline_is_useful(title, window):
                continue
            date_str = date_match.group(1) if date_match else 'Recent'
            item = _build_news_item(
                title=title,
                source="search",
                url="",
                published_at=date_str,
                snippet=window,
                ticker=ticker,
                confidence=0.4,
            )
            if item:
                news_items.append(item)
            
            if len(news_items) >= 5:
                break
    
    return news_items


def get_company_news(ticker: str) -> List[Dict[str, Any]]:
    """
    æ™ºèƒ½è·å–æ–°é—»ï¼šè‡ªåŠ¨è¯†åˆ«æ˜¯å…¬å¸è‚¡ç¥¨è¿˜æ˜¯å¸‚åœºæŒ‡æ•°ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰ã€‚
    - å…¬å¸è‚¡ç¥¨ï¼šä½¿ç”¨ API (yfinance, Finnhub, Alpha Vantage)
    - å¸‚åœºæŒ‡æ•°ï¼šä½¿ç”¨æœç´¢ç­–ç•¥è·å–å®è§‚å¸‚åœºæ–°é—»
    """
    # ğŸ” å…³é”®åˆ¤æ–­ï¼šè¿™æ˜¯æŒ‡æ•°è¿˜æ˜¯å…¬å¸è‚¡ç¥¨ï¼Ÿ
    if _is_market_index(ticker):
        # ä¼˜å…ˆç”¨ alert_scheduler çš„æ–°é—»æŠ“å–ï¼ˆå«48hè¿‡æ»¤ï¼‰
        try:
            from backend.services.alert_scheduler import fetch_news_articles
            articles = fetch_news_articles(ticker)
            if articles:
                items: List[Dict[str, Any]] = []
                for a in articles:
                    title = a.get("title") or a.get("headline") or a.get("summary") or "No title"
                    snippet = a.get("summary") or a.get("description") or ""
                    if not _headline_is_useful(title, snippet):
                        continue
                    source = a.get("source") or a.get("publisher") or "Unknown"
                    published_at = a.get("published_at") or a.get("datetime") or a.get("providerPublishTime") or 0
                    url = a.get("url") or a.get("link") or ""
                    item = _build_news_item(
                        title=title,
                        source=source,
                        url=url,
                        published_at=published_at,
                        snippet=snippet,
                        ticker=ticker,
                        confidence=0.7,
                    )
                    if item:
                        items.append(item)
                    if len(items) >= 5:
                        break
                if items:
                    return items
        except Exception as e:
            logger.info(f"index news via alert_scheduler failed: {e}")

        # å…ˆè¯• yfinance çš„æ–°é—»ï¼ˆéƒ¨åˆ†æŒ‡æ•°ä¹Ÿæœ‰ï¼‰
        try:
            stock = yf.Ticker(ticker)
            news = stock.news
            if news:
                items = []
                for article in news:
                    title = article.get('title', 'No title')
                    snippet = article.get('summary') or article.get('description') or ""
                    if not _headline_is_useful(title, snippet):
                        continue
                    publisher = article.get('publisher', 'Unknown source')
                    pub_time = article.get('providerPublishTime', 0)
                    url = article.get('link') or article.get('url') or ''
                    item = _build_news_item(
                        title=title,
                        source=publisher,
                        url=url,
                        published_at=pub_time,
                        snippet=snippet,
                        ticker=ticker,
                        confidence=0.7,
                    )
                    if item:
                        items.append(item)
                    if len(items) >= 5:
                        break
                if items:
                    return items
        except Exception as e:
            logger.info(f"yfinance index news error for {ticker}: {e}")

        # å†é€€å›æœç´¢ç­–ç•¥
        return _get_index_news(ticker)
    
    # --- ä»¥ä¸‹æ˜¯åŸæœ‰çš„å…¬å¸æ–°é—»è·å–é€»è¾‘ ---
    
    # æ–¹æ³•1: yfinance
    try:
        stock = yf.Ticker(ticker)
        news = stock.news
        if news:
            items = []
            for article in news:
                title = article.get('title', 'No title')
                snippet = article.get('summary') or article.get('description') or ""
                if not _headline_is_useful(title, snippet):
                    continue
                publisher = article.get('publisher', 'Unknown source')
                pub_time = article.get('providerPublishTime', 0)
                url = article.get('link') or article.get('url') or ''
                item = _build_news_item(
                    title=title,
                    source=publisher,
                    url=url,
                    published_at=pub_time,
                    snippet=snippet,
                    ticker=ticker,
                    confidence=0.7,
                )
                if item:
                    items.append(item)
                if len(items) >= 5:
                    break
            if items:
                return items
    except Exception as e:
        logger.info(f"yfinance news error for {ticker}: {e}")

    # æ–¹æ³•2: Finnhub
    if finnhub_client:
        try:
            logger.info(f"Trying Finnhub news for {ticker}")
            to_date = date.today().strftime("%Y-%m-%d")
            from_date = (date.today() - timedelta(days=7)).strftime("%Y-%m-%d")
            news = finnhub_client.company_news(ticker, _from=from_date, to=to_date)
            if news:
                items = []
                for article in news:
                    title = article.get('headline', 'No title')
                    snippet = article.get('summary') or ""
                    if not _headline_is_useful(title, snippet):
                        continue
                    source = article.get('source', 'Unknown')
                    pub_time = article.get('datetime', 0)
                    url = article.get('url') or ''
                    item = _build_news_item(
                        title=title,
                        source=source,
                        url=url,
                        published_at=pub_time,
                        snippet=snippet,
                        ticker=ticker,
                        confidence=0.8,
                    )
                    if item:
                        items.append(item)
                    if len(items) >= 5:
                        break
                if items:
                    return items
        except Exception as e:
            logger.info(f"Finnhub news fetch failed: {e}")

    # æ–¹æ³•3: Alpha Vantage
    try:
        logger.info(f"Trying Alpha Vantage news for {ticker}")
        url = "https://www.alphavantage.co/query"
        params = {'function': 'NEWS_SENTIMENT', 'tickers': ticker, 'limit': 5, 'apikey': ALPHA_VANTAGE_API_KEY}
        response = _http_get(url, params=params, timeout=10)
        data = response.json()
        if 'feed' in data and data['feed']:
            items = []
            for article in data['feed']:
                title = article.get('title', 'No title')
                source = article.get('source', 'Unknown')
                date_str = article.get('time_published', '')[:8]
                if date_str:
                    date_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
                snippet = article.get('summary') or ""
                if not _headline_is_useful(title, snippet):
                    continue
                url = article.get('url') or article.get('link') or ''
                item = _build_news_item(
                    title=title,
                    source=source,
                    url=url,
                    published_at=date_str,
                    snippet=snippet,
                    ticker=ticker,
                    confidence=0.8,
                )
                if item:
                    items.append(item)
                if len(items) >= 5:
                    break
            if items:
                return items
    except Exception as e:
        logger.info(f"Alpha Vantage news fetch failed: {e}")
    
    # æ–¹æ³•4: å›é€€åˆ°å…¬å¸ç‰¹å®šæœç´¢
    logger.info(f"Falling back to search for {ticker} news")
    fallback_text = search(f"{ticker} company latest news stock")
    items = _build_search_news_items(fallback_text, limit=5, max_age_days=7)
    if items:
        for item in items:
            if isinstance(item, dict):
                item.setdefault("ticker", ticker)
        return items
    return []



def get_news_sentiment(ticker: str, limit: int = 5) -> str:
    """
    è·å–æ–°é—»æƒ…ç»ª (Alpha Vantage NEWS_SENTIMENT)
    """
    if not ticker:
        return "News Sentiment: ticker is required."

    if not ALPHA_VANTAGE_API_KEY:
        return "News Sentiment: ALPHA_VANTAGE_API_KEY not configured."

    try:
        url = "https://www.alphavantage.co/query"
        params = {
            'function': 'NEWS_SENTIMENT',
            'tickers': ticker,
            'limit': limit,
            'apikey': ALPHA_VANTAGE_API_KEY,
        }
        response = _http_get(url, params=params, timeout=10)
        data = response.json()

        if not data or 'feed' not in data or not data.get('feed'):
            if isinstance(data, dict):
                if data.get('Note'):
                    return f"News Sentiment: rate limited ({data.get('Note')})"
                if data.get('Information'):
                    return f"News Sentiment: {data.get('Information')}"
                if data.get('Error Message'):
                    return f"News Sentiment: {data.get('Error Message')}"
            return "News Sentiment: no data found."

        def _extract_sentiment(item: Dict[str, Any], symbol: str):
            symbol_upper = symbol.upper()
            for ts in item.get('ticker_sentiment', []):
                if ts.get('ticker', '').upper() == symbol_upper:
                    return ts.get('ticker_sentiment_score'), ts.get('ticker_sentiment_label')
            return item.get('overall_sentiment_score'), item.get('overall_sentiment_label')

        lines = []
        scores: List[float] = []
        for i, item in enumerate(data.get('feed', [])[:limit], 1):
            title = item.get('title', 'No title')
            source = item.get('source', 'Unknown')
            time_published = item.get('time_published', '')
            date_str = time_published[:8]
            if date_str and len(date_str) == 8:
                date_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            else:
                date_str = 'Unknown date'
            url = item.get('url') or item.get('link') or ''
            score, label = _extract_sentiment(item, ticker)
            sentiment_desc = "N/A"
            try:
                if score is not None:
                    score_val = float(score)
                    scores.append(score_val)
                    sentiment_desc = f"{label or 'Unknown'} ({score_val:.2f})"
                elif label:
                    sentiment_desc = label
            except Exception:
                if label:
                    sentiment_desc = label

            headline = f"[{title}]({url})" if url else title
            lines.append(f"{i}. [{date_str}] {headline} ({source}) æƒ…ç»ª: {sentiment_desc}")

        avg_text = ""
        if scores:
            avg_score = sum(scores) / len(scores)
            avg_text = f"\nå¹³å‡æƒ…ç»ªåˆ†æ•°: {avg_score:.2f}"

        return f"News Sentiment ({ticker}):{avg_text}\n" + "\n".join(lines)
    except Exception as e:
        return f"News Sentiment: fetch failed ({str(e)})"



def get_market_news_headlines(limit: int = 5) -> str:
    """
    å¸‚åœºæ³›åŒ–æ–°é—»ï¼šä¸å¸¦ ticker çš„æƒ…å†µï¼ŒæŠ“å–å…¨çƒ/ç¾è‚¡è¦é—»ã€‚
    ä½¿ç”¨æœç´¢èšåˆå¹¶æå–ç¼–å·è¡Œä½œä¸ºæ ‡é¢˜ï¼Œå¦åˆ™è¿”å›ç®€çŸ­æç¤ºã€‚
    """
    # 0) å®˜æ–¹ RSSï¼ˆReuters/Bloombergï¼‰ï¼Œä¼˜å…ˆ 48h å†…
    reuters_feeds = [
        "https://feeds.reuters.com/reuters/businessNews",
        "https://feeds.reuters.com/reuters/topNews",
        "https://feeds.reuters.com/reuters/worldNews",
        "https://feeds.reuters.com/reuters/technologyNews",
    ]
    bloomberg_default_feeds = [
        "https://feeds.bloomberg.com/markets/news.rss",
        "https://feeds.bloomberg.com/technology/news.rss",
        "https://feeds.bloomberg.com/politics/news.rss",
        "https://feeds.bloomberg.com/wealth/news.rss",
        "https://feeds.bloomberg.com/pursuits/news.rss",
        "https://feeds.bloomberg.com/businessweek/news.rss",
        "https://feeds.bloomberg.com/industries/news.rss",
    ]
    bloomberg_env = os.getenv("BLOOMBERG_RSS_URLS", "").strip()
    bloomberg_env_feeds = [u.strip() for u in bloomberg_env.split(",") if u.strip()]
    if bloomberg_env_feeds:
        bloomberg_feeds = bloomberg_default_feeds + [
            u for u in bloomberg_env_feeds if u not in bloomberg_default_feeds
        ]
    else:
        bloomberg_feeds = bloomberg_default_feeds

    rss_feeds = reuters_feeds + bloomberg_feeds
    rss_lines, rss_ok = _fetch_rss_headlines(rss_feeds, limit=limit * 2, max_age_days=2)
    if rss_ok:
        return "æœ€è¿‘48å°æ—¶å¸‚åœºè¦é—»(RSS):\n" + "\n".join(rss_lines[:limit])

    # 1) Finnhub å¸‚åœºæ–°é—»ï¼ˆ48hï¼‰
    finnhub_lines, finnhub_ok = _fetch_finnhub_market_news(limit=limit * 2, max_age_hours=48)
    if finnhub_ok:
        return "æœ€è¿‘48å°æ—¶å¸‚åœºè¦é—»(Finnhub):\n" + "\n".join(finnhub_lines[:limit])

    # 2) å°è¯•ç”¨ alert_scheduler çš„æ–°é—»æŠ“å–ï¼ˆå·²å«48hè¿‡æ»¤ï¼‰ï¼Œä¼˜å…ˆæŒ‡æ•°ä¸ä»£è¡¨æ€§ETF
    try:
        from backend.services.alert_scheduler import fetch_news_articles
        for idx_ticker in ["^GSPC", "^IXIC", "SPY", "QQQ", "DIA", "IWM"]:
            try:
                articles = fetch_news_articles(idx_ticker)
            except Exception as inner:
                logger.info(f"[MarketNews] fetch_news_articles failed for {idx_ticker}: {inner}")
                continue
            if articles:
                lines = []
                for a in articles:
                    title = a.get("title") or a.get("headline") or a.get("summary") or "No title"
                    snippet = a.get("summary") or a.get("description") or ""
                    if not _headline_is_useful(title, snippet):
                        continue
                    source = a.get("source") or a.get("publisher") or "Unknown"
                    published_at = a.get("published_at") or a.get("datetime") or a.get("providerPublishTime") or 0
                    if isinstance(published_at, str):
                        date_str = published_at.split("T")[0]
                    else:
                        date_str = datetime.fromtimestamp(published_at).strftime("%Y-%m-%d") if published_at else "Recent"
                    url = a.get("url") or a.get("link") or ""
                    line = _format_headline_line(date_str, title, source, url, snippet)
                    lines.append(f"{len(lines) + 1}. {line}")
                    if len(lines) >= limit:
                        break
                if lines:
                    return "æœ€è¿‘48å°æ—¶å¸‚åœºè¦é—»:\n" + "\n".join(lines)
    except Exception as e:
        logger.info(f"[MarketNews] fetch via alert_scheduler failed: {e}")

    # 3) æœç´¢èšåˆå…œåº•
    queries = [
        "global stock market breaking news today",
        "US stock market headlines today",
        "market moving news today equities"
    ]
    combined = []
    for q in queries:
        try:
            res = search(q)
            combined.append(res)
        except Exception as e:
            logger.info(f"[MarketNews] search failed for '{q}': {e}")
            continue
    if not combined:
        return "æœªèƒ½è·å–å¯é çš„å¸‚åœºçƒ­ç‚¹ä¿¡æ¯ï¼Œè¯·ç›´æ¥æŸ¥çœ‹ Bloomberg/Reuters/WSJ ç­‰æƒå¨æ¥æºã€‚"
    
    text = "\n\n".join(combined)
    lines, has_recent = _format_search_news_items(text, limit=limit, max_age_days=3)
    if not has_recent:
        lines, has_recent = _format_search_news_items(text, limit=limit, max_age_days=7)

    if not has_recent:
        retry_queries = [
            "global stock market news last 24 hours",
            "US stock market headlines last 24 hours",
            "market moving news past week site:reuters.com",
        ]
        retry_combined = []
        for q in retry_queries:
            try:
                res = search(q)
                retry_combined.append(res)
            except Exception as e:
                logger.info(f"[MarketNews] retry search failed for '{q}': {e}")
                continue
        if retry_combined:
            retry_text = "\n\n".join(retry_combined)
            retry_lines, retry_recent = _format_search_news_items(retry_text, limit=limit, max_age_days=7)
            if retry_lines and retry_recent:
                return "æœ€è¿‘å¸‚åœºçƒ­ç‚¹(è¿‘7å¤©):\n" + "\n".join(retry_lines)
            if retry_recent:
                lines = retry_lines
                has_recent = True

    if has_recent and lines:
        return "æœ€è¿‘å¸‚åœºçƒ­ç‚¹(è¿‘7å¤©):\n" + "\n".join(lines)

    return "è¿‘7å¤©å†…æœªæ£€ç´¢åˆ°å¯é å¸‚åœºçƒ­ç‚¹ï¼Œè¯·ç›´æ¥æŸ¥çœ‹ Bloomberg/Reuters/WSJ ç­‰æƒå¨æ¥æºã€‚"
# ============================================
# å…¶ä»–å·¥å…·å‡½æ•°ï¼ˆä¿æŒä¸å˜æˆ–ç¨ä½œä¿®æ”¹ï¼‰
# ============================================
