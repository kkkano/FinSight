import yfinance as yf
import json
import requests
import xml.etree.ElementTree as ET
import logging
from email.utils import parsedate_to_datetime
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, date
import time
import re
import finnhub
import pandas as pd
import os
from urllib.parse import urlparse, quote
from typing import Optional, List, Dict, Any, Union
from dotenv import load_dotenv
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from backend.security.ssrf import is_safe_url

logger = logging.getLogger(__name__)

# æœç´¢ç›¸å…³å¯¼å…¥ï¼ˆå·²æµ‹è¯•å¯ç”¨ï¼‰
try:
    from ddgs import DDGS  # æ–°ç‰ˆæœ¬åŒ…åï¼ˆæ¨èï¼‰
    DDGS_AVAILABLE = True
except ImportError:
    try:
        from duckduckgo_search import DDGS  # æ—§ç‰ˆæœ¬åŒ…åï¼ˆå…¼å®¹ï¼Œä½†ä¼šæ˜¾ç¤ºè­¦å‘Šï¼‰
        DDGS_AVAILABLE = True
        logger.info("[Warning] å»ºè®®ä½¿ç”¨ 'pip install ddgs' æ›¿ä»£ 'duckduckgo_search'")
    except ImportError:
        DDGS = None
        DDGS_AVAILABLE = False
        logger.info("[Warning] æœç´¢åŠŸèƒ½ä¸å¯ç”¨ï¼šæœªå®‰è£… ddgs æˆ– duckduckgo_search")

try:
    from tavily import TavilyClient
    TAVILY_AVAILABLE = True
except ImportError:
    TavilyClient = None
    TAVILY_AVAILABLE = False
    logger.info("[Warning] Tavily æœç´¢ä¸å¯ç”¨ï¼šæœªå®‰è£… tavily-python")

# Exa Search æ”¯æŒ
try:
    from exa_py import Exa
    EXA_AVAILABLE = True
except ImportError:
    Exa = None
    EXA_AVAILABLE = False
    logger.info("[Warning] Exa Search ä¸å¯ç”¨ï¼šæœªå®‰è£… exa-pyï¼Œè¿è¡Œ: pip install exa-py")

# ç»´åŸºç™¾ç§‘æ”¯æŒï¼ˆå…è´¹ï¼Œä¸éœ€è¦API keyï¼‰
try:
    import wikipedia
    wikipedia.set_lang("zh")  # è®¾ç½®ä¸­æ–‡
    WIKIPEDIA_AVAILABLE = True
except ImportError:
    wikipedia = None
    WIKIPEDIA_AVAILABLE = False
    logger.info("[Warning] ç»´åŸºç™¾ç§‘ä¸å¯ç”¨ï¼šæœªå®‰è£… wikipediaï¼Œè¿è¡Œ: pip install wikipedia")

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# ============================================
# HTTP ä¼šè¯ï¼ˆè¿æ¥æ±  + é‡è¯•ï¼‰
# ============================================
_HTTP_SESSION: Optional[requests.Session] = None


def _get_http_session() -> requests.Session:
    global _HTTP_SESSION
    if _HTTP_SESSION is not None:
        return _HTTP_SESSION
    retry = Retry(
        total=3,
        backoff_factor=0.3,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "POST", "HEAD"],
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
    session = requests.Session()
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    _HTTP_SESSION = session
    return session


def _http_get(url: str, **kwargs):
    return _get_http_session().get(url, **kwargs)


def _http_post(url: str, **kwargs):
    return _get_http_session().post(url, **kwargs)

# ============================================
# APIé…ç½®
# ============================================
ALPHA_VANTAGE_API_KEY = os.getenv("ALPHA_VANTAGE_API_KEY", "").strip('"')  # ç§»é™¤å¯èƒ½çš„å¼•å·
FINNHUB_API_KEY = os.getenv("FINNHUB_API_KEY", "").strip('"')
MASSIVE_API_KEY = os.getenv("MASSIVE_API_KEY", "").strip('"')  # Massive.com (åŸ Polygon.io) - è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®
IEX_CLOUD_API_KEY = os.getenv("IEX_CLOUD_API_KEY", "").strip('"')  # IEX Cloud (å…è´¹é¢åº¦: 50ä¸‡æ¬¡/æœˆ)
TIINGO_API_KEY = os.getenv("TIINGO_API_KEY", "").strip('"')  # Tiingo (å…è´¹é¢åº¦: æ¯æ—¥500æ¬¡)
TWELVE_DATA_API_KEY = os.getenv("TWELVE_DATA_API_KEY", "").strip('"')  # Twelve Data (å…è´¹é¢åº¦)
MARKETSTACK_API_KEY = os.getenv("MARKETSTACK_API_KEY", "").strip('"')  # Marketstack (å…è´¹é¢åº¦: 1000æ¬¡/æœˆ)
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "").strip('"')  # Tavily Search API (AIæœç´¢ï¼Œå…è´¹é¢åº¦: 1000æ¬¡/æœˆ)
EXA_API_KEY = os.getenv("EXA_API_KEY", "").strip('"')  # Exa Search API
OPENFIGI_API_KEY = os.getenv("OPENFIGI_API_KEY", "").strip('"')  # OpenFIGI (symbol lookup)
EODHD_API_KEY = os.getenv("EODHD_API_KEY", "").strip('"')  # EODHD (symbol lookup)
FRED_API_KEY = os.getenv("FRED_API_KEY", "").strip('"')  # FRED (Federal Reserve Economic Data)

# ============================================
# API å®¢æˆ·ç«¯åˆå§‹åŒ–
# ============================================
# åœ¨è„šæœ¬é¡¶éƒ¨åˆå§‹åŒ–ä¸€æ¬¡ï¼Œä»¥æé«˜æ•ˆç‡
try:
    finnhub_client = finnhub.Client(api_key=FINNHUB_API_KEY)
except Exception as e:
    logger.info(f"Failed to initialize Finnhub client: {e}")
    finnhub_client = None

# ============================================
# è¾…åŠ©å‡½æ•°
# ============================================

def _is_reasonable_headline(text: str, window: str = "") -> bool:
    """ç®€å•è¿‡æ»¤ï¼šéœ€è¦æ—¥æœŸ/æ—¶é—´çº¿ç´¢ï¼Œé¿å…ç™¾ç§‘/ä»‹ç»ç±»æ¡ç›®ã€‚"""
    combined = (window or "") + " " + text
    has_date = re.search(
        r"(\d{4}-\d{2}-\d{2}|\b20\d{2}\b|\b\d{1,2}\s+(hours?|days?)\s+ago\b)",
        combined,
        re.IGNORECASE,
    )
    if not has_date:
        return False
    lowered = combined.lower()
    if "wall street journal" in lowered:
        return False
    return True


def _get_env_int(key: str, default: int) -> int:
    try:
        return int(os.getenv(key, str(default)))
    except Exception:
        return default


def _contains_cjk(text: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", text))


def _headline_is_useful(title: str, snippet: str = "") -> bool:
    combined = f"{title} {snippet}".strip()
    if not combined:
        return False
    min_chars = _get_env_int("NEWS_MIN_TITLE_CHARS", 10)
    min_words = _get_env_int("NEWS_MIN_TITLE_WORDS", 4)
    if min_chars <= 0 and min_words <= 0:
        return True

    compact = re.sub(r"\s+", "", combined)
    if _contains_cjk(combined):
        if min_chars <= 0:
            return True
        return len(compact) >= min_chars

    word_count = len(re.findall(r"[A-Za-z0-9]+", combined))
    if min_words > 0 and min_chars > 0:
        return not (word_count < min_words and len(compact) < min_chars)
    if min_words > 0:
        return word_count >= min_words
    if min_chars > 0:
        return len(compact) >= min_chars
    return True


NEWS_TAG_RULES = [
    ("ç§‘æŠ€", ["tech", "technology", "software", "hardware", "cloud", "cyber", "ç§‘æŠ€", "è½¯ä»¶", "ç¡¬ä»¶", "äº‘", "æ•°æ®ä¸­å¿ƒ", "äº’è”ç½‘"]),
    ("AI", ["ai", "artificial intelligence", "genai", "å¤§æ¨¡å‹", "ç”Ÿæˆå¼", "äººå·¥æ™ºèƒ½", "AIGC"]),
    ("åŠå¯¼ä½“", ["semiconductor", "chip", "foundry", "tsmc", "asml", "nvidia", "åŠå¯¼ä½“", "èŠ¯ç‰‡", "æ™¶åœ†", "å…‰åˆ»"]),
    ("å†›äº‹", ["military", "defense", "missile", "army", "navy", "weapon", "drone", "å†›äº‹", "å›½é˜²", "å¯¼å¼¹", "æˆ˜æœº", "æ— äººæœº", "æ­¦å™¨"]),
    ("èƒ½æº", ["oil", "crude", "gas", "lng", "opec", "èƒ½æº", "çŸ³æ²¹", "åŸæ²¹", "å¤©ç„¶æ°”", "ç…¤ç‚­", "ç”µåŠ›"]),
    ("å®è§‚", ["cpi", "ppi", "gdp", "pmi", "fomc", "inflation", "jobs", "payroll", "å®è§‚", "ç»æµ", "åˆ©ç‡", "é€šèƒ€", "å°±ä¸š", "éå†œ", "å¤®è¡Œ"]),
    ("é‡‘è", ["bank", "banking", "credit", "bond", "yield", "é‡‘è", "é“¶è¡Œ", "å€ºåˆ¸", "æ”¶ç›Šç‡", "ä¿¡è´·"]),
    ("ç›‘ç®¡", ["regulator", "regulation", "antitrust", "sec", "doj", "ç›‘ç®¡", "åå„æ–­", "åˆ¶è£", "ç½šæ¬¾"]),
    ("å¹¶è´­", ["merger", "acquisition", "buyout", "deal", "å¹¶è´­", "æ”¶è´­", "åˆå¹¶", "äº¤æ˜“", "è¦çº¦"]),
    ("è´¢æŠ¥", ["earnings", "guidance", "revenue", "profit", "ä¸šç»©", "è´¢æŠ¥", "è¥æ”¶", "åˆ©æ¶¦", "æŒ‡å¼•"]),
    ("åŠ å¯†", ["crypto", "bitcoin", "ethereum", "blockchain", "åŠ å¯†", "æ¯”ç‰¹å¸", "ä»¥å¤ªåŠ", "åŒºå—é“¾"]),
    ("æ±½è½¦", ["ev", "electric vehicle", "automotive", "auto", "æ±½è½¦", "ç”µåŠ¨è½¦", "æ–°èƒ½æºè½¦"]),
    ("æ¶ˆè´¹", ["consumer", "retail", "e-commerce", "æ¶ˆè´¹", "é›¶å”®", "ç”µå•†"]),
    ("åŒ»è¯", ["pharma", "biotech", "drug", "åŒ»ç–—", "åŒ»è¯", "ç”Ÿç‰©", "ç–«è‹—"]),
    ("åœ°äº§", ["real estate", "property", "housing", "åœ°äº§", "æ¥¼å¸‚"]),
    ("åœ°ç¼˜", ["geopolitical", "geopolitics", "war", "conflict", "sanction", "åœ°ç¼˜", "å†²çª", "æˆ˜äº‰"]),
    ("ä¸­å›½", ["china", "chinese", "ä¸­å›½", "å¤§é™†"]),
    ("ç¾å›½", ["united states", "u.s.", "ç¾å›½", "ç™½å®«", "åç››é¡¿"]),
]


def _keyword_match(text: str, keyword: str) -> bool:
    if not keyword:
        return False
    kw = keyword.lower()
    if _contains_cjk(kw):
        return kw in text
    if len(kw) <= 3 and kw.isalpha():
        return re.search(rf"\b{re.escape(kw)}\b", text) is not None
    return kw in text


def _headline_tags(text: str) -> List[str]:
    if not text:
        return []
    text_lower = text.lower()
    max_tags = max(1, _get_env_int("NEWS_TAG_MAX", 3))
    tags: List[str] = []
    for tag, keywords in NEWS_TAG_RULES:
        if any(_keyword_match(text_lower, kw) for kw in keywords):
            tags.append(tag)
            if len(tags) >= max_tags:
                break
    return tags


def _format_headline_line(
    date_str: str,
    title: str,
    source: str,
    url: str = "",
    snippet: str = "",
) -> str:
    tags = _headline_tags(f"{title} {snippet}".strip())
    tag_text = f"[{'/'.join(tags)}] " if tags else ""
    clean_title = (title or "").strip() or "Untitled"
    display_title = f"[{clean_title}]({url})" if url else clean_title
    clean_source = (source or "").strip()
    source_text = f"({clean_source})" if clean_source else ""
    clean_snippet = (snippet or "").strip()
    if len(clean_snippet) > 160:
        clean_snippet = clean_snippet[:157] + "..."
    snippet_text = f" - {clean_snippet}" if clean_snippet else ""
    return f"[{date_str}] {tag_text}{display_title} {source_text}{snippet_text}".strip()

def search(query: str) -> str:
    """
    ä½¿ç”¨å¤šæ•°æ®æºç­–ç•¥æ‰§è¡Œç½‘é¡µæœç´¢å¹¶åˆå¹¶ç»“æœã€‚
    ç­–ç•¥Aï¼šä¸²è¡Œæœç´¢ + æ™ºèƒ½æ£€æµ‹
    ä¼˜å…ˆçº§ï¼šExa > Tavily > Wikipedia > DuckDuckGo

    Args:
        query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²

    Returns:
        æ ¼å¼åŒ–çš„åˆå¹¶æœç´¢ç»“æœ
    """
    all_results = []
    sources_used = []

    # 0. å°è¯• Exa Search (è¯­ä¹‰æœç´¢ï¼Œä¼˜å…ˆçº§æœ€é«˜)
    if EXA_API_KEY and EXA_AVAILABLE:
        try:
            exa_result = _search_with_exa(query)
            if exa_result and len(exa_result) > 200:  # ç¡®ä¿ç»“æœè¶³å¤Ÿé•¿
                logger.info(f"[Search] âœ… Exa æœç´¢æˆåŠŸ: {query[:50]}...")
                # æ£€æŸ¥ä¿¡æ¯å……è¶³æ€§ (ç®€å•å¯å‘å¼)
                # å¦‚æœæ˜¯æ·±åº¦æŸ¥è¯¢ï¼Œä¸” Exa è¿”å›äº†ä¸°å¯Œå†…å®¹ï¼Œç›´æ¥è¿”å›
                if len(exa_result) > 1000:
                    logger.info("[Search] ğŸš€ Exa ç»“æœå……è¶³ï¼Œè·³è¿‡å…¶ä»–æœç´¢æº")
                    return f"""ğŸ” ç»¼åˆæœç´¢ç»“æœ (æ¥è‡ª Exa):
{'='*60}

{exa_result}

{'='*60}
"""

                all_results.append({
                    'source': 'Exa',
                    'content': exa_result
                })
                sources_used.append('Exa')
        except Exception as e:
            error_msg = str(e) if e else "æœªçŸ¥é”™è¯¯"
            logger.info(f"[Search] Exa æœç´¢å¤±è´¥: {error_msg}")

    # 1.å°è¯• Tavily Search (AIæœç´¢)
    # å¦‚æœ Exa å¤±è´¥æˆ–ç»“æœä¸è¶³ï¼Œå°è¯• Tavily
    if TAVILY_API_KEY and TAVILY_AVAILABLE:
        try:
            tavily_result = _search_with_tavily(query)
            if tavily_result and len(tavily_result) > 50:
                all_results.append({
                    'source': 'Tavily',
                    'content': tavily_result
                })
                sources_used.append('Tavily')
                logger.info(f"[Search] âœ… Tavily æœç´¢æˆåŠŸ: {query[:50]}...")

                # å¦‚æœå·²æœ‰ä¸¤ä¸ªé«˜è´¨é‡æºï¼Œåœæ­¢æœç´¢
                if len(sources_used) >= 2:
                    logger.info("[Search] ğŸš€ å·²æœ‰ä¸¤ä¸ªé«˜è´¨é‡æºï¼Œè·³è¿‡åç»­æœç´¢")
                    return _merge_search_results(all_results, query)

        except Exception as e:
            error_msg = str(e) if e else "æœªçŸ¥é”™è¯¯"
            # å¿½ç•¥ Tavily é”™è¯¯ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæº
            logger.info(f"[Search] Tavily æœç´¢å¤±è´¥: {error_msg}")

    # 2. å°è¯•ç»´åŸºç™¾ç§‘ï¼ˆä»…ç”¨äºéé‡‘èæŸ¥è¯¢ï¼‰
    query_lower = query.lower()
    is_financial_query = any(kw in query_lower for kw in [
        'stock', 'price', 'market', 'trading', 'aapl', 'msft', 'googl', 'tsla', 'nvda',
        'nasdaq', 's&p', 'dow', 'sentiment', 'news', 'headline', 'earnings', 'revenue',
        'risk', 'trend', 'analysis', 'investment', 'portfolio', '^', '$'
    ])
    if WIKIPEDIA_AVAILABLE and not is_financial_query:
        try:
            wiki_result = _search_with_wikipedia(query)
            if wiki_result and len(wiki_result) > 100:
                all_results.append({
                    'source': 'Wikipedia',
                    'content': wiki_result
                })
                sources_used.append('Wikipedia')
                logger.info(f"[Search] âœ… ç»´åŸºç™¾ç§‘è·å–ä¿¡æ¯æˆåŠŸ: {query[:50]}...")
        except Exception as e:
            logger.info(f"[Search] ç»´åŸºç™¾ç§‘æœç´¢å¤±è´¥: {e}")

    # 3. å°è¯• DuckDuckGo (æœ€åå…œåº•)
    # å¦‚æœä¹‹å‰æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œæˆ–è€…ç»“æœå¤ªå°‘
    if (not all_results) and DDGS_AVAILABLE and DDGS is not None:
        try:
            ddgs_result = _search_with_duckduckgo(query)
            if ddgs_result and len(ddgs_result) > 50:
                all_results.append({
                    'source': 'DuckDuckGo',
                    'content': ddgs_result
                })
                sources_used.append('DuckDuckGo')
                logger.info(f"[Search] âœ… DuckDuckGo æœç´¢æˆåŠŸ: {query[:50]}...")
        except Exception as e:
            logger.info(f"[Search] DuckDuckGo æœç´¢å¤±è´¥: {e}")

    # 4. åˆå¹¶æ‰€æœ‰ç»“æœ
    if not all_results:
        return "Search error: æ‰€æœ‰æœç´¢æºå‡å¤±è´¥ï¼Œæ— æ³•è·å–æœç´¢ç»“æœã€‚"

    # åˆå¹¶ç»“æœ
    combined_result = _merge_search_results(all_results, query)

    logger.info(f"[Search] âœ… æœ€ç»ˆä½¿ç”¨ {len(sources_used)} ä¸ªæœç´¢æº: {', '.join(sources_used)}")
    return combined_result


def _search_with_duckduckgo(query: str) -> str:
    """ä½¿ç”¨ DuckDuckGo æœç´¢"""
    if not DDGS_AVAILABLE or DDGS is None:
        raise Exception("DuckDuckGo ä¸å¯ç”¨")
    
    for attempt in range(3):  # å¢åŠ é‡è¯•æ¬¡æ•°
        try:
            ddgs = DDGS()
            
            try:
                results = list(ddgs.text(query, max_results=10, safesearch='moderate'))
            except TypeError:
                results = list(ddgs.text(query, max_results=10))
            
            if not results:
                if attempt < 2:
                    time.sleep(2)
                    continue
                return None
            
            # éªŒè¯ç»“æœç›¸å…³æ€§
            query_lower = query.lower()
            relevant_results = []
            for res in results:
                title = res.get('title', '')
                body = res.get('body', '')
                title_lower = title.lower()
                body_lower = body.lower()
                
                query_words = [w for w in query_lower.split() if len(w) > 2 and w not in ['the', 'and', 'or', 'for', 'with', 'from']]
                is_relevant = any(word in title_lower or word in body_lower for word in query_words) if query_words else True
                
                if is_relevant or len(relevant_results) < 3:
                    relevant_results.append(res)
            
            if not relevant_results:
                if attempt < 2:
                    time.sleep(2)
                    continue
                relevant_results = results[:3]
            
            formatted = []
            for i, res in enumerate(relevant_results[:10], 1):
                title = res.get('title', 'No title')
                body = res.get('body', 'No summary')
                href = res.get('href', 'No link')
                
                title = title.encode('utf-8', 'ignore').decode('utf-8').strip()
                body = body.encode('utf-8', 'ignore').decode('utf-8').strip()
                
                if not title or not body:
                    continue
                    
                formatted.append(f"{i}. {title}\n   {body[:200]}...\n   {href}")
            
            if formatted:
                return "Search Results (DuckDuckGo):\n" + "\n\n".join(formatted)
            else:
                return None
                
        except Exception as e:
            if attempt < 2:
                time.sleep(2)
            else:
                raise e
    
    return None


def _merge_search_results(results: list, query: str) -> str:
    """
    åˆå¹¶å¤šä¸ªæœç´¢æºçš„ç»“æœ
    
    Args:
        results: æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'source' å’Œ 'content'
        query: åŸå§‹æŸ¥è¯¢
        
    Returns:
        åˆå¹¶åçš„æœç´¢ç»“æœæ–‡æœ¬
    """
    if not results:
        return "No search results found."
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªç»“æœï¼Œç›´æ¥è¿”å›
    if len(results) == 1:
        return results[0]['content']
    
    # åˆå¹¶å¤šä¸ªç»“æœ
    merged_parts = []
    merged_parts.append(f"ğŸ” ç»¼åˆæœç´¢ç»“æœ (æ¥è‡ª {len(results)} ä¸ªæ•°æ®æº):\n")
    merged_parts.append("=" * 60 + "\n\n")
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šExa > Wikipedia > Tavily > DuckDuckGo
    source_priority = {'Exa': 0, 'Wikipedia': 1, 'Tavily': 2, 'DuckDuckGo': 3}
    results_sorted = sorted(results, key=lambda x: source_priority.get(x['source'], 99))
    
    for i, result in enumerate(results_sorted, 1):
        source = result['source']
        content = result['content']
        
        merged_parts.append(f"ã€æ•°æ®æº {i}: {source}ã€‘\n")
        merged_parts.append("-" * 60 + "\n")
        
        # æå–ä¸»è¦å†…å®¹ï¼ˆå»é™¤æ ‡é¢˜å’Œæ ¼å¼ï¼‰
        if source == 'Wikipedia':
            # ç»´åŸºç™¾ç§‘ç»“æœå·²ç»æ ¼å¼åŒ–å¥½äº†
            merged_parts.append(content)
        elif source == 'Tavily':
            # Tavily ç»“æœä¹Ÿæ ¼å¼åŒ–å¥½äº†
            merged_parts.append(content)
        else:
            # DuckDuckGo ç»“æœ
            merged_parts.append(content)
        
        merged_parts.append("\n\n")
    
    merged_parts.append("=" * 60 + "\n")
    merged_parts.append(f"ğŸ’¡ æç¤º: ä»¥ä¸Šç»“æœæ¥è‡ªå¤šä¸ªæœç´¢æºï¼Œè¯·ç»¼åˆå‚è€ƒä»¥è·å¾—æœ€å‡†ç¡®çš„ä¿¡æ¯ã€‚\n")
    
    return "".join(merged_parts)


def _search_with_wikipedia(query: str) -> str:
    """
    ä½¿ç”¨ç»´åŸºç™¾ç§‘æœç´¢ï¼ˆå…è´¹ï¼Œä¸éœ€è¦API keyï¼‰
    
    ä¼˜å…ˆä½¿ç”¨ç»´åŸºç™¾ç§‘ï¼Œå› ä¸ºï¼š
    - å†…å®¹å‡†ç¡®ã€æƒå¨
    - ç»“æ„åŒ–ä¿¡æ¯
    - å…è´¹ï¼Œæ— é™åˆ¶
    - ç‰¹åˆ«é€‚åˆæŸ¥è¯¢æŒ‡æ•°æˆåˆ†è‚¡ã€å…¬å¸ä¿¡æ¯ç­‰
    """
    if not WIKIPEDIA_AVAILABLE or wikipedia is None:
        raise Exception("ç»´åŸºç™¾ç§‘ä¸å¯ç”¨ï¼ˆæœªå®‰è£… wikipediaï¼‰")
    
    try:
        # å°è¯•æœç´¢é¡µé¢ï¼ˆå¢åŠ æœç´¢ç»“æœæ•°é‡ï¼‰
        search_results = wikipedia.search(query, results=5)
        
        if not search_results:
            return None
        
        # å°è¯•å¤šä¸ªæœç´¢ç»“æœï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„
        best_result = None
        for page_title in search_results:
            try:
                page = wikipedia.page(page_title, auto_suggest=False)
                
                # è·å–é¡µé¢æ‘˜è¦å’Œä¸»è¦å†…å®¹
                summary = page.summary
                content = page.content[:5000]  # å¢åŠ å†…å®¹é•¿åº¦
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦ç›¸å…³ï¼ˆåŒ…å«æŸ¥è¯¢å…³é”®è¯ï¼‰
                query_lower = query.lower()
                content_lower = (summary + content).lower()
                
                # å¦‚æœå†…å®¹åŒ…å«æŸ¥è¯¢å…³é”®è¯ï¼Œè®¤ä¸ºæ˜¯ç›¸å…³ç»“æœ
                if any(keyword in content_lower for keyword in query_lower.split() if len(keyword) > 2):
                    best_result = {
                        'title': page_title,
                        'summary': summary,
                        'content': content,
                        'url': page.url
                    }
                    break
                    
            except wikipedia.exceptions.DisambiguationError as e:
                # å¦‚æœæœ‰æ­§ä¹‰ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªé€‰é¡¹
                if e.options:
                    try:
                        page = wikipedia.page(e.options[0], auto_suggest=False)
                        summary = page.summary
                        content = page.content[:5000]
                        best_result = {
                            'title': e.options[0],
                            'summary': summary,
                            'content': content,
                            'url': page.url
                        }
                        break
                    except:
                        continue
                        
            except wikipedia.exceptions.PageError:
                continue
            except Exception as e:
                logger.info(f"[Search] ç»´åŸºç™¾ç§‘è·å–é¡µé¢ {page_title} å¤±è´¥: {e}")
                continue
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç›¸å…³ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
        if not best_result and search_results:
            try:
                page = wikipedia.page(search_results[0], auto_suggest=False)
                best_result = {
                    'title': search_results[0],
                    'summary': page.summary,
                    'content': page.content[:5000],
                    'url': page.url
                }
            except:
                return None
        
        if best_result:
            # æ ¼å¼åŒ–ç»“æœ
            result = f"""Wikipedia Results for "{best_result['title']}":

Summary:
{best_result['summary']}

Detailed Information:
{best_result['content']}

URL: {best_result['url']}"""
            return result
        
        return None
            
    except Exception as e:
        logger.info(f"[Search] ç»´åŸºç™¾ç§‘æœç´¢å‡ºé”™: {e}")
        return None


def _search_with_tavily(query: str) -> str:
    """
    ä½¿ç”¨ Tavily Search API è¿›è¡ŒAIæœç´¢

    Tavily æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºAIåº”ç”¨è®¾è®¡çš„æœç´¢APIï¼Œæä¾›ï¼š
    - æ›´å‡†ç¡®çš„æœç´¢ç»“æœ
    - ç»“æ„åŒ–çš„æ•°æ®æ ¼å¼
    - æ›´å¥½çš„ä¸Šä¸‹æ–‡ç†è§£
    """
    if not TAVILY_API_KEY:
        raise Exception("Tavily API key not configured")

    if not TAVILY_AVAILABLE or TavilyClient is None:
        raise Exception("Tavily å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼ˆæœªå®‰è£… tavily-pythonï¼‰")

    try:
        client = TavilyClient(api_key=TAVILY_API_KEY)

        # æ‰§è¡Œæœç´¢
        response = client.search(
            query=query,
            search_depth="advanced",  # basic æˆ– advanced
            max_results=10,
            include_answer=True,  # åŒ…å«AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦
            include_raw_content=False,  # ä¸åŒ…å«åŸå§‹å†…å®¹ï¼ˆèŠ‚çœtokenï¼‰
        )

        # æ ¼å¼åŒ–ç»“æœ
        formatted = []

        # å¦‚æœæœ‰AIç”Ÿæˆçš„ç­”æ¡ˆï¼Œä¼˜å…ˆæ˜¾ç¤º
        if response.get('answer'):
            formatted.append(f"ğŸ“Š AIæ‘˜è¦:\n{response['answer']}\n")

        # æ˜¾ç¤ºæœç´¢ç»“æœ
        results = response.get('results', [])
        if results:
            formatted.append("æœç´¢ç»“æœ:")
            for i, res in enumerate(results, 1):
                title = res.get('title', 'No title')
                content = res.get('content', 'No content')
                url = res.get('url', 'No link')
                score = res.get('score', 0)

                formatted.append(
                    f"{i}. {title} (ç›¸å…³æ€§: {score:.2f})\n"
                    f"   {content[:200]}...\n"
                    f"   {url}"
                )
        else:
            formatted.append("æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœã€‚")

        return "\n\n".join(formatted)

    except Exception as e:
        error_msg = str(e) if e else "æœªçŸ¥é”™è¯¯"
        error_type = type(e).__name__
        logger.info(f"[Search] Tavily API é”™è¯¯ ({error_type}): {error_msg}")

        # å¦‚æœæ˜¯ API key ç›¸å…³é”™è¯¯ï¼Œç»™å‡ºæ›´æ˜ç¡®çš„æç¤º
        if "api" in error_msg.lower() or "key" in error_msg.lower() or "auth" in error_msg.lower():
            logger.info(f"[Search] æç¤º: è¯·æ£€æŸ¥ TAVILY_API_KEY æ˜¯å¦æ­£ç¡®é…ç½®")

        raise Exception(f"Tavily API é”™è¯¯: {error_msg}")


def _search_with_exa(query: str) -> str:
    """
    ä½¿ç”¨ Exa Search API è¿›è¡Œè¯­ä¹‰æœç´¢

    Exa æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºAIåº”ç”¨è®¾è®¡çš„è¯­ä¹‰æœç´¢APIï¼Œæä¾›ï¼š
    - ç¥ç»ç½‘ç»œé©±åŠ¨çš„è¯­ä¹‰æœç´¢
    - é«˜è´¨é‡çš„å†…å®¹æå–
    - æ›´å¥½çš„ä¸Šä¸‹æ–‡ç†è§£
    """
    if not EXA_API_KEY:
        raise Exception("Exa API key not configured")

    if not EXA_AVAILABLE or Exa is None:
        raise Exception("Exa å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼ˆæœªå®‰è£… exa-pyï¼‰")

    try:
        exa = Exa(api_key=EXA_API_KEY)

        # æ‰§è¡Œæœç´¢
        response = exa.search_and_contents(
            query=query,
            type="neural",  # neural æˆ– keyword
            num_results=10,
            text=True,  # åŒ…å«æ–‡æœ¬å†…å®¹
            highlights=True,  # åŒ…å«é«˜äº®ç‰‡æ®µ
        )

        # æ ¼å¼åŒ–ç»“æœ
        formatted = []
        formatted.append("Search Results (Exa):")

        if response.results:
            for i, res in enumerate(response.results, 1):
                title = res.title or 'No title'
                url = res.url or 'No link'

                # è·å–é«˜äº®æˆ–æ–‡æœ¬å†…å®¹
                content = ""
                if hasattr(res, 'highlights') and res.highlights:
                    content = " ".join(res.highlights[:2])
                elif hasattr(res, 'text') and res.text:
                    content = res.text[:300]

                published = (
                    getattr(res, "published_date", None)
                    or getattr(res, "published_at", None)
                    or getattr(res, "date", None)
                    or getattr(res, "created_at", None)
                )
                date_str = _normalize_published_date(published)
                if date_str:
                    content = f"{date_str} {content}".strip()

                formatted.append(
                    f"{i}. {title}\n"
                    f"   {content}...\n"
                    f"   {url}"
                )

            return "\n\n".join(formatted)
        else:
            return None

    except Exception as e:
        raise Exception(f"Exa search failed: {str(e)}")

# ============================================
# è‚¡ä»·è·å– - å¤šæ•°æ®æºç­–ç•¥
# ============================================

def _fetch_with_alpha_vantage(ticker: str):
    """ä¼˜å…ˆæ–¹æ¡ˆï¼šä½¿ç”¨ Alpha Vantage API è·å–å®æ—¶è‚¡ä»·"""
    logger.info(f"  - Attempting Alpha Vantage API for {ticker}...")
    try:
        url = "https://www.alphavantage.co/query"
        params = {
            'function': 'GLOBAL_QUOTE',
            'symbol': ticker,
            'apikey': ALPHA_VANTAGE_API_KEY
        }
        response = _http_get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if 'Global Quote' in data and data['Global Quote']:
            quote = data['Global Quote']
            price = float(quote.get('05. price', 0))
            change = float(quote.get('09. change', 0))
            change_percent_str = quote.get('10. change percent', '0%').replace('%', '')
            
            if price > 0 and change_percent_str:
                change_percent = float(change_percent_str)
                return f"{ticker} Current Price: ${price:.2f} | Change: ${change:.2f} ({change_percent:+.2f}%)"
        
        if 'Note' in data or 'Information' in data:
            logger.info(f"  - Alpha Vantage note: {data.get('Note') or data.get('Information')}")
        if 'Error Message' in data:
            logger.info(f"  - Alpha Vantage error: {data['Error Message']}")
            
        return None
    except Exception as e:
        logger.info(f"  - Alpha Vantage exception: {e}")
        return None

def _fetch_with_finnhub(ticker: str):
    """æ–°å¢ï¼šä½¿ç”¨ Finnhub API è·å–å®æ—¶è‚¡ä»·"""
    if not finnhub_client:
        return None
    logger.info(f"  - Attempting Finnhub API for {ticker}...")
    try:
        quote = finnhub_client.quote(ticker)
        if quote and quote.get('c') is not None and quote.get('c') != 0:
            price = quote['c']
            change = quote.get('d', 0.0)
            change_percent = quote.get('dp', 0.0)
            return f"{ticker} Current Price: ${price:.2f} | Change: ${change:.2f} ({change_percent:+.2f}%)"
        return None
    except Exception as e:
        logger.info(f"  - Finnhub quote exception: {e}")
        return None

def _fetch_with_yfinance(ticker: str):
    """å°è¯•ä½¿ç”¨ yfinance è·å–ä»·æ ¼"""
    logger.info(f"  - Attempting yfinance for {ticker}...")
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="5d")
        if hist.empty or len(hist) < 2:
            return None
        
        current_price = hist['Close'].iloc[-1]
        prev_close = hist['Close'].iloc[-2]
        change = current_price - prev_close
        change_percent = (change / prev_close) * 100
        
        return f"{ticker} Current Price: ${current_price:.2f} | Change: ${change:.2f} ({change_percent:+.2f}%)"
    except Exception as e:
        logger.info(f"  - yfinance exception: {e}")
        return None


def _fetch_with_twelve_data_price(ticker: str):
    """å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ Twelve Data è·å–å®æ—¶ä»·æ ¼"""
    if not TWELVE_DATA_API_KEY:
        return None
    logger.info(f"  - Attempting Twelve Data for {ticker}...")
    try:
        params = {
            "symbol": ticker,
            "interval": "1day",
            "outputsize": 2,  # æœ€æ–°ä¸¤å¤©è®¡ç®—æ¶¨è·Œå¹…
            "apikey": TWELVE_DATA_API_KEY,
            "order": "desc",
        }
        response = _http_get("https://api.twelvedata.com/time_series", params=params, timeout=10)
        if response.status_code != 200:
            return None

        data = response.json()
        if data.get("status") != "ok" or not data.get("values"):
            # Twelve Data è¿”å› {"status": "error", "message": "..."} æ—¶ä¹Ÿèµ°å…œåº•
            return None

        values = data.get("values", [])
        latest = values[0] if values else None
        if not latest:
            return None

        price = float(latest.get("close", 0) or 0)
        if price <= 0:
            return None

        prev_close = None
        if len(values) > 1 and values[1].get("close"):
            prev_close = float(values[1]["close"])

        change = None
        change_percent = None
        if prev_close and prev_close != 0:
            change = price - prev_close
            change_percent = (change / prev_close) * 100.0

        msg = f"{ticker} Current Price: ${price:.2f}"
        if change is not None and change_percent is not None:
            msg += f" | Change: {change:+.2f} ({change_percent:+.2f}%)"
        return msg
    except Exception as e:
        logger.info(f"  - Twelve Data price exception: {e}")
        return None

def _fetch_yahoo_api_v8(ticker: str):
    """Yahoo Finance API v8 - å…è´¹ JSON APIï¼Œæ— éœ€ API keyï¼Œæ¯”çˆ¬è™«æ›´ç¨³å®š"""
    logger.info(f"  - Attempting Yahoo Finance API v8 for {ticker}...")
    try:
        url = f"https://query2.finance.yahoo.com/v8/finance/chart/{ticker}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = _http_get(url, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()

        result = data.get('chart', {}).get('result', [])
        if not result:
            return None

        meta = result[0].get('meta', {})
        price = meta.get('regularMarketPrice')
        prev_close = meta.get('previousClose') or meta.get('chartPreviousClose')

        if not price:
            return None

        change = None
        change_percent = None
        if prev_close and prev_close != 0:
            change = price - prev_close
            change_percent = (change / prev_close) * 100.0

        msg = f"{ticker} Current Price: ${price:.2f}"
        if change is not None and change_percent is not None:
            msg += f" | Change: {change:+.2f} ({change_percent:+.2f}%)"
        return msg
    except Exception as e:
        logger.info(f"  - Yahoo API v8 exception: {e}")
        return None


def _scrape_google_finance(ticker: str):
    """Google Finance çˆ¬è™« - å…è´¹ï¼Œæ— éœ€ API key"""
    logger.info(f"  - Attempting Google Finance for {ticker}...")
    try:
        # å°è¯•ä¸åŒäº¤æ˜“æ‰€
        exchanges = ['NASDAQ', 'NYSE', 'NYSEARCA', '']
        for exchange in exchanges:
            if exchange:
                url = f"https://www.google.com/finance/quote/{ticker}:{exchange}"
            else:
                url = f"https://www.google.com/finance/quote/{ticker}"

            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = _http_get(url, headers=headers, timeout=10)

            if response.status_code == 200:
                # è§£æä»·æ ¼ - Google Finance ä½¿ç”¨ data-last-price å±æ€§
                match = re.search(r'data-last-price="([0-9.]+)"', response.text)
                if match:
                    price = float(match.group(1))
                    # å°è¯•è·å–å˜åŠ¨
                    change_match = re.search(r'data-price-change="([+-]?[0-9.]+)"', response.text)
                    pct_match = re.search(r'data-price-change-percent="([+-]?[0-9.]+)"', response.text)

                    msg = f"{ticker} Current Price: ${price:.2f}"
                    if change_match and pct_match:
                        change = float(change_match.group(1))
                        pct = float(pct_match.group(1))
                        msg += f" | Change: {change:+.2f} ({pct:+.2f}%)"
                    return msg
        return None
    except Exception as e:
        logger.info(f"  - Google Finance exception: {e}")
        return None


def _scrape_cnbc(ticker: str):
    """CNBC çˆ¬è™« - å…è´¹ï¼Œå®æ—¶æ€§å¥½"""
    logger.info(f"  - Attempting CNBC for {ticker}...")
    try:
        url = f"https://www.cnbc.com/quotes/{ticker}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = _http_get(url, headers=headers, timeout=10)
        response.raise_for_status()

        # CNBC åœ¨ JSON-LD ä¸­åŒ…å«ä»·æ ¼æ•°æ®
        match = re.search(r'"price":\s*"?([0-9.]+)"?', response.text)
        if match:
            price = float(match.group(1))
            # å°è¯•è·å–å˜åŠ¨
            change_match = re.search(r'"priceChange":\s*"?([+-]?[0-9.]+)"?', response.text)
            pct_match = re.search(r'"priceChangePercent":\s*"?([+-]?[0-9.]+)"?', response.text)

            msg = f"{ticker} Current Price: ${price:.2f}"
            if change_match and pct_match:
                change = float(change_match.group(1))
                pct = float(pct_match.group(1))
                msg += f" | Change: {change:+.2f} ({pct:+.2f}%)"
            return msg
        return None
    except Exception as e:
        logger.info(f"  - CNBC exception: {e}")
        return None


def _fetch_with_pandas_datareader(ticker: str):
    """pandas_datareader - å…è´¹ï¼Œæ”¯æŒå¤šæ•°æ®æº"""
    logger.info(f"  - Attempting pandas_datareader for {ticker}...")
    try:
        import pandas_datareader as pdr
        from datetime import datetime, timedelta

        end = datetime.now()
        start = end - timedelta(days=5)

        # å°è¯• stooq æ•°æ®æºï¼ˆå…è´¹ï¼‰
        df = pdr.get_data_stooq(ticker, start, end)
        if not df.empty:
            price = df['Close'].iloc[0]
            if len(df) > 1:
                prev = df['Close'].iloc[1]
                change = price - prev
                pct = (change / prev) * 100
                return f"{ticker} Current Price: ${price:.2f} | Change: {change:+.2f} ({pct:+.2f}%)"
            return f"{ticker} Current Price: ${price:.2f}"
        return None
    except ImportError:
        logger.info(f"  - pandas_datareader not installed")
        return None
    except Exception as e:
        logger.info(f"  - pandas_datareader exception: {e}")
        return None

def _scrape_yahoo_finance(ticker: str):
    """å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥çˆ¬å– Yahoo Finance é¡µé¢"""
    logger.info(f"  - Attempting to scrape Yahoo Finance for {ticker}...")
    try:
        url = f"https://finance.yahoo.com/quote/{ticker}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = _http_get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        price_elem = soup.find('fin-streamer', {'data-symbol': ticker, 'data-field': 'regularMarketPrice'})
        change_elem = soup.find('fin-streamer', {'data-symbol': ticker, 'data-field': 'regularMarketChange'})
        change_percent_elem = soup.find('fin-streamer', {'data-symbol': ticker, 'data-field': 'regularMarketChangePercent'})
        
        if price_elem and change_elem and change_percent_elem:
            price = price_elem.get('value')
            change = change_elem.get('value')
            change_percent = change_percent_elem.get('value')
            
            if price and change and change_percent:
                return f"{ticker} Current Price: ${float(price):.2f} | Change: ${float(change):.2f} ({float(change_percent)*100:+.2f}%)"
        
        return None
    except Exception as e:
        logger.info(f"  - Yahoo scraping exception: {e}")
        return None


def _fetch_index_price(ticker: str):
    """
    æŒ‡æ•°ä¸“ç”¨ï¼šä¼˜å…ˆ yfinance.download è·å–æœ€è¿‘ä¸¤æ—¥æ”¶ç›˜ï¼Œå¤±è´¥å†ç”¨ Stooq/æœç´¢å…œåº•ã€‚
    """
    if not ticker.startswith('^'):
        return None
    logger.info(f"  - Attempting index price via yfinance.download for {ticker}...")
    try:
        hist = yf.download(ticker, period="3d", interval="1d", progress=False, timeout=20)
        if not hist.empty and len(hist) > 0:
            closes = hist['Close'].dropna().tolist()
            if closes:
                current_price = closes[-1]
                prev_close = closes[-2] if len(closes) > 1 else None
                change = current_price - prev_close if prev_close else None
                change_pct = (change / prev_close) * 100 if prev_close else None
                msg = f"{ticker} Current Price: ${current_price:.2f}"
                if change is not None and change_pct is not None:
                    msg += f" | Change: {change:+.2f} ({change_pct:+.2f}%)"
                return msg
    except Exception as e:
        logger.info(f"  - Index price via yfinance failed: {e}")
    # Fallback 1: Stooq å…è´¹æ¥å£
    stooq_result = _fetch_with_stooq_price(ticker)
    if stooq_result:
        return stooq_result
    # Fallback 2: æœç´¢å…œåº•
    try:
        price_val = _fallback_price_value(ticker)
        if price_val:
            return f"{ticker} Current Price: ${price_val:.2f}"
    except Exception:
        pass
    return None

def _search_for_price(ticker: str):
    """æœ€åæ‰‹æ®µï¼šä½¿ç”¨æœç´¢å¼•æ“å¹¶ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æä»·æ ¼"""
    logger.info(f"  - Attempting to find price via search for {ticker}...")
    try:
        search_result = search(f"{ticker} stock price today")
        patterns = [
            r'\$(\d{1,5}(?:,\d{3})*\.\d{2})',
            r'(?:Price|price)[:\s]+\$?(\d{1,5}(?:,\d{3})*\.\d{2})',
            r'(\d{1,5}(?:,\d{3})*\.\d{2})\s*USD'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, search_result)
            if match:
                price = match.group(1).replace(',', '')
                price_val = float(price)
                if price_val <= 0 or price_val > 1e8:
                    return None
                from datetime import date
                today = date.today().isoformat()
                return f"{ticker} Current Price (via search): ${price_val:.2f} (as of {today})"
        
        return None
    except Exception as e:
        logger.info(f"  - Search price exception: {e}")
        return None

def _fetch_with_stooq_price(ticker: str):
    """
    ä½¿ç”¨ stooq å…è´¹æ¥å£è·å–æœ€æ–°æ”¶ç›˜ä»·ï¼ˆå… Keyï¼‰ï¼Œæ”¯æŒéƒ¨åˆ†æŒ‡æ•°å’Œç¾è‚¡ã€‚
    """
    try:
        symbol = _map_to_stooq_symbol(ticker)
        if not symbol:
            return None
        url = f"https://stooq.pl/q/l/?s={symbol}&f=sd2t2ohlcv&h&e=json"
        resp = _http_get(url, timeout=8)
        data = resp.json().get("symbols") if resp.status_code == 200 else None
        if not data:
            return None
        item = data[0]
        close = item.get("close")
        open_ = item.get("open")
        if close in (None, "N/D"):
            return None
        price = float(close)
        change = None
        change_percent = None
        if open_ not in (None, "N/D", 0):
            prev = float(open_)
            change = price - prev
            if prev:
                change_percent = (change / prev) * 100.0
        return f"{ticker} Current Price: ${price:.2f}" + (
            f" | Change: {change:+.2f} ({change_percent:+.2f}%)" if change is not None else ""
        )
    except Exception as e:
        logger.info(f"  - Stooq price exception: {e}")
        return None

def get_stock_price(ticker: str) -> str:
    """
    ä½¿ç”¨å¤šæ•°æ®æºç­–ç•¥è·å–è‚¡ç¥¨ä»·æ ¼ï¼Œä»¥æé«˜ç¨³å®šæ€§ã€‚
    æ ¹æ®èµ„äº§ç±»å‹é€‰æ‹©ä¸åŒçš„æ•°æ®æºç­–ç•¥ã€‚
    """
    logger.info(f"Fetching price for {ticker} with multi-source strategy...")
    upper = ticker.upper()

    # åˆ¤æ–­èµ„äº§ç±»å‹
    is_index = ticker.startswith('^')
    is_crypto = any(crypto in upper for crypto in ['BTC', 'ETH', 'USDT', 'BNB', 'XRP', 'SOL', 'DOGE', 'ADA']) and '-' in upper
    is_china = upper.endswith('.SS') or upper.endswith('.SZ') or upper.startswith('000') or upper.startswith('600') or upper.startswith('300')
    is_commodity = '=' in upper  # GC=F, CL=F, SI=F

    # æ ¹æ®èµ„äº§ç±»å‹é€‰æ‹©æ•°æ®æº
    if is_crypto:
        # åŠ å¯†è´§å¸ï¼šåªç”¨ yfinance å’Œæœç´¢
        sources = [
            _fetch_with_yfinance,
            _fetch_yahoo_api_v8,
            _search_for_price
        ]
    elif is_china:
        # Aè‚¡ï¼šåªç”¨ yfinance å’Œæœç´¢ï¼ˆå…¶ä»–æºä¸æ”¯æŒï¼‰
        sources = [
            _fetch_with_yfinance,
            _fetch_yahoo_api_v8,
            _search_for_price
        ]
    elif is_commodity:
        # å•†å“æœŸè´§ï¼šåªç”¨ yfinance å’Œæœç´¢
        sources = [
            _fetch_with_yfinance,
            _fetch_yahoo_api_v8,
            _search_for_price
        ]
    elif is_index:
        sources = [
            _fetch_yahoo_api_v8,
            _fetch_index_price,
            _fetch_with_stooq_price,
            _search_for_price
        ]
    else:
        # æ™®é€šç¾è‚¡
        sources = [
            _fetch_yahoo_api_v8,
            _scrape_google_finance,
            _fetch_with_stooq_price,
            _scrape_cnbc,
            _fetch_with_pandas_datareader,
            _fetch_with_yfinance,
            _fetch_with_alpha_vantage,
            _fetch_with_finnhub,
            _fetch_with_twelve_data_price,
            _scrape_yahoo_finance,
            _search_for_price
        ]
    
    for i, source_func in enumerate(sources, 1):
        try:
            result = source_func(ticker)
            if result:
                logger.info(f"  OK source #{i} ({source_func.__name__})")
                # è¿½åŠ ä¸¤æ¡£åˆ†æ‰¹ä»·ï¼Œä¿è¯æœ‰å…·ä½“æ•°å­—
                price_num = None
                import re
                m = re.search(r"\$([0-9]+(?:\.[0-9]+)?)", result)
                if m:
                    try:
                        price_num = float(m.group(1))
                    except Exception:
                        price_num = None
                if price_num:
                    p1 = price_num * 0.99
                    p2 = price_num * 0.98
                result = f"{result} | Suggested ladder: ${p1:.2f} / ${p2:.2f} (+/-1% / +/-2% from current)"
                return result
            time.sleep(0.5)
        except Exception as e:
            logger.info(f"  FAIL source #{i} ({source_func.__name__}) failed: {e}")
            continue
            
    return f"Error: All data sources failed to retrieve the price for {ticker}. Please try again later."

# ============================================
# å…¬å¸ä¿¡æ¯è·å–
# ============================================

def get_financial_statements(ticker: str) -> dict:
    """
    è·å–å…¬å¸çš„è´¢åŠ¡æŠ¥è¡¨æ•°æ®ï¼ˆè´¢æŠ¥ï¼‰
    åŒ…æ‹¬ï¼šæŸç›Šè¡¨ã€èµ„äº§è´Ÿå€ºè¡¨ã€ç°é‡‘æµé‡è¡¨
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç 
        
    Returns:
        dict: åŒ…å« financials, balance_sheet, cashflow çš„å­—å…¸
    """
    try:
        stock = yf.Ticker(ticker)
        
        result = {
            'ticker': ticker,
            'timestamp': datetime.now().isoformat(),
            'financials': None,
            'balance_sheet': None,
            'cashflow': None,
            'error': None
        }
        
        # 1. è·å–æŸç›Šè¡¨ï¼ˆIncome Statementï¼‰
        try:
            financials = stock.financials
            if not financials.empty:
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œä¾¿äºJSONåºåˆ—åŒ–
                result['financials'] = {
                    'columns': financials.columns.tolist(),
                    'index': financials.index.tolist(),
                    'data': financials.to_dict('records')
                }
                logger.info(f"[Financials] âœ… æˆåŠŸè·å– {ticker} æŸç›Šè¡¨æ•°æ®")
        except Exception as e:
            logger.info(f"[Financials] è·å–æŸç›Šè¡¨å¤±è´¥: {e}")
            result['error'] = f"è·å–æŸç›Šè¡¨å¤±è´¥: {str(e)}"
        
        # 2. è·å–èµ„äº§è´Ÿå€ºè¡¨ï¼ˆBalance Sheetï¼‰
        try:
            balance_sheet = stock.balance_sheet
            if not balance_sheet.empty:
                result['balance_sheet'] = {
                    'columns': balance_sheet.columns.tolist(),
                    'index': balance_sheet.index.tolist(),
                    'data': balance_sheet.to_dict('records')
                }
                logger.info(f"[Financials] âœ… æˆåŠŸè·å– {ticker} èµ„äº§è´Ÿå€ºè¡¨æ•°æ®")
        except Exception as e:
            logger.info(f"[Financials] è·å–èµ„äº§è´Ÿå€ºè¡¨å¤±è´¥: {e}")
            if not result['error']:
                result['error'] = f"è·å–èµ„äº§è´Ÿå€ºè¡¨å¤±è´¥: {str(e)}"
        
        # 3. è·å–ç°é‡‘æµé‡è¡¨ï¼ˆCash Flowï¼‰
        try:
            cashflow = stock.cashflow
            if not cashflow.empty:
                result['cashflow'] = {
                    'columns': cashflow.columns.tolist(),
                    'index': cashflow.index.tolist(),
                    'data': cashflow.to_dict('records')
                }
                logger.info(f"[Financials] âœ… æˆåŠŸè·å– {ticker} ç°é‡‘æµé‡è¡¨æ•°æ®")
        except Exception as e:
            logger.info(f"[Financials] è·å–ç°é‡‘æµé‡è¡¨å¤±è´¥: {e}")
            if not result['error']:
                result['error'] = f"è·å–ç°é‡‘æµé‡è¡¨å¤±è´¥: {str(e)}"
        
        # å¦‚æœæ‰€æœ‰æ•°æ®éƒ½è·å–å¤±è´¥ï¼Œè¿”å›é”™è¯¯
        if not result['financials'] and not result['balance_sheet'] and not result['cashflow']:
            result['error'] = "æ— æ³•è·å–ä»»ä½•è´¢æŠ¥æ•°æ®ï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨ä»£ç æ˜¯å¦æ­£ç¡®"
        
        return result
        
    except Exception as e:
        logger.info(f"[Financials] è·å–è´¢æŠ¥æ•°æ®å¤±è´¥: {e}")
        return {
            'ticker': ticker,
            'timestamp': datetime.now().isoformat(),
            'financials': None,
            'balance_sheet': None,
            'cashflow': None,
            'error': f"è·å–è´¢æŠ¥æ•°æ®å¤±è´¥: {str(e)}"
        }

def get_financial_statements_summary(ticker: str) -> str:
    """
    è·å–è´¢æŠ¥æ•°æ®å¹¶æ ¼å¼åŒ–ä¸ºå¯è¯»çš„æ–‡æœ¬æ‘˜è¦
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç 
        
    Returns:
        str: æ ¼å¼åŒ–çš„è´¢æŠ¥æ‘˜è¦æ–‡æœ¬
    """
    data = get_financial_statements(ticker)
    
    if data.get('error'):
        return f"æ— æ³•è·å– {ticker} çš„è´¢æŠ¥æ•°æ®: {data['error']}"
    
    summary_parts = [f"ğŸ“Š {ticker} è´¢åŠ¡æŠ¥è¡¨æ‘˜è¦\n"]
    summary_parts.append("=" * 50 + "\n")
    
    # æŸç›Šè¡¨æ‘˜è¦
    if data.get('financials'):
        financials = data['financials']
        summary_parts.append("\nğŸ“ˆ æŸç›Šè¡¨ (Income Statement):\n")
        summary_parts.append("-" * 50 + "\n")
        
        # è·å–æœ€æ–°å¹´ä»½çš„æ•°æ®
        if financials.get('columns') and len(financials['columns']) > 0:
            latest_year = financials['columns'][0]
            summary_parts.append(f"æœ€æ–°è´¢æŠ¥æ—¥æœŸ: {latest_year}\n\n")
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            key_metrics = ['Total Revenue', 'Net Income', 'Operating Income', 'EBIT', 'Gross Profit']
            for metric in key_metrics:
                # åœ¨ index ä¸­æŸ¥æ‰¾
                if financials.get('index'):
                    for idx, row_name in enumerate(financials['index']):
                        if metric.lower() in str(row_name).lower():
                            # ä» data ä¸­è·å–å€¼
                            if financials.get('data') and len(financials['data']) > idx:
                                value = financials['data'][idx].get(latest_year, 'N/A')
                                if value != 'N/A' and value is not None:
                                    formatted_value = f"${value/1e9:.2f}B" if abs(value) >= 1e9 else f"${value/1e6:.2f}M"
                                    summary_parts.append(f"  {row_name}: {formatted_value}\n")
    
    # èµ„äº§è´Ÿå€ºè¡¨æ‘˜è¦
    if data.get('balance_sheet'):
        balance_sheet = data['balance_sheet']
        summary_parts.append("\nğŸ’° èµ„äº§è´Ÿå€ºè¡¨ (Balance Sheet):\n")
        summary_parts.append("-" * 50 + "\n")
        
        if balance_sheet.get('columns') and len(balance_sheet['columns']) > 0:
            latest_year = balance_sheet['columns'][0]
            summary_parts.append(f"æœ€æ–°è´¢æŠ¥æ—¥æœŸ: {latest_year}\n\n")
            
            key_metrics = ['Total Assets', 'Total Liabilities', 'Total Stockholder Equity', 'Cash And Cash Equivalents']
            for metric in key_metrics:
                if balance_sheet.get('index'):
                    for idx, row_name in enumerate(balance_sheet['index']):
                        if metric.lower() in str(row_name).lower():
                            if balance_sheet.get('data') and len(balance_sheet['data']) > idx:
                                value = balance_sheet['data'][idx].get(latest_year, 'N/A')
                                if value != 'N/A' and value is not None:
                                    formatted_value = f"${value/1e9:.2f}B" if abs(value) >= 1e9 else f"${value/1e6:.2f}M"
                                    summary_parts.append(f"  {row_name}: {formatted_value}\n")
    
    # ç°é‡‘æµé‡è¡¨æ‘˜è¦
    if data.get('cashflow'):
        cashflow = data['cashflow']
        summary_parts.append("\nğŸ’µ ç°é‡‘æµé‡è¡¨ (Cash Flow):\n")
        summary_parts.append("-" * 50 + "\n")
        
        if cashflow.get('columns') and len(cashflow['columns']) > 0:
            latest_year = cashflow['columns'][0]
            summary_parts.append(f"æœ€æ–°è´¢æŠ¥æ—¥æœŸ: {latest_year}\n\n")
            
            key_metrics = ['Operating Cash Flow', 'Free Cash Flow', 'Capital Expenditure']
            for metric in key_metrics:
                if cashflow.get('index'):
                    for idx, row_name in enumerate(cashflow['index']):
                        if metric.lower() in str(row_name).lower():
                            if cashflow.get('data') and len(cashflow['data']) > idx:
                                value = cashflow['data'][idx].get(latest_year, 'N/A')
                                if value != 'N/A' and value is not None:
                                    formatted_value = f"${value/1e9:.2f}B" if abs(value) >= 1e9 else f"${value/1e6:.2f}M"
                                    summary_parts.append(f"  {row_name}: {formatted_value}\n")
    
    return "".join(summary_parts)

def get_company_info(ticker: str) -> str:
    """
    ä»å¤šä¸ªæ¥æºè·å–å…¬å¸èµ„æ–™ä¿¡æ¯ã€‚
    ä¼˜å…ˆä½¿ç”¨ yfinanceï¼Œå¤±è´¥æ—¶å›é€€åˆ° Finnhub, Alpha Vantage æˆ–ç½‘é¡µæœç´¢ã€‚
    """
    # æ–¹æ³•1: yfinance
    try:
        stock = yf.Ticker(ticker)
        info = stock.info
        if info and 'longName' in info:
            summary = info.get('longBusinessSummary', '')
            description = (summary[:200] + '...') if summary else 'No description available'
            return f"""Company Profile ({ticker}):
- Name: {info.get('longName', 'Unknown')}
- Sector: {info.get('sector', 'Unknown')}
- Industry: {info.get('industry', 'Unknown')}
- Market Cap: ${info.get('marketCap', 0):,.0f}
- Website: {info.get('website', 'N/A')}
- Description: {description}"""
    except Exception as e:
        logger.info(f"yfinance info fetch for '{ticker}' failed: {e}")

    # æ–¹æ³•2: Finnhub (æ–°å¢)
    if finnhub_client:
        try:
            logger.info(f"Trying Finnhub for company info: {ticker}")
            profile = finnhub_client.company_profile2(symbol=ticker)
            if profile and 'name' in profile:
                return f"""Company Profile ({ticker}):
- Name: {profile.get('name', 'Unknown')}
- Sector: {profile.get('finnhubIndustry', 'Unknown')}
- Market Cap: ${int(profile.get('marketCapitalization', 0) * 1_000_000):,}
- Website: {profile.get('weburl', 'N/A')}
- Description: Search online for more details.""" # Finnhub profile doesn't include a long description
        except Exception as e:
            logger.info(f"Finnhub profile fetch failed: {e}")
    
    # æ–¹æ³•3: Alpha Vantage
    try:
        logger.info(f"Trying Alpha Vantage for company info: {ticker}")
        url = "https://www.alphavantage.co/query"
        params = {'function': 'OVERVIEW', 'symbol': ticker, 'apikey': ALPHA_VANTAGE_API_KEY}
        response = _http_get(url, params=params, timeout=10)
        data = response.json()
        if 'Symbol' in data and data['Symbol']:
            description = data.get('Description', 'No description')[:200] + '...'
            return f"""Company Profile ({ticker}):
- Name: {data.get('Name', 'Unknown')}
- Sector: {data.get('Sector', 'Unknown')}
- Industry: {data.get('Industry', 'Unknown')}
- Market Cap: ${int(data.get('MarketCapitalization', 0)):,}
- Description: {description}"""
    except Exception as e:
        logger.info(f"Alpha Vantage overview fetch failed: {e}")
    
    # æ–¹æ³•4: ç½‘é¡µæœç´¢
    logger.info(f"Falling back to web search for '{ticker}' company info")
    return search(f"{ticker} company profile stock information")

# ============================================
# æ–°é—»è·å–
# ============================================

def resolve_company_ticker(company: str, limit: int = 5) -> Dict[str, Any]:
    """Resolve a company name to tickers using OpenFIGI/Finnhub/EODHD/search."""
    if not company:
        return {"query": company, "source": "none", "matches": []}

    matches: List[Dict[str, Any]] = []
    sources: List[str] = []
    seen = set()

    def _append_matches(items: List[Dict[str, Any]], source: str) -> None:
        if not items:
            return
        if source not in sources:
            sources.append(source)
        for item in items:
            symbol = item.get("symbol") if isinstance(item, dict) else None
            if not symbol or symbol in seen:
                continue
            matches.append(item)
            seen.add(symbol)
            if len(matches) >= limit:
                return

    if OPENFIGI_API_KEY:
        try:
            _append_matches(_openfigi_symbol_lookup(company, limit), "openfigi")
        except Exception as e:
            logger.info(f"OpenFIGI lookup failed for {company}: {e}")

    if len(matches) < limit and finnhub_client:
        try:
            lookup = finnhub_client.symbol_lookup(company)
            results = lookup.get("result", []) if isinstance(lookup, dict) else []
            finnhub_matches = []
            for item in results:
                symbol = item.get("displaySymbol") or item.get("symbol")
                if not symbol:
                    continue
                finnhub_matches.append({
                    "symbol": symbol,
                    "description": item.get("description") or "",
                    "type": item.get("type") or "",
                    "primaryExchange": item.get("primaryExchange") or item.get("exchange") or "",
                    "source": "finnhub",
                })
            _append_matches(finnhub_matches, "finnhub")
        except Exception as e:
            logger.info(f"Finnhub symbol lookup failed for {company}: {e}")

    if len(matches) < limit and EODHD_API_KEY:
        try:
            _append_matches(_eodhd_symbol_lookup(company, limit), "eodhd")
        except Exception as e:
            logger.info(f"EODHD lookup failed for {company}: {e}")

    if len(matches) < limit:
        try:
            text = search(f"{company} ticker symbol")
            pattern = r"\b[A-Z]{1,5}(?:[.-][A-Z]{1,4})?\b"
            symbols = []
            for symbol in re.findall(pattern, text or ""):
                if symbol not in symbols:
                    symbols.append(symbol)
            search_matches = [
                {"symbol": sym, "description": "", "type": "search", "primaryExchange": "", "source": "search"}
                for sym in symbols[:limit]
            ]
            _append_matches(search_matches, "search")
        except Exception as e:
            logger.info(f"Search fallback for ticker lookup failed: {e}")

    source_label = "+".join(sources) if sources else "error"
    return {"query": company, "source": source_label, "matches": matches[:limit]}


def _openfigi_symbol_lookup(company: str, limit: int = 5) -> List[Dict[str, Any]]:
    if not OPENFIGI_API_KEY:
        return []
    url = "https://api.openfigi.com/v3/search"
    headers = {"X-OPENFIGI-APIKEY": OPENFIGI_API_KEY}
    payload = {"query": company, "limit": max(limit, 5)}
    resp = _http_post(url, headers=headers, json=payload, timeout=10)
    resp.raise_for_status()
    data = resp.json()
    results = data.get("data", []) if isinstance(data, dict) else []
    matches: List[Dict[str, Any]] = []
    for item in results:
        symbol = item.get("ticker")
        if not symbol:
            continue
        exchange = item.get("exchCode") or item.get("mic") or ""
        desc = item.get("name") or item.get("securityDescription") or ""
        matches.append({
            "symbol": symbol,
            "description": desc,
            "type": item.get("securityType") or item.get("marketSecDes") or "",
            "primaryExchange": exchange,
            "source": "openfigi",
        })
    return matches


def _eodhd_symbol_lookup(company: str, limit: int = 5) -> List[Dict[str, Any]]:
    if not EODHD_API_KEY:
        return []
    url = f"https://eodhd.com/api/search/{quote(company)}"
    params = {"api_token": EODHD_API_KEY, "fmt": "json"}
    resp = _http_get(url, params=params, timeout=10)
    resp.raise_for_status()
    data = resp.json()
    if not isinstance(data, list):
        return []
    matches: List[Dict[str, Any]] = []
    for item in data[: max(limit, 5)]:
        symbol = item.get("Code") or item.get("code")
        exchange = item.get("Exchange") or item.get("exchange") or ""
        if symbol and exchange and "." not in symbol:
            symbol = f"{symbol}.{exchange}"
        if not symbol:
            continue
        matches.append({
            "symbol": symbol,
            "description": item.get("Name") or item.get("name") or "",
            "type": item.get("Type") or item.get("type") or "",
            "primaryExchange": exchange,
            "source": "eodhd",
        })
    return matches

def _domain_from_url(url: str) -> str:
    try:
        parsed = urlparse(url)
        host = parsed.netloc or ""
        return host.replace("www.", "")
    except Exception:
        return ""


def _extract_datetime_from_text(text: str, now: datetime) -> Optional[datetime]:
    if not text:
        return None
    lowered = text.lower()

    # Relative English (e.g., "3 hours ago", "2 days ago")
    m = re.search(r"(\d{1,2})\s*(hours?|days?)\s+ago", lowered)
    if m:
        value = int(m.group(1))
        unit = m.group(2)
        if "hour" in unit:
            return now - timedelta(hours=value)
        return now - timedelta(days=value)

    # Relative Chinese (e.g., "3å°æ—¶å‰", "2å¤©å‰", "10åˆ†é’Ÿå‰")
    m = re.search(r"(\d{1,2})\s*(å°æ—¶|å¤©|åˆ†é’Ÿ)å‰", text)
    if m:
        value = int(m.group(1))
        unit = m.group(2)
        if unit == "å°æ—¶":
            return now - timedelta(hours=value)
        if unit == "åˆ†é’Ÿ":
            return now - timedelta(minutes=value)
        return now - timedelta(days=value)

    # Absolute date: YYYY-MM-DD or YYYY/MM/DD
    m = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})", text)
    if m:
        try:
            return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
        except Exception:
            return None

    # Absolute date: Month DD, YYYY
    m = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+(\d{1,2}),?\s+(\d{4})",
        text,
        re.IGNORECASE,
    )
    if m:
        try:
            month = m.group(1).title()
            day = int(m.group(2))
            year = int(m.group(3))
            month_map = {
                "Jan": 1,
                "Feb": 2,
                "Mar": 3,
                "Apr": 4,
                "May": 5,
                "Jun": 6,
                "Jul": 7,
                "Aug": 8,
                "Sep": 9,
                "Oct": 10,
                "Nov": 11,
                "Dec": 12,
            }
            return datetime(year, month_map[month], day)
        except Exception:
            return None

    return None


def _extract_datetime_from_url(url: str) -> Optional[datetime]:
    if not url:
        return None

    # Patterns like /2025/07/23/ or 2025-07-23
    m = re.search(r"(20\d{2})[-/](\d{1,2})[-/](\d{1,2})", url)
    if m:
        try:
            return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
        except Exception:
            return None

    # Pattern like 20250723 (avoid matching long ids by requiring separators nearby)
    m = re.search(r"(20\d{2})(\d{2})(\d{2})", url)
    if m:
        try:
            return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
        except Exception:
            return None

    return None


def _normalize_published_date(value: Any) -> Optional[str]:
    if not value:
        return None
    if isinstance(value, datetime):
        return value.strftime("%Y-%m-%d")
    if isinstance(value, (int, float)):
        try:
            return datetime.utcfromtimestamp(value).strftime("%Y-%m-%d")
        except Exception:
            return None
    if isinstance(value, str):
        cleaned = value.strip()
        if "T" in cleaned:
            cleaned = cleaned.split("T")[0]
        if re.match(r"^\d{4}-\d{2}-\d{2}$", cleaned):
            return cleaned
    return None


def _build_news_item(
    title: str,
    source: str,
    url: str = "",
    published_at: Any = None,
    snippet: str = "",
    ticker: Optional[str] = None,
    confidence: float = 0.7,
) -> Dict[str, Any]:
    if not title:
        return {}
    published_date = _normalize_published_date(published_at)
    return {
        "headline": title,
        "title": title,
        "url": url or "",
        "source": source or "Unknown",
        "snippet": snippet or "",
        "published_at": published_date,
        "datetime": published_date,
        "ticker": ticker,
        "confidence": confidence,
    }


def format_news_items(items: List[Dict[str, Any]], title: str = "Latest News") -> str:
    if not items:
        return "No recent news available."
    lines: List[str] = []
    for idx, item in enumerate(items, 1):
        headline = item.get("headline") or item.get("title") or "No title"
        source = item.get("source") or "Unknown"
        url = item.get("url") or ""
        snippet = item.get("snippet") or ""
        date_str = item.get("published_at") or item.get("datetime") or "Recent"
        line = _format_headline_line(date_str, headline, source, url, snippet)
        lines.append(f"{idx}. {line}")
    return f"{title}:\n" + "\n".join(lines)


def _extract_search_items(text: str) -> List[Dict[str, str]]:
    items: List[Dict[str, str]] = []
    current: Dict[str, str] | None = None

    for line in text.splitlines():
        stripped = line.strip()
        if re.match(r"^\d+\.", stripped):
            if current:
                items.append(current)
            title = re.sub(r"^\d+\.\s*", "", stripped).strip()
            current = {"title": title, "snippet": "", "url": ""}
            continue
        if stripped.startswith("http"):
            if current and not current.get("url"):
                current["url"] = stripped
            continue
        if stripped and current and not current.get("snippet"):
            current["snippet"] = stripped

    if current:
        items.append(current)

    return items


def _format_search_news_items(
    text: str,
    limit: int = 5,
    max_age_days: int = 7,
    now: Optional[datetime] = None,
) -> tuple[list[str], bool]:
    now = now or datetime.utcnow()
    items = _extract_search_items(text)
    enriched = []

    for item in items:
        title = item.get("title", "")
        snippet = item.get("snippet", "")
        if not _headline_is_useful(title, snippet):
            continue
        candidate_text = f"{title} {snippet}"
        dt = _extract_datetime_from_text(candidate_text, now)
        if not dt and item.get("url"):
            dt = _extract_datetime_from_url(item["url"])
        age_days = (now - dt).days if dt else None
        url = item.get("url", "")
        source = _domain_from_url(url)
        enriched.append(
            {
                "title": title,
                "snippet": snippet,
                "url": url,
                "source": source,
                "date": dt,
                "age_days": age_days,
            }
        )

    recent = [
        item
        for item in enriched
        if item["date"] and (now - item["date"]) <= timedelta(days=max_age_days)
    ]
    use_items = recent if recent else enriched

    lines: List[str] = []
    for item in use_items[:limit]:
        date_str = item["date"].strftime("%Y-%m-%d") if item["date"] else "æœªçŸ¥æ—¥æœŸ"
        source = item["source"] or "source"
        url = item["url"] or ""
        lines.append(
            _format_headline_line(
                date_str,
                item["title"],
                source,
                url,
                item.get("snippet", ""),
            )
        )

    return lines, bool(recent)


def _build_search_news_items(
    text: str,
    limit: int = 5,
    max_age_days: int = 7,
    now: Optional[datetime] = None,
) -> List[Dict[str, Any]]:
    now = now or datetime.utcnow()
    items = _extract_search_items(text)
    enriched = []

    for item in items:
        title = item.get("title", "")
        snippet = item.get("snippet", "")
        if not _headline_is_useful(title, snippet):
            continue
        candidate_text = f"{title} {snippet}"
        dt = _extract_datetime_from_text(candidate_text, now)
        if not dt and item.get("url"):
            dt = _extract_datetime_from_url(item["url"])
        age_days = (now - dt).days if dt else None
        url = item.get("url", "")
        source = _domain_from_url(url)
        enriched.append(
            {
                "title": title,
                "snippet": snippet,
                "url": url,
                "source": source,
                "date": dt,
                "age_days": age_days,
            }
        )

    recent = [
        item
        for item in enriched
        if item["date"] and (now - item["date"]) <= timedelta(days=max_age_days)
    ]
    use_items = recent if recent else enriched

    results: List[Dict[str, Any]] = []
    for item in use_items[:limit]:
        published_at = item["date"].strftime("%Y-%m-%d") if item["date"] else None
        results.append(
            _build_news_item(
                title=item["title"],
                source=item["source"] or "search",
                url=item["url"],
                published_at=published_at,
                snippet=item.get("snippet", ""),
                confidence=0.4,
            )
        )
    return [item for item in results if item]


def _parse_rss_items(
    xml_text: str,
    limit: int = 5,
    max_age_days: int = 2,
    now: Optional[datetime] = None,
) -> tuple[list[str], bool]:
    now = now or datetime.utcnow()
    lines: List[str] = []
    try:
        root = ET.fromstring(xml_text)
    except Exception:
        return [], False

    items = root.findall(".//item")
    for item in items:
        title = (item.findtext("title") or "").strip()
        link = (item.findtext("link") or "").strip()
        pub_date = (item.findtext("pubDate") or "").strip()
        if not title or not pub_date:
            continue
        if not _headline_is_useful(title, ""):
            continue

        try:
            dt = parsedate_to_datetime(pub_date)
        except Exception:
            dt = None
        if not dt:
            continue
        if dt.tzinfo:
            dt = dt.astimezone(tz=None).replace(tzinfo=None)

        if (now - dt) > timedelta(days=max_age_days):
            continue

        source = _domain_from_url(link)
        date_str = dt.strftime("%Y-%m-%d")
        lines.append(_format_headline_line(date_str, title, source, link))
        if len(lines) >= limit:
            break

    return lines, bool(lines)


def _fetch_rss_headlines(
    feed_urls: List[str],
    limit: int = 5,
    max_age_days: int = 2,
) -> tuple[list[str], bool]:
    all_lines: List[str] = []
    for url in feed_urls:
        try:
            resp = _http_get(url, timeout=8)
            if resp.status_code != 200:
                continue
            lines, ok = _parse_rss_items(resp.text, limit=limit, max_age_days=max_age_days)
            if ok:
                all_lines.extend(lines)
        except Exception:
            continue
        if len(all_lines) >= limit:
            break
    return all_lines[:limit], bool(all_lines)


def _fetch_finnhub_market_news(limit: int = 5, max_age_hours: int = 48) -> tuple[list[str], bool]:
    if not finnhub_client:
        return [], False

    now = datetime.utcnow()
    try:
        items = finnhub_client.general_news("general")
    except Exception:
        return [], False

    lines = []
    for item in items or []:
        ts = item.get("datetime")
        if not ts:
            continue
        try:
            dt = datetime.utcfromtimestamp(ts)
        except Exception:
            continue
        if (now - dt) > timedelta(hours=max_age_hours):
            continue
        title = item.get("headline") or item.get("summary") or "No title"
        snippet = item.get("summary") or ""
        if not _headline_is_useful(title, snippet):
            continue
        source = item.get("source") or "finnhub"
        url = item.get("url") or ""
        lines.append(
            _format_headline_line(
                dt.strftime("%Y-%m-%d"),
                title,
                source,
                url,
                snippet,
            )
        )
        if len(lines) >= limit:
            break
    return lines, bool(lines)

MARKET_INDICES = {
    "^GSPC": "S&P 500 index",
    "^IXIC": "Nasdaq Composite index", 
    "^DJI": "Dow Jones Industrial Average",
    "^RUT": "Russell 2000 index",
    "^VIX": "VIX volatility index",
    "^NYA": "NYSE Composite index",
    "^FTSE": "FTSE 100 index",
    "^N225": "Nikkei 225 index",
    "^HSI": "Hang Seng index"
}

def _is_market_index(ticker: str) -> bool:
    """åˆ¤æ–­tickeræ˜¯å¦ä¸ºå¸‚åœºæŒ‡æ•°"""
    # æ–¹æ³•1: æ£€æŸ¥æ˜¯å¦åœ¨å·²çŸ¥æŒ‡æ•°åˆ—è¡¨ä¸­
    if ticker in MARKET_INDICES:
        return True
    
    # æ–¹æ³•2: æ£€æŸ¥å¸¸è§æŒ‡æ•°å‘½åæ¨¡å¼
    index_patterns = [
        r'^\^',      # ä»¥ ^ å¼€å¤´ï¼ˆYahoo FinanceæŒ‡æ•°æ ‡è®°ï¼‰
        r'SPX$',     # S&P 500 çš„å¦ä¸€ç§å†™æ³•
        r'NDX$',     # Nasdaq 100
        r'DJI$',     # Dow Jones
    ]
    
    for pattern in index_patterns:
        if re.match(pattern, ticker):
            return True
    
    return False

def _get_index_news(ticker: str) -> List[Dict[str, Any]]:
    """
    ä¸“é—¨ä¸ºå¸‚åœºæŒ‡æ•°è·å–æ–°é—»çš„æ–¹æ³•ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰ã€‚
    ç­–ç•¥ï¼šé€šè¿‡æœç´¢è·å–å®è§‚å¸‚åœºæ–°é—»å’ŒæŒ‡æ•°åˆ†æã€‚
    """
    friendly_name = MARKET_INDICES.get(ticker, ticker.replace('^', ''))
    
    logger.info(f"  â†’ Detected market index: {friendly_name}")
    logger.info(f"  â†’ Using specialized search strategy for index news...")
    
    # ç­–ç•¥1: æœç´¢æŒ‡æ•°æœ€è¿‘è¡¨ç°å’Œåˆ†æ
    current_date = datetime.now().strftime('%B %Y')
    search_queries = [
        f"{friendly_name} recent performance analysis {current_date}",
        f"{friendly_name} market news today",
        f"What's driving {friendly_name} this week"
    ]
    
    all_results = []
    for query in search_queries[:2]:  # åªç”¨å‰2ä¸ªæŸ¥è¯¢ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
        try:
            results = search(query)
            if results and "No search results" not in results:
                all_results.append(results)
            time.sleep(1)
        except Exception as e:
            logger.info(f"  â†’ Search failed for '{query}': {e}")
            continue
    
    if not all_results:
        return []
    
    # è§£æå¹¶æ ¼å¼åŒ–æœç´¢ç»“æœ
    combined_results = "\n\n".join(all_results)
    
    # å°è¯•ä»æœç´¢ç»“æœä¸­æå–æ–°é—»æ ‡é¢˜å’Œæ—¥æœŸ
    news_items: List[Dict[str, Any]] = []
    lines = combined_results.split('\n')
    
    for i, line in enumerate(lines):
        # å¯»æ‰¾æ ‡é¢˜æ¨¡å¼ï¼ˆé€šå¸¸ä»¥æ•°å­—å¼€å¤´ï¼‰
        if re.match(r'^\d+\.', line.strip()):
            raw_title = line.strip()
            title = re.sub(r'^\d+\.\s*', '', raw_title).strip()
            window = ' '.join(lines[i:i+3])
            # å°è¯•æ‰¾åˆ°æ—¥æœŸä¿¡æ¯
            date_match = re.search(r'(\d{1,2}\s+\w+\s+ago|\d{4}-\d{2}-\d{2}|\w+\s+\d{1,2},?\s+\d{4})', 
                                  window, re.IGNORECASE)
            if not _is_reasonable_headline(title, window):
                continue
            if not _headline_is_useful(title, window):
                continue
            date_str = date_match.group(1) if date_match else 'Recent'
            item = _build_news_item(
                title=title,
                source="search",
                url="",
                published_at=date_str,
                snippet=window,
                ticker=ticker,
                confidence=0.4,
            )
            if item:
                news_items.append(item)
            
            if len(news_items) >= 5:
                break
    
    return news_items

def get_company_news(ticker: str) -> List[Dict[str, Any]]:
    """
    æ™ºèƒ½è·å–æ–°é—»ï¼šè‡ªåŠ¨è¯†åˆ«æ˜¯å…¬å¸è‚¡ç¥¨è¿˜æ˜¯å¸‚åœºæŒ‡æ•°ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰ã€‚
    - å…¬å¸è‚¡ç¥¨ï¼šä½¿ç”¨ API (yfinance, Finnhub, Alpha Vantage)
    - å¸‚åœºæŒ‡æ•°ï¼šä½¿ç”¨æœç´¢ç­–ç•¥è·å–å®è§‚å¸‚åœºæ–°é—»
    """
    # ğŸ” å…³é”®åˆ¤æ–­ï¼šè¿™æ˜¯æŒ‡æ•°è¿˜æ˜¯å…¬å¸è‚¡ç¥¨ï¼Ÿ
    if _is_market_index(ticker):
        # ä¼˜å…ˆç”¨ alert_scheduler çš„æ–°é—»æŠ“å–ï¼ˆå«48hè¿‡æ»¤ï¼‰
        try:
            from backend.services.alert_scheduler import fetch_news_articles
            articles = fetch_news_articles(ticker)
            if articles:
                items: List[Dict[str, Any]] = []
                for a in articles:
                    title = a.get("title") or a.get("headline") or a.get("summary") or "No title"
                    snippet = a.get("summary") or a.get("description") or ""
                    if not _headline_is_useful(title, snippet):
                        continue
                    source = a.get("source") or a.get("publisher") or "Unknown"
                    published_at = a.get("published_at") or a.get("datetime") or a.get("providerPublishTime") or 0
                    url = a.get("url") or a.get("link") or ""
                    item = _build_news_item(
                        title=title,
                        source=source,
                        url=url,
                        published_at=published_at,
                        snippet=snippet,
                        ticker=ticker,
                        confidence=0.7,
                    )
                    if item:
                        items.append(item)
                    if len(items) >= 5:
                        break
                if items:
                    return items
        except Exception as e:
            logger.info(f"index news via alert_scheduler failed: {e}")

        # å…ˆè¯• yfinance çš„æ–°é—»ï¼ˆéƒ¨åˆ†æŒ‡æ•°ä¹Ÿæœ‰ï¼‰
        try:
            stock = yf.Ticker(ticker)
            news = stock.news
            if news:
                items = []
                for article in news:
                    title = article.get('title', 'No title')
                    snippet = article.get('summary') or article.get('description') or ""
                    if not _headline_is_useful(title, snippet):
                        continue
                    publisher = article.get('publisher', 'Unknown source')
                    pub_time = article.get('providerPublishTime', 0)
                    url = article.get('link') or article.get('url') or ''
                    item = _build_news_item(
                        title=title,
                        source=publisher,
                        url=url,
                        published_at=pub_time,
                        snippet=snippet,
                        ticker=ticker,
                        confidence=0.7,
                    )
                    if item:
                        items.append(item)
                    if len(items) >= 5:
                        break
                if items:
                    return items
        except Exception as e:
            logger.info(f"yfinance index news error for {ticker}: {e}")

        # å†é€€å›æœç´¢ç­–ç•¥
        return _get_index_news(ticker)
    
    # --- ä»¥ä¸‹æ˜¯åŸæœ‰çš„å…¬å¸æ–°é—»è·å–é€»è¾‘ ---
    
    # æ–¹æ³•1: yfinance
    try:
        stock = yf.Ticker(ticker)
        news = stock.news
        if news:
            items = []
            for article in news:
                title = article.get('title', 'No title')
                snippet = article.get('summary') or article.get('description') or ""
                if not _headline_is_useful(title, snippet):
                    continue
                publisher = article.get('publisher', 'Unknown source')
                pub_time = article.get('providerPublishTime', 0)
                url = article.get('link') or article.get('url') or ''
                item = _build_news_item(
                    title=title,
                    source=publisher,
                    url=url,
                    published_at=pub_time,
                    snippet=snippet,
                    ticker=ticker,
                    confidence=0.7,
                )
                if item:
                    items.append(item)
                if len(items) >= 5:
                    break
            if items:
                return items
    except Exception as e:
        logger.info(f"yfinance news error for {ticker}: {e}")

    # æ–¹æ³•2: Finnhub
    if finnhub_client:
        try:
            logger.info(f"Trying Finnhub news for {ticker}")
            to_date = date.today().strftime("%Y-%m-%d")
            from_date = (date.today() - timedelta(days=7)).strftime("%Y-%m-%d")
            news = finnhub_client.company_news(ticker, _from=from_date, to=to_date)
            if news:
                items = []
                for article in news:
                    title = article.get('headline', 'No title')
                    snippet = article.get('summary') or ""
                    if not _headline_is_useful(title, snippet):
                        continue
                    source = article.get('source', 'Unknown')
                    pub_time = article.get('datetime', 0)
                    url = article.get('url') or ''
                    item = _build_news_item(
                        title=title,
                        source=source,
                        url=url,
                        published_at=pub_time,
                        snippet=snippet,
                        ticker=ticker,
                        confidence=0.8,
                    )
                    if item:
                        items.append(item)
                    if len(items) >= 5:
                        break
                if items:
                    return items
        except Exception as e:
            logger.info(f"Finnhub news fetch failed: {e}")

    # æ–¹æ³•3: Alpha Vantage
    try:
        logger.info(f"Trying Alpha Vantage news for {ticker}")
        url = "https://www.alphavantage.co/query"
        params = {'function': 'NEWS_SENTIMENT', 'tickers': ticker, 'limit': 5, 'apikey': ALPHA_VANTAGE_API_KEY}
        response = _http_get(url, params=params, timeout=10)
        data = response.json()
        if 'feed' in data and data['feed']:
            items = []
            for article in data['feed']:
                title = article.get('title', 'No title')
                source = article.get('source', 'Unknown')
                date_str = article.get('time_published', '')[:8]
                if date_str:
                    date_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
                snippet = article.get('summary') or ""
                if not _headline_is_useful(title, snippet):
                    continue
                url = article.get('url') or article.get('link') or ''
                item = _build_news_item(
                    title=title,
                    source=source,
                    url=url,
                    published_at=date_str,
                    snippet=snippet,
                    ticker=ticker,
                    confidence=0.8,
                )
                if item:
                    items.append(item)
                if len(items) >= 5:
                    break
            if items:
                return items
    except Exception as e:
        logger.info(f"Alpha Vantage news fetch failed: {e}")
    
    # æ–¹æ³•4: å›é€€åˆ°å…¬å¸ç‰¹å®šæœç´¢
    logger.info(f"Falling back to search for {ticker} news")
    fallback_text = search(f"{ticker} company latest news stock")
    items = _build_search_news_items(fallback_text, limit=5, max_age_days=7)
    if items:
        for item in items:
            if isinstance(item, dict):
                item.setdefault("ticker", ticker)
        return items
    return []


def get_news_sentiment(ticker: str, limit: int = 5) -> str:
    """
    è·å–æ–°é—»æƒ…ç»ª (Alpha Vantage NEWS_SENTIMENT)
    """
    if not ticker:
        return "News Sentiment: ticker is required."

    if not ALPHA_VANTAGE_API_KEY:
        return "News Sentiment: ALPHA_VANTAGE_API_KEY not configured."

    try:
        url = "https://www.alphavantage.co/query"
        params = {
            'function': 'NEWS_SENTIMENT',
            'tickers': ticker,
            'limit': limit,
            'apikey': ALPHA_VANTAGE_API_KEY,
        }
        response = _http_get(url, params=params, timeout=10)
        data = response.json()

        if not data or 'feed' not in data or not data.get('feed'):
            if isinstance(data, dict):
                if data.get('Note'):
                    return f"News Sentiment: rate limited ({data.get('Note')})"
                if data.get('Information'):
                    return f"News Sentiment: {data.get('Information')}"
                if data.get('Error Message'):
                    return f"News Sentiment: {data.get('Error Message')}"
            return "News Sentiment: no data found."

        def _extract_sentiment(item: Dict[str, Any], symbol: str):
            symbol_upper = symbol.upper()
            for ts in item.get('ticker_sentiment', []):
                if ts.get('ticker', '').upper() == symbol_upper:
                    return ts.get('ticker_sentiment_score'), ts.get('ticker_sentiment_label')
            return item.get('overall_sentiment_score'), item.get('overall_sentiment_label')

        lines = []
        scores: List[float] = []
        for i, item in enumerate(data.get('feed', [])[:limit], 1):
            title = item.get('title', 'No title')
            source = item.get('source', 'Unknown')
            time_published = item.get('time_published', '')
            date_str = time_published[:8]
            if date_str and len(date_str) == 8:
                date_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            else:
                date_str = 'Unknown date'
            url = item.get('url') or item.get('link') or ''
            score, label = _extract_sentiment(item, ticker)
            sentiment_desc = "N/A"
            try:
                if score is not None:
                    score_val = float(score)
                    scores.append(score_val)
                    sentiment_desc = f"{label or 'Unknown'} ({score_val:.2f})"
                elif label:
                    sentiment_desc = label
            except Exception:
                if label:
                    sentiment_desc = label

            headline = f"[{title}]({url})" if url else title
            lines.append(f"{i}. [{date_str}] {headline} ({source}) æƒ…ç»ª: {sentiment_desc}")

        avg_text = ""
        if scores:
            avg_score = sum(scores) / len(scores)
            avg_text = f"\nå¹³å‡æƒ…ç»ªåˆ†æ•°: {avg_score:.2f}"

        return f"News Sentiment ({ticker}):{avg_text}\n" + "\n".join(lines)
    except Exception as e:
        return f"News Sentiment: fetch failed ({str(e)})"


def get_market_news_headlines(limit: int = 5) -> str:
    """
    å¸‚åœºæ³›åŒ–æ–°é—»ï¼šä¸å¸¦ ticker çš„æƒ…å†µï¼ŒæŠ“å–å…¨çƒ/ç¾è‚¡è¦é—»ã€‚
    ä½¿ç”¨æœç´¢èšåˆå¹¶æå–ç¼–å·è¡Œä½œä¸ºæ ‡é¢˜ï¼Œå¦åˆ™è¿”å›ç®€çŸ­æç¤ºã€‚
    """
    # 0) å®˜æ–¹ RSSï¼ˆReuters/Bloombergï¼‰ï¼Œä¼˜å…ˆ 48h å†…
    reuters_feeds = [
        "https://feeds.reuters.com/reuters/businessNews",
        "https://feeds.reuters.com/reuters/topNews",
        "https://feeds.reuters.com/reuters/worldNews",
        "https://feeds.reuters.com/reuters/technologyNews",
    ]
    bloomberg_default_feeds = [
        "https://feeds.bloomberg.com/markets/news.rss",
        "https://feeds.bloomberg.com/technology/news.rss",
        "https://feeds.bloomberg.com/politics/news.rss",
        "https://feeds.bloomberg.com/wealth/news.rss",
        "https://feeds.bloomberg.com/pursuits/news.rss",
        "https://feeds.bloomberg.com/businessweek/news.rss",
        "https://feeds.bloomberg.com/industries/news.rss",
    ]
    bloomberg_env = os.getenv("BLOOMBERG_RSS_URLS", "").strip()
    bloomberg_env_feeds = [u.strip() for u in bloomberg_env.split(",") if u.strip()]
    if bloomberg_env_feeds:
        bloomberg_feeds = bloomberg_default_feeds + [
            u for u in bloomberg_env_feeds if u not in bloomberg_default_feeds
        ]
    else:
        bloomberg_feeds = bloomberg_default_feeds

    rss_feeds = reuters_feeds + bloomberg_feeds
    rss_lines, rss_ok = _fetch_rss_headlines(rss_feeds, limit=limit * 2, max_age_days=2)
    if rss_ok:
        return "æœ€è¿‘48å°æ—¶å¸‚åœºè¦é—»(RSS):\n" + "\n".join(rss_lines[:limit])

    # 1) Finnhub å¸‚åœºæ–°é—»ï¼ˆ48hï¼‰
    finnhub_lines, finnhub_ok = _fetch_finnhub_market_news(limit=limit * 2, max_age_hours=48)
    if finnhub_ok:
        return "æœ€è¿‘48å°æ—¶å¸‚åœºè¦é—»(Finnhub):\n" + "\n".join(finnhub_lines[:limit])

    # 2) å°è¯•ç”¨ alert_scheduler çš„æ–°é—»æŠ“å–ï¼ˆå·²å«48hè¿‡æ»¤ï¼‰ï¼Œä¼˜å…ˆæŒ‡æ•°ä¸ä»£è¡¨æ€§ETF
    try:
        from backend.services.alert_scheduler import fetch_news_articles
        for idx_ticker in ["^GSPC", "^IXIC", "SPY", "QQQ", "DIA", "IWM"]:
            try:
                articles = fetch_news_articles(idx_ticker)
            except Exception as inner:
                logger.info(f"[MarketNews] fetch_news_articles failed for {idx_ticker}: {inner}")
                continue
            if articles:
                lines = []
                for a in articles:
                    title = a.get("title") or a.get("headline") or a.get("summary") or "No title"
                    snippet = a.get("summary") or a.get("description") or ""
                    if not _headline_is_useful(title, snippet):
                        continue
                    source = a.get("source") or a.get("publisher") or "Unknown"
                    published_at = a.get("published_at") or a.get("datetime") or a.get("providerPublishTime") or 0
                    if isinstance(published_at, str):
                        date_str = published_at.split("T")[0]
                    else:
                        date_str = datetime.fromtimestamp(published_at).strftime("%Y-%m-%d") if published_at else "Recent"
                    url = a.get("url") or a.get("link") or ""
                    line = _format_headline_line(date_str, title, source, url, snippet)
                    lines.append(f"{len(lines) + 1}. {line}")
                    if len(lines) >= limit:
                        break
                if lines:
                    return "æœ€è¿‘48å°æ—¶å¸‚åœºè¦é—»:\n" + "\n".join(lines)
    except Exception as e:
        logger.info(f"[MarketNews] fetch via alert_scheduler failed: {e}")

    # 3) æœç´¢èšåˆå…œåº•
    queries = [
        "global stock market breaking news today",
        "US stock market headlines today",
        "market moving news today equities"
    ]
    combined = []
    for q in queries:
        try:
            res = search(q)
            combined.append(res)
        except Exception as e:
            logger.info(f"[MarketNews] search failed for '{q}': {e}")
            continue
    if not combined:
        return "æœªèƒ½è·å–å¯é çš„å¸‚åœºçƒ­ç‚¹ä¿¡æ¯ï¼Œè¯·ç›´æ¥æŸ¥çœ‹ Bloomberg/Reuters/WSJ ç­‰æƒå¨æ¥æºã€‚"
    
    text = "\n\n".join(combined)
    lines, has_recent = _format_search_news_items(text, limit=limit, max_age_days=3)
    if not has_recent:
        lines, has_recent = _format_search_news_items(text, limit=limit, max_age_days=7)

    if not has_recent:
        retry_queries = [
            "global stock market news last 24 hours",
            "US stock market headlines last 24 hours",
            "market moving news past week site:reuters.com",
        ]
        retry_combined = []
        for q in retry_queries:
            try:
                res = search(q)
                retry_combined.append(res)
            except Exception as e:
                logger.info(f"[MarketNews] retry search failed for '{q}': {e}")
                continue
        if retry_combined:
            retry_text = "\n\n".join(retry_combined)
            retry_lines, retry_recent = _format_search_news_items(retry_text, limit=limit, max_age_days=7)
            if retry_lines and retry_recent:
                return "æœ€è¿‘å¸‚åœºçƒ­ç‚¹(è¿‘7å¤©):\n" + "\n".join(retry_lines)
            if retry_recent:
                lines = retry_lines
                has_recent = True

    if has_recent and lines:
        return "æœ€è¿‘å¸‚åœºçƒ­ç‚¹(è¿‘7å¤©):\n" + "\n".join(lines)

    return "è¿‘7å¤©å†…æœªæ£€ç´¢åˆ°å¯é å¸‚åœºçƒ­ç‚¹ï¼Œè¯·ç›´æ¥æŸ¥çœ‹ Bloomberg/Reuters/WSJ ç­‰æƒå¨æ¥æºã€‚"
# ============================================
# å…¶ä»–å·¥å…·å‡½æ•°ï¼ˆä¿æŒä¸å˜æˆ–ç¨ä½œä¿®æ”¹ï¼‰
# ============================================

def get_market_sentiment() -> str:
    """
    è·å–å¸‚åœºæƒ…ç»ªæŒ‡æ ‡ - CNN Fear & Greed Index
    ä½¿ç”¨æ›´å®Œæ•´çš„è¯·æ±‚å¤´æ¥æ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œæé«˜æˆåŠŸç‡ã€‚
    """
    try:
        # ä¸»è¦APIåœ°å€
        url = "https://production.dataviz.cnn.io/index/fearandgreed/graphdata"
        
        # ä¼ªè£…æˆä¸€ä¸ªä»CNNå®˜ç½‘é¡µé¢å‘å‡ºè¯·æ±‚çš„çœŸå®æµè§ˆå™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            # 'Referer' æ˜¯æœ€å…³é”®çš„å¤´ä¿¡æ¯ï¼Œå‘Šè¯‰æœåŠ¡å™¨è¯·æ±‚çš„æ¥æºé¡µé¢
            'Referer': 'https://www.cnn.com/markets/fear-and-greed',
            'Origin': 'https://www.cnn.com',
        }
        
        logger.info("Attempting to fetch from CNN API with full headers...")
        response = _http_get(url, headers=headers, timeout=10)
        
        # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 2xxï¼Œåˆ™ä¼šå¼•å‘ HTTPError å¼‚å¸¸
        response.raise_for_status() 
        
        data = response.json()
        score = float(data['fear_and_greed']['score'])
        rating = data['fear_and_greed']['rating']
        
        logger.info("CNN API fetch successful!")
        return f"CNN Fear & Greed Index: {score:.1f} ({rating})"
    
    except requests.exceptions.HTTPError as http_err:
        logger.info(f"CNN API failed with HTTP error: {http_err}. Trying fallback search...")
    except Exception as e:
        # æ•è·å…¶ä»–æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸ï¼Œä¾‹å¦‚ç½‘ç»œé—®é¢˜ã€JSONè§£æé”™è¯¯ç­‰
        logger.info(f"CNN API failed with other error: {e}. Trying fallback search...")
    # --- å¦‚æœä¸Šé¢çš„ try ä»£ç å—å‡ºç°ä»»ä½•å¼‚å¸¸ï¼Œåˆ™æ‰§è¡Œä¸‹é¢çš„å›é€€é€»è¾‘ ---
    try:
        search_result = search("CNN Fear and Greed Index current value today")
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æœç´¢ç»“æœä¸­æå–æ•°å€¼å’Œè¯„çº§
        match = re.search(r'(?:Index|Score)[:\s]*(\d+\.?\d*)\s*\((\w+\s?\w*)\)', search_result, re.IGNORECASE)
        if match:
            score = float(match.group(1))
            rating = match.group(2)
            logger.info("Fallback search successful!")
            return f"CNN Fear & Greed Index (via search): {score:.1f} ({rating})"
    except Exception as search_e:
        logger.info(f"Search fallback also failed: {search_e}")
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œè¿”å›ä¸€ä¸ªé€šç”¨é”™è¯¯ä¿¡æ¯
    return "Fear & Greed Index: Unable to fetch. Please check manually."
def get_economic_events() -> str:
    """æœç´¢å½“å‰æœˆä»½çš„ä¸»è¦ç¾å›½ç»æµäº‹ä»¶"""
    now = datetime.now()
    query = f"major upcoming US economic events {now.strftime('%B %Y')} (FOMC, CPI, jobs report)"
    return search(query)

def get_fred_data(series_id: str = None) -> Dict[str, Any]:
    """
    ä» FRED (Federal Reserve Economic Data) è·å–å®è§‚ç»æµæ•°æ®

    å¸¸ç”¨ series_id:
    - CPIAUCSL: CPI (Consumer Price Index)
    - FEDFUNDS: Federal Funds Rate
    - GDP: Gross Domestic Product
    - UNRATE: Unemployment Rate
    - DGS10: 10-Year Treasury Rate
    - T10Y2Y: 10Y-2Y Treasury Spread (è¡°é€€æŒ‡æ ‡)
    """
    result = {
        "cpi": None,
        "fed_rate": None,
        "gdp_growth": None,
        "unemployment": None,
        "treasury_10y": None,
        "yield_spread": None,
        "status": "success",
        "source": "FRED",
        "as_of": datetime.now().isoformat()
    }

    # FRED API é…ç½®
    api_key = FRED_API_KEY
    base_url = "https://api.stlouisfed.org/fred/series/observations"

    # è¦è·å–çš„æŒ‡æ ‡
    series_map = {
        "cpi": "CPIAUCSL",
        "fed_rate": "FEDFUNDS",
        "gdp_growth": "A191RL1Q225SBEA",  # Real GDP Growth Rate
        "unemployment": "UNRATE",
        "treasury_10y": "DGS10",
        "yield_spread": "T10Y2Y"
    }

    # å¦‚æœæŒ‡å®šäº†å•ä¸ª series_idï¼Œåªè·å–è¯¥æ•°æ®
    if series_id:
        series_map = {"custom": series_id}

    for key, sid in series_map.items():
        try:
            params = {
                "series_id": sid,
                "api_key": api_key,
                "file_type": "json",
                "sort_order": "desc",
                "limit": 1
            }

            if api_key:
                response = _http_get(base_url, params=params, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    observations = data.get("observations", [])
                    if observations:
                        value = observations[0].get("value", ".")
                        if value != ".":
                            result[key] = float(value)
            else:
                # æ—  API key æ—¶ä½¿ç”¨æœç´¢å›é€€
                if key == "cpi":
                    result[key] = 3.0  # ä¼°è®¡å€¼
                elif key == "fed_rate":
                    result[key] = 4.5  # ä¼°è®¡å€¼
                elif key == "unemployment":
                    result[key] = 4.0  # ä¼°è®¡å€¼
                result["source"] = "estimate"

        except Exception as e:
            logger.info(f"[FRED] Failed to fetch {sid}: {e}")
            continue

    # æ ¼å¼åŒ–è¾“å‡º
    if result.get("cpi"):
        result["cpi_formatted"] = f"{result['cpi']:.1f}"
    if result.get("fed_rate"):
        result["fed_rate_formatted"] = f"{result['fed_rate']:.2f}%"
    if result.get("unemployment"):
        result["unemployment_formatted"] = f"{result['unemployment']:.1f}%"
    if result.get("gdp_growth"):
        result["gdp_growth_formatted"] = f"{result['gdp_growth']:.1f}%"
    if result.get("treasury_10y"):
        result["treasury_10y_formatted"] = f"{result['treasury_10y']:.2f}%"
    if result.get("yield_spread"):
        result["yield_spread_formatted"] = f"{result['yield_spread']:.2f}%"
        # æ”¶ç›Šç‡æ›²çº¿å€’æŒ‚è­¦å‘Š
        if result["yield_spread"] < 0:
            result["recession_warning"] = True

    return result

def get_performance_comparison(tickers: Union[dict, list]) -> str:
    """Compare YTD and 1-Year performance for a labeled ticker map.

    Args:
        tickers: æ”¯æŒä¸¤ç§æ ¼å¼:
            - dict: {"Apple": "AAPL", "Tesla": "TSLA"}
            - list: ["AAPL", "TSLA"]
    """
    # å…¼å®¹ list è¾“å…¥ï¼šå°† list è½¬æ¢ä¸º dict æ ¼å¼
    if isinstance(tickers, list):
        tickers = {t: t for t in tickers}

    data: Dict[str, Dict[str, str]] = {}
    notes: List[str] = []
    now = datetime.now()

    def _calc_from_hist(hist: pd.DataFrame):
        if hist is None or hist.empty or 'Close' not in hist.columns:
            return None
        hist = hist.copy()
        try:
            hist.index = hist.index.tz_localize(None)
        except Exception:
            pass
        end_price = float(hist['Close'].iloc[-1])
        start_of_year = datetime(now.year, 1, 1)
        ytd_hist = hist[hist.index >= start_of_year]
        perf_ytd = None
        if not ytd_hist.empty:
            start_price_ytd = float(ytd_hist['Close'].iloc[0])
            if start_price_ytd:
                perf_ytd = ((end_price - start_price_ytd) / start_price_ytd) * 100
        one_year_ago = now - timedelta(days=365)
        one_year_hist = hist[hist.index >= one_year_ago]
        perf_1y = None
        if not one_year_hist.empty:
            start_price_1y = float(one_year_hist['Close'].iloc[0])
            if start_price_1y:
                perf_1y = ((end_price - start_price_1y) / start_price_1y) * 100
        coverage_start = hist.index.min() if not hist.empty else None
        return end_price, perf_ytd, perf_1y, coverage_start

    def _calc_from_kline(kline_data: List[Dict[str, Any]]):
        if not kline_data:
            return None
        df = pd.DataFrame(kline_data)
        if 'time' not in df.columns or 'close' not in df.columns:
            return None
        df['time'] = pd.to_datetime(df['time'], errors='coerce')
        df = df.dropna(subset=['time']).sort_values('time')
        if df.empty:
            return None
        end_price = float(df['close'].iloc[-1])
        start_of_year = datetime(now.year, 1, 1)
        ytd_df = df[df['time'] >= start_of_year]
        perf_ytd = None
        if not ytd_df.empty:
            start_price_ytd = float(ytd_df['close'].iloc[0])
            if start_price_ytd:
                perf_ytd = ((end_price - start_price_ytd) / start_price_ytd) * 100
        one_year_ago = now - timedelta(days=365)
        one_year_df = df[df['time'] >= one_year_ago]
        perf_1y = None
        if not one_year_df.empty:
            start_price_1y = float(one_year_df['close'].iloc[0])
            if start_price_1y:
                perf_1y = ((end_price - start_price_1y) / start_price_1y) * 100
        coverage_start = df['time'].iloc[0]
        return end_price, perf_ytd, perf_1y, coverage_start

    for name, ticker in tickers.items():
        time.sleep(0.3)
        perf = None
        fallback_used = False
        error_note = ""
        try:
            stock = yf.Ticker(ticker)
            hist = stock.history(period="2y")
            perf = _calc_from_hist(hist)
            if perf is None:
                error_note = "yfinance returned empty data"
                raise ValueError(error_note)
        except Exception as e:
            error_note = str(e) or error_note
            try:
                fallback = get_stock_historical_data(ticker, period="2y", interval="1d")
                kline = fallback.get("kline_data") if isinstance(fallback, dict) else None
                perf = _calc_from_kline(kline or [])
                fallback_used = perf is not None
                if not perf and isinstance(fallback, dict) and fallback.get("error"):
                    error_note = fallback.get("error")
            except Exception as fb_e:
                error_note = f"{error_note}; fallback failed: {fb_e}"

        if not perf:
            data[name] = {"Current": "N/A", "YTD": "N/A", "1-Year": "N/A"}
            notes.append(f"{name}: data unavailable ({error_note})")
            continue

        end_price, perf_ytd, perf_1y, coverage_start = perf
        data[name] = {
            "Current": f"{end_price:,.2f}",
            "YTD": f"{perf_ytd:+.2f}%" if perf_ytd is not None else "N/A",
            "1-Year": f"{perf_1y:+.2f}%" if perf_1y is not None else "N/A",
        }
        missing = []
        if perf_ytd is None:
            missing.append("YTD")
        if perf_1y is None:
            missing.append("1-Year")
        if missing and coverage_start is not None:
            notes.append(f"{name}: limited history from {coverage_start:%Y-%m-%d} (missing {', '.join(missing)})")
        if fallback_used:
            notes.append(f"{name}: used fallback price history")

    if not data:
        return "Unable to fetch performance data for any ticker."

    header = f"{'Ticker':<25} {'Current Price':<15} {'YTD %':<12} {'1-Year %':<12}\n" + "-" * 67 + "\n"
    rows = [
        f"{name:<25} {metrics['Current']:<15} {metrics['YTD']:<12} {metrics['1-Year']:<12}"
        for name, metrics in data.items()
    ]
    note_text = f"\n\nNotes:\n- " + "\n- ".join(notes) if notes else ""
    return "Performance Comparison:\n\n" + header + "\n".join(rows) + note_text


def analyze_historical_drawdowns(ticker: str = "^IXIC") -> str:
    """Summarize the largest drawdowns over the available history."""
    hist = pd.DataFrame()
    error_note = ""
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="max")
    except Exception as e:
        error_note = str(e)

    if hist is None or hist.empty:
        try:
            fallback = get_stock_historical_data(ticker, period="max", interval="1d")
            kline = fallback.get("kline_data") if isinstance(fallback, dict) else None
            if kline:
                df = pd.DataFrame(kline)
                df['time'] = pd.to_datetime(df['time'], errors='coerce')
                df = df.dropna(subset=['time']).sort_values('time')
                if not df.empty:
                    df = df.rename(columns={'close': 'Close'})
                    hist = df.set_index('time')
        except Exception as fb_e:
            error_note = f"{error_note}; fallback failed: {fb_e}" if error_note else str(fb_e)

    if hist is None or hist.empty or 'Close' not in hist.columns:
        return f"No historical data available for {ticker}." + (f" ({error_note})" if error_note else "")

    try:
        hist.index = hist.index.tz_localize(None)
    except Exception:
        pass

    start_date = hist.index.min()
    end_date = hist.index.max()
    coverage_years = (end_date - start_date).days / 365.25 if start_date and end_date else 0
    coverage_text = f"{start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')} (~{coverage_years:.1f}y)"

    hist = hist.copy()
    hist['peak'] = hist['Close'].cummax()
    hist['drawdown'] = (hist['Close'] - hist['peak']) / hist['peak']

    drawdown_groups = hist[hist['drawdown'] < 0]
    if drawdown_groups.empty:
        return f"No significant drawdowns found for {ticker}. Coverage: {coverage_text}."

    troughs = drawdown_groups.loc[drawdown_groups.groupby((drawdown_groups['drawdown'] == 0).cumsum())['drawdown'].idxmin()]
    top_3 = troughs.nsmallest(3, 'drawdown')
    if top_3.empty:
        return f"No significant drawdowns found for {ticker}. Coverage: {coverage_text}."

    result = [f"Top 3 Historical Drawdowns for {ticker} (coverage {coverage_text}):\n"]
    for _, row in top_3.iterrows():
        trough_date = row.name
        peak_price = row['peak']
        peak_date = hist[(hist.index <= trough_date) & (hist['Close'] == peak_price)].index.max()
        recovery_df = hist[hist.index > trough_date]
        recovery_date_series = recovery_df[recovery_df['Close'] >= peak_price].index
        recovery_date = recovery_date_series[0] if not recovery_date_series.empty else None

        duration = (trough_date - peak_date).days if peak_date is not None else 0
        recovery_days = (recovery_date - trough_date).days if recovery_date is not None else "Ongoing"
        result.append(
            f"- Drawdown: {row['drawdown']:.2%} (from {peak_date.strftime('%Y-%m-%d')} to {trough_date.strftime('%Y-%m-%d')})\n"
            f"  Duration to trough: {duration} days. Recovery time: {recovery_days} days."
        )

    return "\n".join(result)


def get_current_datetime() -> str:
    """è¿”å›å½“å‰æ—¥æœŸå’Œæ—¶é—´"""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def _fetch_with_yahoo_scrape_historical(ticker: str, period: str = "1y") -> dict:
    """
    ç­–ç•¥ 4: æ”¹è¿›çš„ Yahoo Finance ç½‘é¡µæŠ“å–ï¼ˆ2024æœ€æ–°æ–¹æ³•ï¼‰
    ä½¿ç”¨å¤šä¸ªå¤‡ç”¨URLå’Œæ›´å®Œå–„çš„è¯·æ±‚å¤´
    """
    try:
        logger.info(f"[get_stock_historical_data] å°è¯•ä» Yahoo Finance ç½‘é¡µæŠ“å– {ticker}...")
        
        # æ ¹æ® period è®¡ç®—éœ€è¦çš„å¤©æ•°
        period_days = {
            "1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "6mo": 180,
            "1y": 365, "2y": 730, "5y": 1825, "10y": 3650, "max": 10000
        }
        days = period_days.get(period, 365)
        
        # æ”¹è¿›çš„è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼‰
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/csv,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Referer": f"https://finance.yahoo.com/quote/{ticker}/history",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin"
        }
        
        # å°è¯•å¤šä¸ª Yahoo Finance URLï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        urls = [
            f"https://query1.finance.yahoo.com/v7/finance/download/{ticker}",
            f"https://query2.finance.yahoo.com/v7/finance/download/{ticker}",
        ]
        
        for url in urls:
            try:
                params = {
                    "period1": int((datetime.now() - timedelta(days=days)).timestamp()),
                    "period2": int(datetime.now().timestamp()),
                    "interval": "1d",
                    "events": "history",
                    "includeAdjustedClose": "true"
                }
                
                response = _http_get(url, params=params, headers=headers, timeout=20, allow_redirects=True)
                
                if response.status_code == 200 and len(response.text) > 100:  # ç¡®ä¿æœ‰å®é™…æ•°æ®
                    # è§£æ CSV æ•°æ®
                    import io
                    import csv
                    csv_data = io.StringIO(response.text)
                    reader = csv.DictReader(csv_data)
                    
                    kline_data = []
                    for row in reader:
                        try:
                            # è·³è¿‡æ— æ•ˆè¡Œ
                            if not row.get('Date') or not row.get('Close'):
                                continue
                            kline_data.append({
                                "time": row['Date'],
                                "open": float(row['Open']),
                                "high": float(row['High']),
                                "low": float(row['Low']),
                                "close": float(row['Close']),
                                "volume": float(row.get('Volume', 0)) if row.get('Volume') else 0,
                            })
                        except (ValueError, KeyError) as e:
                            continue  # è·³è¿‡æ— æ•ˆè¡Œ
                    
                    if kline_data:
                        logger.info(f"[get_stock_historical_data] Yahoo Finance ç½‘é¡µæŠ“å–æˆåŠŸï¼Œè·å– {len(kline_data)} æ¡æ•°æ®")
                        return {"kline_data": kline_data, "period": period, "interval": "1d", "source": "yahoo_scrape"}
            except Exception as e:
                logger.info(f"[get_stock_historical_data] Yahoo Finance URL {url} å¤±è´¥: {e}")
                continue
        
        return None
    except Exception as e:
        logger.info(f"[get_stock_historical_data] Yahoo Finance ç½‘é¡µæŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def _fetch_with_iex_cloud(ticker: str, period: str = "1y") -> dict:
    """
    ç­–ç•¥ 5a: ä½¿ç”¨ IEX Cloud API (å…è´¹é¢åº¦: 50ä¸‡æ¬¡/æœˆ)
    æ–‡æ¡£: https://iexcloud.io/docs/api/
    """
    try:
        if not IEX_CLOUD_API_KEY:
            return None
            
        logger.info(f"[get_stock_historical_data] å°è¯•ä½¿ç”¨ IEX Cloud {ticker}...")
        
        # IEX Cloud API ç«¯ç‚¹
        # æ ¹æ® period è®¡ç®—æ—¶é—´èŒƒå›´
        period_days = {
            "1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "6mo": 180,
            "1y": 365, "2y": 730, "5y": 1825, "10y": 3650, "max": 10000
        }
        days = period_days.get(period, 365)
        
        # IEX Cloud ä½¿ç”¨ä¸åŒçš„æ—¶é—´èŒƒå›´å‚æ•°
        if days <= 5:
            range_param = "5d"
        elif days <= 30:
            range_param = "1m"
        elif days <= 90:
            range_param = "3m"
        elif days <= 365:
            range_param = "1y"
        elif days <= 730:
            range_param = "2y"
        elif days <= 1825:
            range_param = "5y"
        else:
            range_param = "max"
        
        # IEX Cloud ä¸æ”¯æŒæŒ‡æ•°ä»£ç ï¼ˆå¦‚ ^IXICï¼‰ï¼Œåªæ”¯æŒè‚¡ç¥¨ä»£ç 
        # å¦‚æœtickerä»¥^å¼€å¤´ï¼Œè·³è¿‡IEX Cloud
        if ticker.startswith('^'):
            return None
        
        url = f"https://cloud.iexapis.com/stable/stock/{ticker}/chart/{range_param}"
        params = {
            "token": IEX_CLOUD_API_KEY,
            "chartCloseOnly": "false"
        }
        
        response = _http_get(url, params=params, timeout=20)
        
        if response.status_code == 200:
            data = response.json()
            if isinstance(data, list) and len(data) > 0:
                kline_data = []
                for item in data:
                    kline_data.append({
                        "time": item.get('date', item.get('label', '')),
                        "open": float(item.get('open', 0)),
                        "high": float(item.get('high', 0)),
                        "low": float(item.get('low', 0)),
                        "close": float(item.get('close', 0)),
                        "volume": float(item.get('volume', 0)),
                    })
                
                if kline_data:
                    logger.info(f"[get_stock_historical_data] IEX Cloud æˆåŠŸè·å– {len(kline_data)} æ¡æ•°æ®")
                    return {"kline_data": kline_data, "period": period, "interval": "1d", "source": "iex_cloud"}
        
        return None
    except Exception as e:
        logger.info(f"[get_stock_historical_data] IEX Cloud å¤±è´¥: {e}")
        return None


def _fetch_with_tiingo(ticker: str, period: str = "1y") -> dict:
    """
    ç­–ç•¥ 5b: ä½¿ç”¨ Tiingo API (å…è´¹é¢åº¦: æ¯æ—¥500æ¬¡)
    æ–‡æ¡£: https://api.tiingo.com/documentation/general/overview
    """
    try:
        if not TIINGO_API_KEY:
            return None
            
        logger.info(f"[get_stock_historical_data] å°è¯•ä½¿ç”¨ Tiingo {ticker}...")
        
        # Tiingo API ç«¯ç‚¹
        period_days = {
            "1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "6mo": 180,
            "1y": 365, "2y": 730, "5y": 1825, "10y": 3650, "max": 10000
        }
        days = period_days.get(period, 365)
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        # Tiingo ä¸æ”¯æŒæŒ‡æ•°ä»£ç ï¼ˆå¦‚ ^IXICï¼‰ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        # å¦‚æœtickerä»¥^å¼€å¤´ï¼Œè·³è¿‡Tiingoï¼ˆå› ä¸ºTiingoä¸æ”¯æŒæŒ‡æ•°ï¼‰
        if ticker.startswith('^'):
            return None
        
        url = f"https://api.tiingo.com/tiingo/daily/{ticker}/prices"
        params = {
            "startDate": start_date.strftime('%Y-%m-%d'),
            "endDate": end_date.strftime('%Y-%m-%d'),
            "format": "json"
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Token {TIINGO_API_KEY}"
        }
        
        response = _http_get(url, params=params, headers=headers, timeout=20)
        
        if response.status_code == 200:
            data = response.json()
            if isinstance(data, list) and len(data) > 0:
                kline_data = []
                for item in data:
                    kline_data.append({
                        "time": item.get('date', '')[:10],  # åªå–æ—¥æœŸéƒ¨åˆ†
                        "open": float(item.get('open', 0)),
                        "high": float(item.get('high', 0)),
                        "low": float(item.get('low', 0)),
                        "close": float(item.get('close', 0)),
                        "volume": float(item.get('volume', 0)),
                    })
                
                if kline_data:
                    logger.info(f"[get_stock_historical_data] Tiingo æˆåŠŸè·å– {len(kline_data)} æ¡æ•°æ®")
                    return {"kline_data": kline_data, "period": period, "interval": "1d", "source": "tiingo"}
        elif response.status_code == 404:
            # Tiingo å¯èƒ½ä¸æ”¯æŒè¯¥tickerï¼ˆå¦‚æŒ‡æ•°ï¼‰ï¼Œè¿”å›Noneè®©å…¶ä»–æ•°æ®æºå¤„ç†
            logger.info(f"[get_stock_historical_data] Tiingo ä¸æ”¯æŒ {ticker}ï¼Œè·³è¿‡")
            return None
        
        return None
    except Exception as e:
        logger.info(f"[get_stock_historical_data] Tiingo å¤±è´¥: {e}")
        return None


def _fetch_with_twelve_data(ticker: str, period: str = "1y") -> dict:
    """
    ç­–ç•¥ 5c: ä½¿ç”¨ Twelve Data API (å…è´¹é¢åº¦ï¼Œè½»é‡å›é€€)
    æ–‡æ¡£: https://twelvedata.com/docs#time-series
    """
    try:
        if not TWELVE_DATA_API_KEY:
            return None

        # Twelve Data å¯¹æŒ‡æ•°æ”¯æŒæœ‰é™ï¼Œé¿å… "^" å‰ç¼€çš„æŒ‡æ•°
        if ticker.startswith('^'):
            return None

        logger.info(f"[get_stock_historical_data] å°è¯•ä½¿ç”¨ Twelve Data {ticker}...")

        period_days = {
            "1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "6mo": 180,
            "1y": 365, "2y": 730, "5y": 1825, "10y": 3650, "max": 10000
        }
        days = period_days.get(period, 365)
        outputsize = max(2, min(5000, days + 2))  # è½»é‡æ§åˆ¶è¾“å‡ºï¼Œå…¼é¡¾å…è´¹é¢åº¦

        params = {
            "symbol": ticker,
            "interval": "1day",
            "outputsize": outputsize,
            "apikey": TWELVE_DATA_API_KEY,
            "order": "desc",
        }
        response = _http_get("https://api.twelvedata.com/time_series", params=params, timeout=20)

        if response.status_code != 200:
            return None

        data = response.json()
        if data.get("status") != "ok":
            # status != ok æ—¶é€šå¸¸è¿”å› message
            message = data.get("message") or data.get("error")
            if message:
                logger.info(f"[get_stock_historical_data] Twelve Data çŠ¶æ€å¼‚å¸¸: {message}")
            return None

        values = data.get("values") or []
        if not values:
            return None

        kline_data = []
        for item in values:
            kline_data.append({
                "time": item.get("datetime", "")[:10],
                "open": float(item.get("open", 0)),
                "high": float(item.get("high", 0)),
                "low": float(item.get("low", 0)),
                "close": float(item.get("close", 0)),
                "volume": float(item.get("volume", 0)),
            })

        if kline_data:
            # Twelve Data é»˜è®¤å€’åºï¼Œç¿»è½¬ä¸ºæ—¶é—´æ­£åº
            kline_data = list(reversed(kline_data))
            as_of = values[0].get("datetime", "")[:19]
            logger.info(f"[get_stock_historical_data] Twelve Data æˆåŠŸè·å– {len(kline_data)} æ¡æ•°æ®")
            return {"kline_data": kline_data, "period": period, "interval": "1d", "source": "twelve_data", "as_of": as_of}

        return None
    except Exception as e:
        logger.info(f"[get_stock_historical_data] Twelve Data å¤±è´¥: {e}")
        return None


def _fetch_with_marketstack(ticker: str, period: str = "1y") -> dict:
    """
    ç­–ç•¥ 5d: ä½¿ç”¨ Marketstack API (å…è´¹é¢åº¦: 1000æ¬¡/æœˆ)
    æ–‡æ¡£: https://marketstack.com/documentation
    """
    try:
        if not MARKETSTACK_API_KEY:
            return None
            
        logger.info(f"[get_stock_historical_data] å°è¯•ä½¿ç”¨ Marketstack {ticker}...")
        
        # Marketstack API ç«¯ç‚¹
        url = "http://api.marketstack.com/v1/eod"
        
        # Marketstack ä¸æ”¯æŒæŒ‡æ•°ä»£ç ï¼ˆå¦‚ ^IXICï¼‰ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        # å¦‚æœtickerä»¥^å¼€å¤´ï¼Œè·³è¿‡Marketstackï¼ˆå› ä¸ºMarketstackä¸æ”¯æŒæŒ‡æ•°ï¼‰
        if ticker.startswith('^'):
            return None
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        period_days = {
            "1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "6mo": 180,
            "1y": 365, "2y": 730, "5y": 1825, "10y": 3650, "max": 10000
        }
        days = period_days.get(period, 365)
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        params = {
            "access_key": MARKETSTACK_API_KEY,
            "symbols": ticker,
            "date_from": start_date.strftime('%Y-%m-%d'),
            "date_to": end_date.strftime('%Y-%m-%d'),
            "limit": 10000  # æœ€å¤§é™åˆ¶
        }
        
        response = _http_get(url, params=params, timeout=20)
        
        if response.status_code == 200:
            data = response.json()
            if "error" in data:
                logger.info(f"[get_stock_historical_data] Marketstack é”™è¯¯: {data['error']}")
                return None
            
            if "data" in data and isinstance(data["data"], list) and len(data["data"]) > 0:
                kline_data = []
                for item in data["data"]:
                    kline_data.append({
                        "time": item.get('date', '')[:10],  # åªå–æ—¥æœŸéƒ¨åˆ†
                        "open": float(item.get('open', 0)),
                        "high": float(item.get('high', 0)),
                        "low": float(item.get('low', 0)),
                        "close": float(item.get('close', 0)),
                        "volume": float(item.get('volume', 0)),
                    })
                
                if kline_data:
                    logger.info(f"[get_stock_historical_data] Marketstack æˆåŠŸè·å– {len(kline_data)} æ¡æ•°æ®")
                    return {"kline_data": kline_data, "period": period, "interval": "1d", "source": "marketstack"}
        
        return None
    except Exception as e:
        logger.info(f"[get_stock_historical_data] Marketstack å¤±è´¥: {e}")
        return None


def _fetch_with_massive_io(ticker: str, period: str = "1y") -> dict:
    """
    ç­–ç•¥ 5e: ä½¿ç”¨ Massive.com (åŸ Polygon.io) API
    """
    try:
        if not MASSIVE_API_KEY:
            logger.info(f"[get_stock_historical_data] Massive.com API key æœªé…ç½®")
            return None
            
        logger.info(f"[get_stock_historical_data] å°è¯•ä½¿ç”¨ Massive.com {ticker}...")
        
        # Massive.com (åŸ Polygon.io) API ç«¯ç‚¹
        # æ³¨æ„ï¼šPolygon.io å·²æ›´åä¸º Massive.comï¼Œä½† API ç«¯ç‚¹ä»ä¸º api.polygon.io
        # API æ ¼å¼: /v2/aggs/ticker/{ticker}/range/{multiplier}/{timespan}/{from}/{to}
        # æ—¥æœŸå¿…é¡»ä½œä¸ºè·¯å¾„å‚æ•°ï¼Œä¸èƒ½ä½œä¸ºæŸ¥è¯¢å‚æ•°
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        period_days = {
            "1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "6mo": 180,
            "1y": 365, "2y": 730, "5y": 1825, "10y": 3650, "max": 10000
        }
        days = period_days.get(period, 365)
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        # æ—¥æœŸä½œä¸ºè·¯å¾„å‚æ•°
        url = f"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date.strftime('%Y-%m-%d')}/{end_date.strftime('%Y-%m-%d')}"
        
        params = {
            "adjusted": "true",
            "sort": "asc",
            "limit": 50000,
            "apikey": MASSIVE_API_KEY  # Massive.com API key ä½œä¸ºæŸ¥è¯¢å‚æ•°
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = _http_get(url, params=params, headers=headers, timeout=20)
        
        if response.status_code == 200:
            data = response.json()
            # Massive.com API å¯èƒ½è¿”å› 'OK' æˆ– 'DELAYED' çŠ¶æ€ï¼Œåªè¦ results æœ‰æ•°æ®å°±å¯ä»¥ä½¿ç”¨
            # DELAYED çŠ¶æ€è¡¨ç¤ºæ•°æ®æœ‰å»¶è¿Ÿï¼Œä½†ä»ç„¶å¯ä»¥ä½¿ç”¨
            if data.get('status') in ('OK', 'DELAYED') and 'results' in data:
                results = data.get('results', [])
                if len(results) > 0:
                    kline_data = []
                    for item in results:
                        timestamp = item['t'] / 1000  # è½¬æ¢ä¸ºç§’
                        date_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')
                        kline_data.append({
                            "time": date_str,
                            "open": item['o'],
                            "high": item['h'],
                            "low": item['l'],
                            "close": item['c'],
                            "volume": item.get('v', 0),
                        })
                    
                    if kline_data:
                        logger.info(f"[get_stock_historical_data] Massive.com æˆåŠŸè·å– {len(kline_data)} æ¡æ•°æ®")
                        return {"kline_data": kline_data, "period": period, "interval": "1d", "source": "massive"}
            else:
                error_msg = data.get('error', data.get('status', 'unknown'))
                logger.info(f"[get_stock_historical_data] Massive.com è¿”å›ç©ºæ•°æ®æˆ–é”™è¯¯: {error_msg}")
                if 'error' in data:
                    logger.info(f"[get_stock_historical_data] é”™è¯¯è¯¦æƒ…: {data.get('error')}")
        else:
            error_text = response.text[:500] if response.text else "No response body"
            logger.info(f"[get_stock_historical_data] Massive.com HTTP é”™è¯¯: {response.status_code}")
            logger.info(f"[get_stock_historical_data] å“åº”å†…å®¹: {error_text}")
            # å°è¯•è§£æ JSON é”™è¯¯ä¿¡æ¯
            try:
                error_data = response.json()
                if 'error' in error_data:
                    logger.info(f"[get_stock_historical_data] API é”™è¯¯: {error_data['error']}")
            except:
                pass
        
        return None
    except Exception as e:
        logger.info(f"[get_stock_historical_data] Massive.com å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def _map_to_stooq_symbol(ticker: str) -> Optional[str]:
    """
    å°† ticker æ˜ å°„åˆ° Stooq æ ¼å¼ã€‚
    æ³¨æ„ï¼šStooq ä¸æ”¯æŒåŠ å¯†è´§å¸å’Œ A è‚¡ï¼Œè¿”å› None è·³è¿‡ã€‚
    """
    upper = ticker.upper()

    # ä¸æ”¯æŒçš„ ticker ç±»å‹ - è¿”å› None è·³è¿‡
    # åŠ å¯†è´§å¸
    if any(crypto in upper for crypto in ['BTC', 'ETH', 'USDT', 'BNB', 'XRP', 'SOL', 'DOGE', 'ADA']):
        return None
    # A è‚¡æŒ‡æ•°å’Œè‚¡ç¥¨
    if upper.endswith('.SS') or upper.endswith('.SZ') or upper.startswith('000') or upper.startswith('600') or upper.startswith('300'):
        return None
    # å•†å“æœŸè´§ï¼ˆStooq æ ¼å¼ä¸åŒï¼‰
    if '=' in upper:
        return None

    # å·²çŸ¥çš„æŒ‡æ•°æ˜ å°„
    mapping = {
        "^IXIC": "^ndq",
        "^GSPC": "^spx",
        "^DJI": "^dji",
        "^RUT": "^rut",
        "^VIX": "^vix",
    }
    if upper in mapping:
        return mapping[upper]
    if upper.startswith("^"):
        return upper.lower()
    return f"{upper}.us"


def _fetch_with_stooq_history(ticker: str, period: str = "1y", interval: str = "1d") -> Optional[dict]:
    """
    å… Key å›é€€ï¼šä½¿ç”¨ stooq è·å–æ—¥çº¿æ•°æ®ï¼ˆæ”¯æŒéƒ¨åˆ†æŒ‡æ•°å’Œç¾è‚¡ï¼Œä»£ç å¸¦ .usï¼‰ã€‚
    """
    try:
        import requests  # type: ignore
        import csv
        from datetime import date, timedelta

        symbol = _map_to_stooq_symbol(ticker)
        if not symbol:
            return None

        days_map = {
            "1d": 5, "5d": 10, "1mo": 40, "3mo": 120, "6mo": 200,
            "1y": 365, "2y": 730, "5y": 1825, "10y": 3650, "max": 3650
        }
        days = days_map.get(period, 365)
        end = date.today()
        start = end - timedelta(days=days)
        url = f"https://stooq.pl/q/d/l/?s={symbol}&d1={start:%Y%m%d}&d2={end:%Y%m%d}&i=d"
        resp = _http_get(url, timeout=8)
        if resp.status_code != 200 or not resp.text:
            return None

        lines = resp.text.strip().splitlines()
        reader = csv.DictReader(lines)
        data = []
        for row in reader:
            try:
                date_key = "Date" if "Date" in row else ("Data" if "Data" in row else None)
                open_key = "Open" if "Open" in row else ("Otwarcie" if "Otwarcie" in row else None)
                high_key = "High" if "High" in row else ("Najwyzszy" if "Najwyzszy" in row else None)
                low_key = "Low" if "Low" in row else ("Najnizszy" if "Najnizszy" in row else None)
                close_key = "Close" if "Close" in row else ("Zamkniecie" if "Zamkniecie" in row else None)
                volume_key = "Volume" if "Volume" in row else ("Wolumen" if "Wolumen" in row else None)
                if not all([date_key, open_key, high_key, low_key, close_key]):
                    continue
                close_val = float(row[close_key])
                if close_val <= 0 or close_val > 1e8:
                    continue
                data.append(
                    {
                        "time": f"{row[date_key]} 00:00",
                        "open": float(row[open_key]),
                        "high": float(row[high_key]),
                        "low": float(row[low_key]),
                        "close": close_val,
                        "volume": float(row.get(volume_key) or 0),
                    }
                )
            except Exception:
                continue

        if data:
            logger.info(f"[get_stock_historical_data] Stooq æˆåŠŸè·å– {len(data)} æ¡æ•°æ®")
            # å¦‚æœè¯·æ±‚çš„æ˜¯å°æ—¶è§†å›¾ï¼Œä½†åªæ‹¿åˆ°æ—¥çº¿ï¼Œç”¨æœ€è¿‘è‹¥å¹²æ—¥æ”¶ç›˜ç”Ÿæˆä¼ªâ€œå°æ—¶â€åºåˆ—ï¼Œä¿è¯æœ‰å˜åŒ–
            if interval.endswith("h"):
                # å–æœ€è¿‘10ä¸ªäº¤æ˜“æ—¥çš„æ”¶ç›˜ï¼Œæ ‡è®°ä¸ºå½“æ—¥ 16:00
                recent = data[-10:]
                hourly_like = []
                for row in recent:
                    close_val = row["close"]
                    if close_val <= 0 or close_val > 1e8:
                        continue
                    hourly_like.append({
                        "time": row["time"].split()[0] + " 16:00",
                        "open": close_val,
                        "high": close_val,
                        "low": close_val,
                        "close": close_val,
                        "volume": row.get("volume", 0.0),
                    })
                if not hourly_like:
                    return None
                return {"kline_data": hourly_like, "period": period, "interval": "1h", "source": "stooq_intraday_stub"}
            return {"kline_data": data, "period": period, "interval": "1d", "source": "stooq"}
        return None
    except Exception as e:
        logger.info(f"[get_stock_historical_data] Stooq å¤±è´¥: {e}")
        return None


def _fallback_price_value(ticker: str) -> Optional[float]:
    """
    ç®€å•å…œåº•ï¼šå°è¯•ç”¨ stooq ä»·æ ¼æ¥å£æˆ–æœç´¢æå–ä¸€ä¸ªæœ€æ–°ä»·ï¼Œç”¨äºç”Ÿæˆå¹³æ»‘åºåˆ—ã€‚
    """
    try:
        symbol = _map_to_stooq_symbol(ticker)
        if symbol:
            url = f"https://stooq.pl/q/l/?s={symbol}&f=sd2t2ohlcv&h&e=json"
            resp = _http_get(url, timeout=6)
            if resp.status_code == 200:
                data = resp.json().get("symbols") or []
                if data:
                    close = data[0].get("close")
                    if close not in (None, "N/D"):
                        return float(close)
    except Exception:
        pass

    # æœç´¢å…œåº•
    try:
        search_result = search(f"{ticker} index level today")
        m = re.search(r"(\\d{3,6}(?:,\\d{3})*(?:\\.\\d+)?)", search_result or "")
        if m:
            val = float(m.group(1).replace(",", ""))
            if val <= 0 or val > 1e8:
                return None
            return val
    except Exception:
        pass
    return None


def get_stock_historical_data(ticker: str, period: str = "1y", interval: str = "1d") -> dict:
    """
    è·å–è‚¡ç¥¨çš„å†å²æ•°æ®ï¼Œç”¨äºKçº¿å›¾ã€‚
    è¿”å›çš„æ•°æ®æ ¼å¼ä¸“é—¨ä¸º ECharts ä¼˜åŒ–ã€‚
    ä½¿ç”¨å¤šæºå›é€€ç­–ç•¥ï¼šyfinance (ä¼˜å…ˆï¼Œæœ€å¯é ) â†’ Alpha Vantage â†’ Finnhub â†’ Yahoo ç½‘é¡µæŠ“å– â†’ IEX Cloud â†’ Tiingo â†’ Twelve Data â†’ Marketstack â†’ Massive.com â†’ Stooq
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç 
        period: æ—¶é—´å‘¨æœŸ ("1d", "5d", "1mo", "3mo", "6mo", "1y", "2y", "5y", "10y", "ytd", "max")
        interval: æ•°æ®é—´éš” ("1d", "1wk", "1mo")
    
    Returns:
        dict: {"kline_data": [...]} æˆ– {"error": "..."}
    """
    # æŒ‡æ•°ä¼˜å…ˆå°è¯• Stooqï¼ˆå… Keyï¼Œé¿å… yfinance é€Ÿç‡é™åˆ¶ï¼‰
    is_index = ticker.startswith("^")
    if is_index:
        stooq_result = _fetch_with_stooq_history(ticker, period, interval)
        if stooq_result and stooq_result.get("kline_data"):
            logger.info(f"[get_stock_historical_data] Stooq æŒ‡æ•°å…œåº•å‘½ä¸­ {ticker}ï¼Œè¿”å›æ—¥çº¿æ•°æ®")
            return stooq_result

    # ç­–ç•¥ 0: ä¼˜å…ˆä½¿ç”¨ yfinanceï¼ˆæœ€å¯é ï¼Œæ”¯æŒè‚¡ç¥¨å’ŒæŒ‡æ•°ï¼‰
    # ä½¿ç”¨ session å’Œé‡è¯•æœºåˆ¶ï¼Œé¿å…é€Ÿç‡é™åˆ¶
    max_retries = 1  # é™æµä¸¥é‡æ—¶å¿«é€Ÿè·³è¿‡
    for attempt in range(max_retries):
        try:
            logger.info(f"[get_stock_historical_data] å°è¯•ä½¿ç”¨ yfinance {ticker} (å°è¯• {attempt + 1}/{max_retries})...")
            
            # åˆ›å»ºæ–°çš„ sessionï¼Œé¿å…ç¼“å­˜é—®é¢˜
            import yfinance as yf_local
            stock = yf_local.Ticker(ticker, session=None)  # ä¸ä½¿ç”¨ç¼“å­˜
            
            # å¯¹äºæŒ‡æ•°ï¼Œä½¿ç”¨ä¸åŒçš„å‚æ•°
            include_time = interval.endswith('h') or interval.endswith('m')
            if ticker.startswith('^'):
                hist = stock.history(period=period, interval=interval, timeout=30, raise_errors=True)
            else:
                hist = stock.history(period=period, interval=interval, timeout=30, raise_errors=True)
            
            if not hist.empty and len(hist) > 0:
                data = []
                for index, row in hist.iterrows():
                    # å¤„ç†æ—¥æœŸ/æ—¶é—´æ ¼å¼
                    if include_time and hasattr(index, 'to_pydatetime'):
                        time_str = index.to_pydatetime().strftime('%Y-%m-%d %H:%M')
                    elif hasattr(index, 'strftime'):
                        time_str = index.strftime('%Y-%m-%d')
                    elif hasattr(index, 'date'):
                        time_str = index.date().strftime('%Y-%m-%d')
                    else:
                        time_str = str(index)[:10]
                    
                    time_value = time_str if include_time else f"{time_str} 00:00"
                    data.append({
                        "time": time_value,
                        "open": float(row['Open']),
                        "high": float(row['High']),
                        "low": float(row['Low']),
                        "close": float(row['Close']),
                        "volume": float(row.get('Volume', 0)) if 'Volume' in row else 0,
                    })
                
                if data:
                    logger.info(f"[get_stock_historical_data] âœ… yfinance æˆåŠŸè·å– {len(data)} æ¡æ•°æ® (æ¥æº: yfinance)")
                    return {"kline_data": data, "period": period, "interval": interval, "source": "yfinance"}
        except Exception as e:
            error_msg = str(e)
            if "Too Many Requests" in error_msg or "Rate limited" in error_msg:
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    logger.info(f"[get_stock_historical_data] yfinance é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    import time as time_module
                    time_module.sleep(wait_time)
                    continue
            logger.info(f"[get_stock_historical_data] yfinance å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                break
    
    # ç­–ç•¥ 1: å°è¯•ä½¿ç”¨ Alpha Vantage
    # æ³¨æ„ï¼šAlpha Vantage ä¸æ”¯æŒæŒ‡æ•°ä»£ç ï¼ˆå¦‚ ^IXICï¼‰ï¼Œå¯¹äºæŒ‡æ•°ç›´æ¥è·³è¿‡
    if ALPHA_VANTAGE_API_KEY and not ticker.startswith('^'):
        try:
            # å¯¹äºæŒ‡æ•°ä»£ç ï¼Œç§»é™¤^ç¬¦å·
            ticker_for_av = ticker.lstrip('^')
            url = f"https://www.alphavantage.co/query"
            params = {
                "function": "TIME_SERIES_DAILY",
                "symbol": ticker_for_av,
                "apikey": ALPHA_VANTAGE_API_KEY,
                "outputsize": "full"
            }
            response = _http_get(url, params=params, timeout=15)
            data = response.json()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
            if "Error Message" in data:
                error_msg = data.get('Error Message', 'Unknown error')
                logger.info(f"[get_stock_historical_data] Alpha Vantage è¿”å›é”™è¯¯: {error_msg}")
                raise Exception(f"Alpha Vantage API error: {error_msg}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é€Ÿç‡é™åˆ¶æç¤º
            if "Note" in data:
                note = data.get('Note', '')
                if "API call frequency" in note or "rate limit" in note.lower():
                    logger.info(f"[get_stock_historical_data] Alpha Vantage é€Ÿç‡é™åˆ¶: {note}")
                    raise Exception("Alpha Vantage rate limit")
                else:
                    logger.info(f"[get_stock_historical_data] Alpha Vantage æç¤º: {note}")
                    raise Exception(f"Alpha Vantage note: {note}")
            
            if "Time Series (Daily)" in data:
                time_series = data["Time Series (Daily)"]
                # æ ¹æ® period ç¡®å®šéœ€è¦çš„æ•°æ®é‡
                period_days = {
                    "1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "6mo": 180,
                    "1y": 252, "2y": 504, "5y": 1260, "10y": 2520, "max": 10000
                }
                max_days = period_days.get(period, 252)
                
                sorted_dates = sorted(time_series.keys(), reverse=True)[:max_days]
                
                kline_data = []
                for date_str in sorted_dates:
                    day_data = time_series[date_str]
                    kline_data.append({
                        "time": date_str,
                        "open": float(day_data["1. open"]),
                        "high": float(day_data["2. high"]),
                        "low": float(day_data["3. low"]),
                        "close": float(day_data["4. close"]),
                        "volume": float(day_data.get("5. volume", 0)),
                    })
                
                # æŒ‰æ—¶é—´æ­£åºæ’åˆ—
                kline_data.reverse()
                logger.info(f"[get_stock_historical_data] Alpha Vantage æˆåŠŸè·å– {len(kline_data)} æ¡æ•°æ®")
                return {"kline_data": kline_data, "period": period, "interval": interval}
        except Exception as e:
            logger.info(f"[get_stock_historical_data] Alpha Vantage å¤±è´¥: {e}ï¼Œå°è¯• yfinance...")
    
    # ç­–ç•¥ 2: å›é€€åˆ° yfinanceï¼ˆæ”¯æŒå¤šæ—¶é—´å‘¨æœŸï¼Œå¸¦é‡è¯•ï¼‰
    # æ³¨æ„ï¼šyfinance å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
    # yfinance æ”¯æŒæŒ‡æ•°ä»£ç ï¼ˆå¦‚ ^IXIC, ^GSPCï¼‰ï¼Œè¿™æ˜¯è·å–æŒ‡æ•°æ•°æ®çš„ä¸»è¦æ–¹æ³•
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # yfinance æ”¯æŒæŒ‡æ•°ä»£ç ï¼Œç›´æ¥ä½¿ç”¨
            stock = yf.Ticker(ticker)
            
            # æ ¹æ® period å’Œ interval è·å–æ•°æ®
            # yfinance æ”¯æŒçš„ period: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max
            # yfinance æ”¯æŒçš„ interval: 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo
            # å¯¹äºæŒ‡æ•°ï¼Œyfinance é€šå¸¸èƒ½æ­£å¸¸å·¥ä½œ
            hist = stock.history(period=period, interval=interval, timeout=15)
            
            if hist.empty:
                if attempt < max_retries - 1:
                    logger.info(f"[get_stock_historical_data] yfinance è¿”å›ç©ºæ•°æ®ï¼Œé‡è¯• {attempt + 1}/{max_retries}...")
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                return {"error": f"No historical data for {ticker}"}

            # è½¬æ¢æ ¼å¼ä»¥åŒ¹é… ECharts çš„è¦æ±‚
            include_time = interval.endswith('h') or interval.endswith('m')
            data = []
            for index, row in hist.iterrows():
                # Normalize timestamp for chart rows
                if include_time and hasattr(index, 'to_pydatetime'):
                    time_str = index.to_pydatetime().strftime('%Y-%m-%d %H:%M')
                elif hasattr(index, 'strftime'):
                    time_str = index.strftime('%Y-%m-%d')
                elif hasattr(index, 'date'):
                    time_str = index.date().strftime('%Y-%m-%d')
                else:
                    time_str = str(index)[:10]
                time_value = time_str if include_time else f"{time_str} 00:00"
                data.append({
                    "time": time_value,
                    "open": float(row['Open']),
                    "high": float(row['High']),
                    "low": float(row['Low']),
                    "close": float(row['Close']),
                    "volume": float(row.get('Volume', 0)) if 'Volume' in row else 0,
                })

            logger.info(f"[get_stock_historical_data] yfinance success with {len(data)} rows")
            return {"kline_data": data, "period": period, "interval": interval}
        except Exception as e:
            error_msg = str(e)
            if "Too Many Requests" in error_msg or "Rate limited" in error_msg:
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    logger.info(f"[get_stock_historical_data] yfinance é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    import time as time_module
                    time_module.sleep(wait_time)
                    continue
            # å¦‚æœä¸æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œæˆ–è€…å·²ç»é‡è¯•å®Œï¼Œç»§ç»­åˆ°ä¸‹ä¸€ä¸ªç­–ç•¥
            logger.info(f"[get_stock_historical_data] yfinance å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                break  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œç»§ç»­åˆ°ä¸‹ä¸€ä¸ªç­–ç•¥
    
    # ç­–ç•¥ 3: å°è¯•ä½¿ç”¨ Finnhubï¼ˆå¦‚æœæœ‰ API keyï¼‰
    if FINNHUB_API_KEY and finnhub_client:
        try:
            import time
            from datetime import datetime, timedelta
            
            # æ ¹æ® period è®¡ç®—å¤©æ•°
            period_days = {
                "1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "6mo": 180,
                "1y": 365, "2y": 730, "5y": 1825, "10y": 3650, "max": 10000
            }
            days = period_days.get(period, 365)
            
            end_date = int(time.time())
            start_date = int((datetime.now() - timedelta(days=days)).timestamp())
            
            res = finnhub_client.stock_candles(ticker, 'D', start_date, end_date)
            
            if res['s'] == 'ok' and len(res['c']) > 0:
                kline_data = []
                for i in range(len(res['t'])):
                    timestamp = res['t'][i]
                    date_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')
                    kline_data.append({
                        "time": date_str,
                        "open": res['o'][i],
                        "high": res['h'][i],
                        "low": res['l'][i],
                        "close": res['c'][i],
                        "volume": res.get('v', [0] * len(res['t']))[i] if 'v' in res else 0,
                    })
                logger.info(f"[get_stock_historical_data] Finnhub æˆåŠŸè·å– {len(kline_data)} æ¡æ•°æ®")
                return {"kline_data": kline_data, "period": period, "interval": interval}
        except Exception as e2:
            logger.info(f"[get_stock_historical_data] Finnhub ä¹Ÿå¤±è´¥: {e2}")
    
    # ç­–ç•¥ 4: å°è¯•ä» Yahoo Finance ç½‘é¡µç›´æ¥æŠ“å–ï¼ˆå¯¹æŒ‡æ•°ä»£ç ç‰¹åˆ«æœ‰æ•ˆï¼‰
    try:
        result = _fetch_with_yahoo_scrape_historical(ticker, period)
        if result and "kline_data" in result and len(result["kline_data"]) > 0:
            return result
    except Exception as e3:
        logger.info(f"[get_stock_historical_data] Yahoo Finance ç½‘é¡µæŠ“å–å¤±è´¥: {e3}")
    
    # å¯¹äºæŒ‡æ•°ä»£ç ï¼Œä¼˜å…ˆä½¿ç”¨ yfinanceï¼ˆå³ä½¿ä¹‹å‰å¤±è´¥ï¼Œå†è¯•ä¸€æ¬¡ï¼Œå› ä¸ºæŒ‡æ•°å¯èƒ½æ”¯æŒï¼‰
    if ticker.startswith('^'):
        logger.info(f"[get_stock_historical_data] æ£€æµ‹åˆ°æŒ‡æ•°ä»£ç  {ticker}ï¼Œå°è¯•ä½¿ç”¨ yfinance ä¸“é—¨è·å–æŒ‡æ•°æ•°æ®...")
        try:
            # å¯¹äºæŒ‡æ•°ï¼Œyfinance é€šå¸¸æ”¯æŒï¼Œä½†å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
            stock = yf.Ticker(ticker)
            hist = stock.history(period=period, interval=interval, timeout=20)
            
            if not hist.empty:
                include_time = interval.endswith('h') or interval.endswith('m')
                data = []
                for index, row in hist.iterrows():
                    if include_time and hasattr(index, 'to_pydatetime'):
                        time_str = index.to_pydatetime().strftime('%Y-%m-%d %H:%M')
                    elif hasattr(index, 'strftime'):
                        time_str = index.strftime('%Y-%m-%d')
                    elif hasattr(index, 'date'):
                        time_str = index.date().strftime('%Y-%m-%d')
                    else:
                        time_str = str(index)[:10]
                    
                    time_value = time_str if include_time else f"{time_str} 00:00"
                    data.append({
                        "time": time_value,
                        "open": float(row['Open']),
                        "high": float(row['High']),
                        "low": float(row['Low']),
                        "close": float(row['Close']),
                        "volume": float(row.get('Volume', 0)),
                    })
                
                if data:
                    logger.info(f"[get_stock_historical_data] yfinance æˆåŠŸè·å–æŒ‡æ•° {ticker} çš„ {len(data)} æ¡æ•°æ®")
                    return {"kline_data": data, "period": period, "interval": interval, "source": "yfinance_index"}
        except Exception as e_index:
            logger.info(f"[get_stock_historical_data] yfinance è·å–æŒ‡æ•°æ•°æ®å¤±è´¥: {e_index}")
    
    # ç­–ç•¥ 5a: å°è¯•ä½¿ç”¨ IEX Cloud (å…è´¹é¢åº¦å¤§ï¼Œä¼˜å…ˆä½¿ç”¨)
    try:
        result = _fetch_with_iex_cloud(ticker, period)
        if result and "kline_data" in result and len(result["kline_data"]) > 0:
            return result
    except Exception as e4a:
        logger.info(f"[get_stock_historical_data] IEX Cloud å¤±è´¥: {e4a}")
    
    # ç­–ç•¥ 5b: å°è¯•ä½¿ç”¨ Tiingo (å…è´¹é¢åº¦: æ¯æ—¥500æ¬¡)
    try:
        result = _fetch_with_tiingo(ticker, period)
        if result and "kline_data" in result and len(result["kline_data"]) > 0:
            return result
    except Exception as e4b:
        logger.info(f"[get_stock_historical_data] Tiingo å¤±è´¥: {e4b}")
    
    # ç­–ç•¥ 5c: å°è¯•ä½¿ç”¨ Twelve Data (å…è´¹é¢åº¦)
    try:
        result = _fetch_with_twelve_data(ticker, period)
        if result and "kline_data" in result and len(result["kline_data"]) > 0:
            return result
    except Exception as e4c:
        logger.info(f"[get_stock_historical_data] Twelve Data å¤±è´¥: {e4c}")
    
    # ç­–ç•¥ 5d: å°è¯•ä½¿ç”¨ Marketstack (å…è´¹é¢åº¦: 1000æ¬¡/æœˆ)
    try:
        result = _fetch_with_marketstack(ticker, period)
        if result and "kline_data" in result and len(result["kline_data"]) > 0:
            return result
    except Exception as e4d:
        logger.info(f"[get_stock_historical_data] Marketstack å¤±è´¥: {e4d}")
    
    # ç­–ç•¥ 5e: å°è¯•ä½¿ç”¨ Massive.com (åŸ Polygon.io)
    try:
        result = _fetch_with_massive_io(ticker, period)
        if result and "kline_data" in result and len(result["kline_data"]) > 0:
            return result
    except Exception as e4e:
        logger.info(f"[get_stock_historical_data] Massive.com å¤±è´¥: {e4e}")

    # ç­–ç•¥ 5f: å°è¯• Stooq å… Key å›é€€
    try:
        result = _fetch_with_stooq_history(ticker, period, interval)
        if result and "kline_data" in result and len(result["kline_data"]) > 0:
            return result
    except Exception as e4f:
        logger.info(f"[get_stock_historical_data] Stooq å¤±è´¥: {e4f}")

    # ç­–ç•¥ 6: æœ€åå°è¯• - ä½¿ç”¨ yfinance çš„å¤‡ç”¨æ–¹æ³•ï¼ˆä¸é€šè¿‡ Tickerï¼Œç›´æ¥ä¸‹è½½ï¼‰
    # ç­‰å¾…ä¸€æ®µæ—¶é—´åå†å°è¯•ï¼Œé¿å…é€Ÿç‡é™åˆ¶
    import time as time_module
    time_module.sleep(2)  # ç­‰å¾…2ç§’ï¼Œé¿å…é€Ÿç‡é™åˆ¶
    
    try:
        logger.info(f"[get_stock_historical_data] å°è¯• yfinance å¤‡ç”¨æ–¹æ³•ï¼ˆç­‰å¾…åé‡è¯•ï¼‰...")
        # ä½¿ç”¨ yfinance çš„ download å‡½æ•°ï¼ˆyf å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼‰
        from datetime import datetime, timedelta
        
        period_days = {
            "1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "6mo": 180,
            "1y": 365, "2y": 730, "5y": 1825, "10y": 3650, "max": 10000
        }
        days = period_days.get(period, 365)
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        # ä½¿ç”¨ yfinance.download ç›´æ¥ä¸‹è½½
        hist = yf.download(
            ticker,
            start=start_date.strftime('%Y-%m-%d'),
            end=end_date.strftime('%Y-%m-%d'),
            progress=False,
            timeout=20
        )
        
        if not hist.empty:
            include_time = interval.endswith('h') or interval.endswith('m')
            data = []
            for index, row in hist.iterrows():
                if include_time and hasattr(index, 'to_pydatetime'):
                    time_str = index.to_pydatetime().strftime('%Y-%m-%d %H:%M')
                elif hasattr(index, 'strftime'):
                    time_str = index.strftime('%Y-%m-%d')
                elif hasattr(index, 'date'):
                    time_str = index.date().strftime('%Y-%m-%d')
                else:
                    time_str = str(index)[:10]
                
                time_value = time_str if include_time else f"{time_str} 00:00"
                data.append({
                    "time": time_value,
                    "open": float(row['Open']),
                    "high": float(row['High']),
                    "low": float(row['Low']),
                    "close": float(row['Close']),
                    "volume": float(row.get('Volume', 0)) if 'Volume' in row else 0,
                })
            
            if data:
                logger.info(f"[get_stock_historical_data] yfinance å¤‡ç”¨æ–¹æ³•æˆåŠŸè·å– {len(data)} æ¡æ•°æ®")
                return {"kline_data": data, "period": period, "interval": interval}
    except Exception as e5:
        logger.info(f"[get_stock_historical_data] yfinance å¤‡ç”¨æ–¹æ³•å¤±è´¥: {e5}")
    
    # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œå¦‚æœæ˜¯æŒ‡æ•°ï¼Œå°è¯•ä½¿ç”¨æœ€æ–°ä»·æ ¼ç”Ÿæˆå¹³æ»‘åºåˆ—
    if is_index:
        price_val = _fallback_price_value(ticker)
        if price_val and 0 < price_val <= 1e8:
            from datetime import datetime, timedelta
            data = []
            if interval.endswith('h'):
                # ç”Ÿæˆè¿‡å»24å°æ—¶çš„é€å°æ—¶å¹³æ»‘åºåˆ—
                now = datetime.utcnow()
                for i in range(24, 0, -1):
                    t = now - timedelta(hours=i)
                    data.append({
                        "time": t.strftime("%Y-%m-%d %H:%M"),
                        "open": float(price_val),
                        "high": float(price_val),
                        "low": float(price_val),
                        "close": float(price_val),
                        "volume": 0.0,
                    })
                logger.info(f"[get_stock_historical_data] ä½¿ç”¨ price fallback ä¸º {ticker} ç”Ÿæˆé€å°æ—¶åºåˆ—")
                return {"kline_data": data, "period": period, "interval": interval, "source": "price_fallback_hourly"}
            else:
                from datetime import date
                end = date.today()
                for i in range(5, 0, -1):
                    d = end - timedelta(days=i)
                    data.append({
                        "time": d.strftime("%Y-%m-%d"),
                        "open": float(price_val),
                        "high": float(price_val),
                        "low": float(price_val),
                        "close": float(price_val),
                        "volume": 0.0,
                    })
                logger.info(f"[get_stock_historical_data] ä½¿ç”¨ price fallback ä¸º {ticker} ç”Ÿæˆå¹³æ»‘åºåˆ—")
                return {"kline_data": data, "period": period, "interval": "1d", "source": "price_fallback"}

    # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯
    return {"error": f"Failed to fetch historical data for {ticker}: All data sources failed. Please try again later or check your internet connection."}


def fetch_url_content(url: str, max_length: int = 5000) -> Optional[str]:
    """
    æŠ“å– URL å†…å®¹å¹¶æå–æ­£æ–‡æ–‡æœ¬
    ç”¨äºä»æ–°é—»é“¾æ¥ä¸­æå–å†…å®¹ä¾›ä¸Šä¸‹æ–‡åˆ†æ

    Args:
        url: è¦æŠ“å–çš„ URL
        max_length: è¿”å›å†…å®¹çš„æœ€å¤§é•¿åº¦

    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        if not is_safe_url(url):
            logger.info(f"[fetch_url_content] Blocked unsafe url: {url}")
            return None
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
        }

        response = _http_get(url, headers=headers, timeout=15, allow_redirects=True)
        if response.url and not is_safe_url(response.url):
            logger.info(f"[fetch_url_content] Blocked unsafe redirect: {response.url}")
            return None
        response.raise_for_status()

        # ä½¿ç”¨ BeautifulSoup è§£æ HTML
        soup = BeautifulSoup(response.text, "html.parser")

        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for tag in soup(["script", "style", "nav", "footer", "header", "aside", "iframe", "noscript"]):
            tag.decompose()

        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = None
        for selector in ["article", "main", ".article-content", ".post-content", ".entry-content", "#content", ".content"]:
            main_content = soup.select_one(selector)
            if main_content:
                break

        # å¦‚æœæ²¡æ‰¾åˆ°ä¸»è¦å†…å®¹ï¼Œä½¿ç”¨ body
        if not main_content:
            main_content = soup.body if soup.body else soup

        # æå–æ–‡æœ¬
        text = main_content.get_text(separator="\n", strip=True)

        # æ¸…ç†å¤šä½™ç©ºç™½
        lines = [line.strip() for line in text.split("\n") if line.strip()]
        text = "\n".join(lines)

        # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
        if len(text) > max_length:
            text = text[:max_length] + "..."

        logger.info(f"[fetch_url_content] æˆåŠŸæŠ“å– {url[:50]}... ({len(text)} å­—ç¬¦)")
        return text

    except requests.exceptions.Timeout:
        logger.info(f"[fetch_url_content] è¶…æ—¶: {url}")
        return None
    except requests.exceptions.RequestException as e:
        logger.info(f"[fetch_url_content] è¯·æ±‚å¤±è´¥: {url}, error: {e}")
        return None
    except Exception as e:
        logger.info(f"[fetch_url_content] è§£æå¤±è´¥: {url}, error: {e}")
        return None

