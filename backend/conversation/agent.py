# -*- coding: utf-8 -*-
"""
ConversationAgent - å¯¹è¯å¼ Agent ç»Ÿä¸€å…¥å£
æ•´åˆ Routerã€Contextã€Handlers æä¾›ç»Ÿä¸€çš„å¯¹è¯æ¥å£
"""

import sys
import os
import asyncio
from typing import Dict, Any, Optional, Generator, List
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from backend.conversation.context import ContextManager
from backend.conversation.router import ConversationRouter, Intent
from backend.handlers.chat_handler import ChatHandler
from backend.handlers.report_handler import ReportHandler
from backend.handlers.followup_handler import FollowupHandler
from backend.orchestration.supervisor import AgentSupervisor


class ConversationAgent:
    """
    å¯¹è¯å¼è‚¡ç¥¨åˆ†æ Agent

    ç»Ÿä¸€å…¥å£ï¼Œæ•´åˆï¼š
    - ConversationRouter: æ„å›¾è¯†åˆ«
    - ContextManager: ä¸Šä¸‹æ–‡ç®¡ç†
    - ChatHandler: å¿«é€Ÿå¯¹è¯
    - ReportHandler: æ·±åº¦æŠ¥å‘Š
    - FollowupHandler: è¿½é—®å¤„ç†
    - AgentSupervisor: å¤š Agent è°ƒåº¦ (Phase 1 æ–°å¢)

    ä½¿ç”¨æ–¹å¼:
        agent = ConversationAgent()
        response = agent.chat("åˆ†æ AAPL")
    """

    def __init__(
        self,
        llm=None,
        orchestrator=None,
        report_agent=None,
        supervisor=None,
        max_context_turns: int = 10
    ):
        """
        åˆå§‹åŒ–å¯¹è¯ Agent

        Args:
            llm: LLM å®ä¾‹ï¼ˆç”¨äºå¢å¼ºå“åº”ï¼‰
            orchestrator: ToolOrchestrator å®ä¾‹
            report_agent: ç°æœ‰çš„æŠ¥å‘Šç”Ÿæˆ Agentï¼ˆlangchain_agentï¼‰
            supervisor: AgentSupervisor å®ä¾‹
            max_context_turns: æœ€å¤§ä¸Šä¸‹æ–‡è½®æ•°
        """
        self.llm = llm
        self.orchestrator = orchestrator
        self.report_agent = report_agent
        self.supervisor = supervisor

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.context = ContextManager(max_turns=max_context_turns)
        self.router = ConversationRouter(llm=llm)

        # åˆå§‹åŒ–å¤„ç†å™¨
        self.chat_handler = ChatHandler(llm=llm, orchestrator=orchestrator)
        self.report_handler = ReportHandler(
            agent=report_agent,
            orchestrator=orchestrator,
            llm=llm
        )
        self.followup_handler = FollowupHandler(llm=llm, orchestrator=orchestrator)

        # æ³¨å†Œå¤„ç†å™¨åˆ°è·¯ç”±å™¨
        self._register_handlers()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_queries': 0,
            'intents': {
                'chat': 0,
                'report': 0,
                'alert': 0,
                'followup': 0,
                'clarify': 0,
                'greeting': 0
            },
            'errors': 0,
            'session_start': datetime.now(),
        }

    def _register_handlers(self):
        """æ³¨å†Œæ„å›¾å¤„ç†å™¨"""
        self.router.register_handler(Intent.CHAT, self._handle_chat)
        self.router.register_handler(Intent.REPORT, self._handle_report)
        self.router.register_handler(Intent.ALERT, self._handle_alert)
        self.router.register_handler(Intent.FOLLOWUP, self._handle_followup)
        self.router.register_handler(Intent.CLARIFY, self._handle_clarify)
        self.router.register_handler(Intent.GREETING, self._handle_greeting)

    def chat(self, query: str, capture_thinking: bool = False) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆä¸»å…¥å£ï¼‰

        Args:
            query: ç”¨æˆ·è¾“å…¥
            capture_thinking: æ˜¯å¦æ•è·æ€è€ƒè¿‡ç¨‹

        Returns:
            åŒ…å«å“åº”å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        self.stats['total_queries'] += 1
        start_time = datetime.now()
        thinking_steps = [] if capture_thinking else None

        try:
            # 1. è§£ææŒ‡ä»£ï¼ˆå¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼‰
            if capture_thinking:
                thinking_steps.append({
                    "stage": "reference_resolution",
                    "message": "æ­£åœ¨è§£æä¸Šä¸‹æ–‡å¼•ç”¨...",
                    "timestamp": datetime.now().isoformat()
                })

            resolved_query = self.context.resolve_reference(query)

            # 2. è·¯ç”±åˆ°å¯¹åº”å¤„ç†å™¨
            if capture_thinking:
                thinking_steps.append({
                    "stage": "intent_classification",
                    "message": "æ­£åœ¨è¯†åˆ«æŸ¥è¯¢æ„å›¾...",
                    "timestamp": datetime.now().isoformat()
                })

            intent, metadata, handler = self.router.route(resolved_query, self.context)

            if capture_thinking:
                thinking_steps.append({
                    "stage": "intent_classification",
                    "result": {
                        "intent": intent.value,
                        "tickers": metadata.get('tickers', []),
                        "company_names": metadata.get('company_names', [])
                    },
                    "timestamp": datetime.now().isoformat()
                })

            # 3. æ›´æ–°ç»Ÿè®¡
            self.stats['intents'][intent.value] = self.stats['intents'].get(intent.value, 0) + 1

            # 4. æ•°æ®æ”¶é›†é˜¶æ®µï¼ˆå¦‚æœæœ‰è‚¡ç¥¨ä»£ç ï¼‰
            if capture_thinking and metadata.get('tickers'):
                ticker = metadata['tickers'][0]
                thinking_steps.append({
                    "stage": "data_collection",
                    "message": f"æ­£åœ¨è·å– {ticker} çš„æ•°æ®...",
                    "timestamp": datetime.now().isoformat()
                })

            # 5. è°ƒç”¨å¤„ç†å™¨
            if capture_thinking:
                thinking_steps.append({
                    "stage": "processing",
                    "message": f"æ­£åœ¨ç”Ÿæˆ{intent.value}å“åº”...",
                    "timestamp": datetime.now().isoformat()
                })

            if handler:
                result = handler(resolved_query, metadata)
            else:
                result = self._default_handler(resolved_query, metadata)

            if capture_thinking:
                thinking_steps.append({
                    "stage": "complete",
                    "message": "å¤„ç†å®Œæˆ",
                    "timestamp": datetime.now().isoformat()
                })

            # 6. æ›´æ–°ä¸Šä¸‹æ–‡
            self.context.add_turn(
                query=query,
                intent=intent.value,
                response=result.get('response', ''),
                metadata=metadata
            )

            # 7. è‡ªåŠ¨æ·»åŠ å›¾è¡¨æ ‡è®°ï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡å’ŒæŸ¥è¯¢ï¼‰
            # åªæœ‰ CHAT/REPORT æ„å›¾æ‰å°è¯•ç”Ÿæˆå›¾è¡¨ï¼Œé—²èŠä¸ç”Ÿæˆ
            if intent in [Intent.CHAT, Intent.REPORT, Intent.FOLLOWUP]:
                result = self._add_chart_marker(result, query, metadata, resolved_query)

            # 8. æ·»åŠ å…ƒä¿¡æ¯
            result['intent'] = intent.value
            result['metadata'] = metadata
            result['response_time_ms'] = (datetime.now() - start_time).total_seconds() * 1000
            result['thinking_elapsed_seconds'] = round((datetime.now() - start_time).total_seconds(), 2)
            result['current_focus'] = self.context.current_focus

            if capture_thinking and thinking_steps:
                result['thinking'] = thinking_steps

            return result

        except Exception as e:
            self.stats['errors'] += 1
            import traceback
            traceback.print_exc()
            error_result = {
                'success': False,
                'response': f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}",
                'error': str(e),
                'response_time_ms': (datetime.now() - start_time).total_seconds() * 1000,
                'thinking_elapsed_seconds': round((datetime.now() - start_time).total_seconds(), 2),
            }
            if capture_thinking and thinking_steps:
                error_result['thinking'] = thinking_steps
            return error_result


    async def chat_async(self, query: str, capture_thinking: bool = False) -> Dict[str, Any]:
        """
        Async version of chat() that can await Supervisor paths.
        """
        self.stats['total_queries'] += 1
        start_time = datetime.now()
        thinking_steps = [] if capture_thinking else None

        try:
            if capture_thinking:
                thinking_steps.append({
                    "stage": "reference_resolution",
                    "message": "æ­£åœ¨è§£æä¸Šä¸‹æ–‡å¼•ç”¨...",
                    "timestamp": datetime.now().isoformat()
                })

            resolved_query = self.context.resolve_reference(query)

            if capture_thinking:
                thinking_steps.append({
                    "stage": "intent_classification",
                    "message": "æ­£åœ¨è¯†åˆ«æŸ¥è¯¢æ„å›¾...",
                    "timestamp": datetime.now().isoformat()
                })

            intent, metadata, handler = self.router.route(resolved_query, self.context)

            if capture_thinking:
                thinking_steps.append({
                    "stage": "intent_classification",
                    "result": {
                        "intent": intent.value,
                        "tickers": metadata.get('tickers', []),
                        "company_names": metadata.get('company_names', [])
                    },
                    "timestamp": datetime.now().isoformat()
                })

            self.stats['intents'][intent.value] = self.stats['intents'].get(intent.value, 0) + 1

            if capture_thinking and metadata.get('tickers'):
                ticker = metadata['tickers'][0]
                thinking_steps.append({
                    "stage": "data_collection",
                    "message": f"æ­£åœ¨è·å– {ticker} çš„æ•°æ®...",
                    "timestamp": datetime.now().isoformat()
                })

            if capture_thinking:
                thinking_steps.append({
                    "stage": "processing",
                    "message": f"æ­£åœ¨ç”Ÿæˆ{intent.value}å“åº”...",
                    "timestamp": datetime.now().isoformat()
                })

            if intent == Intent.REPORT and self.supervisor and metadata.get('tickers'):
                result = await self._handle_report_async(resolved_query, metadata)
            elif handler:
                result = await asyncio.to_thread(handler, resolved_query, metadata)
            else:
                result = await asyncio.to_thread(self._default_handler, resolved_query, metadata)

            if capture_thinking:
                thinking_steps.append({
                    "stage": "complete",
                    "message": "å¤„ç†å®Œæˆ",
                    "timestamp": datetime.now().isoformat()
                })

            self.context.add_turn(
                query=query,
                intent=intent.value,
                response=result.get('response', ''),
                metadata=metadata
            )

            if intent in [Intent.CHAT, Intent.REPORT, Intent.FOLLOWUP]:
                result = self._add_chart_marker(result, query, metadata, resolved_query)

            result['intent'] = intent.value
            result['metadata'] = metadata
            result['response_time_ms'] = (datetime.now() - start_time).total_seconds() * 1000
            result['thinking_elapsed_seconds'] = round((datetime.now() - start_time).total_seconds(), 2)
            result['current_focus'] = self.context.current_focus

            if capture_thinking and thinking_steps:
                result['thinking'] = thinking_steps

            return result

        except Exception as e:
            self.stats['errors'] += 1
            import traceback
            traceback.print_exc()
            error_result = {
                'success': False,
                'response': f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}",
                'error': str(e),
                'response_time_ms': (datetime.now() - start_time).total_seconds() * 1000,
                'thinking_elapsed_seconds': round((datetime.now() - start_time).total_seconds(), 2),
            }
            if capture_thinking and thinking_steps:
                error_result['thinking'] = thinking_steps
            return error_result

    def _handle_chat(self, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å¿«é€Ÿå¯¹è¯"""
        if self.llm:
            return self.chat_handler.handle_with_llm(query, metadata, self.context)
        return self.chat_handler.handle(query, metadata, self.context)


    async def _handle_report_async(self, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Async report path that can await AgentSupervisor."""
        use_supervisor = bool(
            self.supervisor
            and metadata.get('tickers')
            and (self.report_agent is None or os.getenv("SUPERVISOR_REPORT_FORCE", "false").lower() in ("true", "1", "yes", "on"))
        )
        if use_supervisor:
            ticker = metadata['tickers'][0]
            try:
                analysis_result = await self.supervisor.analyze(query, ticker, user_profile=None)
                forum_output = analysis_result.get("forum_output")

                report_ir = None
                if forum_output and hasattr(self.report_handler, "_convert_to_report_ir"):
                    report_ir = self.report_handler._convert_to_report_ir(ticker, query, forum_output)
                elif forum_output and hasattr(self.report_handler, "_generate_simple_report_ir"):
                    report_ir = self.report_handler._generate_simple_report_ir(ticker, forum_output.consensus)

                response_text = forum_output.consensus if forum_output else ""
                result = {
                    'success': True,
                    'response': response_text,
                    'data': analysis_result,
                    'method': 'supervisor',
                }
                if report_ir:
                    result['report'] = report_ir
                return result
            except Exception as e:
                print(f"[Agent] Supervisor async call failed: {e}")

        return await asyncio.to_thread(self.report_handler.handle, query, metadata, self.context)

    def _handle_report(self, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æŠ¥å‘Šè¯·æ±‚ (ä¼˜å…ˆä½¿ç”¨ Supervisor)"""
        use_supervisor = bool(
            self.supervisor
            and metadata.get('tickers')
            and (self.report_agent is None or os.getenv("SUPERVISOR_REPORT_FORCE", "false").lower() in ("true", "1", "yes", "on"))
        )
        if use_supervisor:
            try:
                asyncio.get_running_loop()
            except RuntimeError:
                try:
                    return asyncio.run(self._handle_report_async(query, metadata))
                except Exception as e:
                    print(f"[Agent] Supervisor è°ƒç”¨å¤±è´¥: {e}")
            except Exception as e:
                print(f"[Agent] Supervisor è°ƒç”¨å¼‚å¸¸: {e}")

        result = self.report_handler.handle(query, metadata, self.context)
        print(f"[Agent._handle_report] report_handler è¿”å› - report å­˜åœ¨: {'report' in result}, å­—æ®µ: {list(result.keys())}")
        return result

    def _handle_alert(self, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†ç›‘æ§è¯·æ±‚ï¼ˆå¾…å®ç°ï¼‰"""
        tickers = metadata.get('tickers', [])
        ticker = tickers[0] if tickers else None
        if not ticker and self.context.current_focus:
            ticker = self.context.current_focus

        return {
            'success': True,
            'response': f"""ğŸ“Š ç›‘æ§åŠŸèƒ½è¯´æ˜

æ‚¨æƒ³ç›‘æ§ {ticker or 'æŸæ”¯è‚¡ç¥¨'} çš„ä»·æ ¼å˜åŠ¨ã€‚

ç›®å‰ç›‘æ§åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ï¼Œå³å°†æ”¯æŒï¼š
1. ä»·æ ¼çªç ´æé†’
2. æ¶¨è·Œå¹…æé†’
3. æˆäº¤é‡å¼‚å¸¸æé†’
4. æ–°é—»åŠ¨æ€æé†’

è¯·ç¨åå†è¯•ï¼Œæˆ–å…ˆä½¿ç”¨ä»·æ ¼æŸ¥è¯¢åŠŸèƒ½äº†è§£å½“å‰è¡Œæƒ…ã€‚""",
            'intent': 'alert',
            'feature_status': 'coming_soon',
        }

    def _handle_followup(self, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†è¿½é—®"""
        return self.followup_handler.handle(query, metadata, self.context)

    def _handle_greeting(self, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†é—®å€™å’Œæ—¥å¸¸é—²èŠ (ä¸è°ƒç”¨æœç´¢)"""
        if any(kw in query.lower() for kw in ['è‡ªæˆ‘ä»‹ç»', 'ä½ æ˜¯è°', 'introduce yourself', 'who are you', 'ä½ æ˜¯åš', 'ä½ æ˜¯å¹²']):
            response = """æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„**é‡‘èå¯¹è¯å¼åˆ†æ Agent**ï¼Œåå« FinSight AIã€‚

æˆ‘çš„ä¸»è¦å·¥ä½œæ˜¯å¸®åŠ©æ‚¨å¿«é€Ÿè·å–å’Œåˆ†æå…¨çƒè‚¡ç¥¨ã€æŒ‡æ•°ã€ETF ç­‰é‡‘èå¸‚åœºä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
1. **å®æ—¶è¡Œæƒ…æŸ¥è¯¢**ï¼šè‚¡ä»·ã€æ¶¨è·Œå¹…ã€K çº¿å›¾ç­‰ã€‚
2. **æ·±åº¦æŠ¥å‘Šç”Ÿæˆ**ï¼šå¯¹ç‰¹å®šè‚¡ç¥¨è¿›è¡ŒåŸºæœ¬é¢ã€è´¢åŠ¡ã€ä¼°å€¼ã€é£é™©åˆ†æã€‚
3. **è¡Œä¸šè¶‹åŠ¿æ´å¯Ÿ**ï¼šåˆ†æå¸‚åœºçƒ­ç‚¹å’Œè¡Œä¸šåŠ¨å‘ã€‚
4. **æŠ•èµ„å»ºè®®**ï¼šæ ¹æ®æ‚¨çš„éœ€æ±‚æä¾›ä¸­è‚¯çš„æŠ•èµ„å»ºè®®ã€‚

æ‚¨æœ‰ä»€ä¹ˆæƒ³äº†è§£çš„è‚¡ç¥¨ï¼ˆä¾‹å¦‚ï¼š**AAPL**ï¼‰æˆ–é‡‘èé—®é¢˜å—ï¼Ÿ"""
        else:
            response = "æ‚¨å¥½ï¼æˆ‘æ˜¯ FinSight AI é‡‘èåŠ©æ‰‹ã€‚æ‚¨ä»Šå¤©æƒ³äº†è§£å“ªæ”¯è‚¡ç¥¨çš„è¡Œæƒ…ï¼Œæˆ–è€…éœ€è¦ç”Ÿæˆå“ªå®¶å…¬å¸çš„åˆ†ææŠ¥å‘Šå—ï¼Ÿ"

        return {
            'success': True,
            'response': response,
            'intent': 'greeting',
        }

    def _handle_clarify(self, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†éœ€è¦æ¾„æ¸…çš„æŸ¥è¯¢"""
        clarify_reason = metadata.get("clarify_reason")
        if clarify_reason == "followup_without_context":
            response = """çœ‹èµ·æ¥ä½ æ˜¯åœ¨è¿½é—®ä¸Šä¸€æ¡å†…å®¹ï¼Œä½†æˆ‘è¿™è¾¹æ²¡æœ‰ä¸Šä¸‹æ–‡ã€‚å¯ä»¥å‘Šè¯‰æˆ‘ä½ å…·ä½“æƒ³è¿½é—®å“ªåªè‚¡ç¥¨/æŒ‡æ•°/è¡Œä¸šæˆ–å“ªæ¡æ–°é—»å—ï¼Ÿ

ä¾‹å¦‚ï¼š
1. "AAPL ä¸ºä»€ä¹ˆä»Šå¤©ä¸‹è·Œï¼Ÿ"
2. "æœ€è¿‘å¸‚åœºçƒ­ç‚¹æœ‰å“ªäº›ï¼Ÿ"
3. "è§£é‡Šä¸€ä¸‹åˆšæ‰çš„ç»“è®ºï¼šXXX"
"""
            return {
                'success': True,
                'response': response,
                'intent': 'clarify',
                'needs_clarification': True,
            }

        return {
            'success': True,
            'response': """æŠ±æ­‰ï¼Œæˆ‘ä¸å¤ªç¡®å®šæ‚¨æƒ³äº†è§£ä»€ä¹ˆã€‚

æˆ‘æ˜¯ä¸“æ³¨äº**é‡‘èå¸‚åœºåˆ†æ**çš„åŠ©æ‰‹ã€‚å¯¹äºéé‡‘èç±»é—®é¢˜ï¼ˆå¦‚ç¼–ç¨‹ä»£ç ã€å¤©æ°”ã€å…«å¦å¨±ä¹ç­‰ï¼‰ï¼Œæˆ‘å¯èƒ½æ— æ³•æä¾›å‡†ç¡®å¸®åŠ©ã€‚

æ‚¨å¯ä»¥å°è¯•é—®æˆ‘ï¼š
1. è‚¡ç¥¨è¡Œæƒ…ï¼š**"AAPL ç°åœ¨å¤šå°‘é’±ï¼Ÿ"**
2. å…¬å¸åˆ†æï¼š**"åˆ†æä¸€ä¸‹ç‰¹æ–¯æ‹‰"**
3. æŠ•èµ„å»ºè®®ï¼š**"ç°åœ¨ä¹°è‹±ä¼Ÿè¾¾æ€ä¹ˆæ ·ï¼Ÿ"**

è¯·æä¾›å…·ä½“çš„è‚¡ç¥¨ä»£ç æˆ–å…¬å¸åç§°ï¼Œé‡æ–°æé—®ï¼""",
            'intent': 'clarify',
            'needs_clarification': True,
        }

    def _default_handler(self, query: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """é»˜è®¤å¤„ç†å™¨"""
        return self._handle_chat(query, metadata)

    def _add_chart_marker(
        self,
        result: Dict[str, Any],
        original_query: str,
        metadata: Dict[str, Any],
        resolved_query: str
    ) -> Dict[str, Any]:
        """
        æ ¹æ®ä¸Šä¸‹æ–‡å’ŒæŸ¥è¯¢è‡ªåŠ¨æ·»åŠ å›¾è¡¨æ ‡è®°

        å›¾è¡¨æ ‡è®°æ ¼å¼: [CHART:TICKER:TYPE]
        """
        try:
            # é—®å€™/æ¾„æ¸…ç­‰éè¡Œæƒ…æ„å›¾ä¸è‡ªåŠ¨åŠ å›¾è¡¨
            intent = result.get('intent') or metadata.get('intent')
            if intent in {'greeting', 'clarify', 'followup', 'alert'}:
                return result

            from backend.api.chart_detector import ChartTypeDetector

            # ä»…ä½¿ç”¨æ˜¾å¼è§£æåˆ°çš„ tickerï¼Œé¿å…æ²¿ç”¨æ—§çš„ current_focus è¯¯åŠ å›¾è¡¨
            ticker = None
            if metadata.get('tickers'):
                ticker = metadata['tickers'][0]

            if not ticker:
                return result

            # æ£€æµ‹å›¾è¡¨ç±»å‹
            query_lower = resolved_query.lower()

            # ç‰¹æ®Šå¤„ç†ï¼šæŒä»“æƒ…å†µ -> é¥¼å›¾
            if any(kw in query_lower for kw in ['æŒä»“', 'æˆåˆ†', 'ç»„æˆ', 'å æ¯”', 'åˆ†å¸ƒ', 'holdings', 'constituent', 'composition']):
                chart_type = 'pie'
            # å¯¹æ¯”æŸ¥è¯¢ -> æŸ±çŠ¶å›¾
            elif any(kw in query_lower for kw in ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'vs', 'åŒºåˆ«', 'compare', 'difference']):
                chart_type = 'bar'
            # ä»·æ ¼/èµ°åŠ¿æŸ¥è¯¢ -> Kçº¿å›¾æˆ–æŠ˜çº¿å›¾
            elif any(kw in query_lower for kw in ['ä»·æ ¼', 'èµ°åŠ¿', 'è¶‹åŠ¿', 'kçº¿', 'æ¶¨è·Œ', 'è¡¨ç°', 'price', 'trend', 'chart']):
                chart_type = 'candlestick' if 'kçº¿' in query_lower or 'candlestick' in query_lower else 'line'
            # ä½¿ç”¨ ChartTypeDetector æ£€æµ‹
            else:
                chart_detection = ChartTypeDetector.detect_chart_type(resolved_query, ticker)
                chart_type = chart_detection.get('chart_type', 'line')

            # å¦‚æœæ£€æµ‹åˆ°éœ€è¦å›¾è¡¨ï¼Œæ·»åŠ æ ‡è®°
            if chart_type and ChartTypeDetector.should_generate_chart(resolved_query):
                chart_marker = f"[CHART:{ticker}:{chart_type}]"
                # åœ¨å“åº”æœ«å°¾æ·»åŠ å›¾è¡¨æ ‡è®°ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                if chart_marker not in result.get('response', ''):
                    result['response'] = result.get('response', '') + f'''

{chart_marker}'''
                    print(f"[Agent] è‡ªåŠ¨æ·»åŠ å›¾è¡¨æ ‡è®°: {chart_marker}")

        except Exception as e:
            print(f"[Agent] æ·»åŠ å›¾è¡¨æ ‡è®°å¤±è´¥: {e}")

        return result

    def get_context_summary(self) -> str:
        """è·å–å½“å‰ä¸Šä¸‹æ–‡æ‘˜è¦"""
        return self.context.get_summary()

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            'context_turns': len(self.context.history),
            'current_focus': self.context.current_focus,
            'session_duration_seconds': (datetime.now() - self.stats['session_start']).total_seconds(),
        }

    def reset(self) -> None:
        """é‡ç½®å¯¹è¯çŠ¶æ€"""
        self.context.clear()
        self.stats = {
            'total_queries': 0,
            'intents': {'chat': 0, 'report': 0, 'alert': 0, 'followup': 0, 'clarify': 0, 'greeting': 0},
            'errors': 0,
            'session_start': datetime.now(),
        }

    def set_focus(self, ticker: str, company_name: str = None) -> None:
        """æ‰‹åŠ¨è®¾ç½®å½“å‰ç„¦ç‚¹"""
        self.context.current_focus = ticker
        if company_name:
            self.context.current_focus_name = company_name

    def describe_report_agent(self) -> Dict[str, Any]:
        """
        è¯Šæ–­æŠ¥å‘Š Agentï¼ˆLangGraphï¼‰çŠ¶æ€ï¼Œä¾›å‰ç«¯æµæ°´çº¿é¢æ¿/å¥åº·æ£€æŸ¥ä½¿ç”¨ã€‚
        ä¸è§¦å‘å¤–éƒ¨ LLM è°ƒç”¨ã€‚
        """
        info: Dict[str, Any] = {"available": False}
        if not getattr(self, "report_agent", None):
            info["error"] = "report_agent_not_initialized"
            return info
        info["available"] = True
        # ä¼˜å…ˆä½¿ç”¨ self_checkï¼Œå…¶æ¬¡ get_agent_info
        if hasattr(self.report_agent, "self_check"):
            try:
                info["self_check"] = self.report_agent.self_check()
            except Exception as exc:  # pragma: no cover (è¯Šæ–­è·¯å¾„)
                info["self_check_error"] = str(exc)
        if hasattr(self.report_agent, "get_agent_info"):
            try:
                info["agent_info"] = self.report_agent.get_agent_info()
            except Exception as exc:  # pragma: no cover
                info["agent_info_error"] = str(exc)
        if hasattr(self.report_agent, "get_recent_trace"):
            try:
                info["recent_trace"] = self.report_agent.get_recent_trace(10)
            except Exception as exc:  # pragma: no cover
                info["recent_trace_error"] = str(exc)
        return info


# === ä¾¿æ·å‡½æ•° ===

def create_agent(
    use_llm: bool = False,
    use_orchestrator: bool = True,
    use_report_agent: bool = False
) -> ConversationAgent:
    """
    åˆ›å»º ConversationAgent å®ä¾‹

    Args:
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM å¢å¼º
        use_orchestrator: æ˜¯å¦ä½¿ç”¨ ToolOrchestrator
        use_report_agent: æ˜¯å¦ä½¿ç”¨ç°æœ‰çš„ LangChain Agent

    Returns:
        ConversationAgent å®ä¾‹
    """
    llm = None
    orchestrator = None
    report_agent = None
    supervisor = None

    # åˆå§‹åŒ– LLM
    if use_llm:
        try:
            # å°è¯•ä» backend.config å¯¼å…¥
            try:
                from backend.config import get_llm_config
            except ImportError:
                # å›é€€åˆ°æ ¹ç›®å½• config
                from config import get_llm_config

            llm_config = get_llm_config()

            # ä¼˜å…ˆå°è¯•ä½¿ç”¨ langchain_openai
            try:
                from langchain_openai import ChatOpenAI
                llm = ChatOpenAI(
                    model=llm_config.get('model', 'gpt-3.5-turbo'),
                    temperature=llm_config.get('temperature', 0.3),
                    openai_api_key=llm_config.get('api_key'),
                    openai_api_base=llm_config.get('api_base'),
                )
                print("[ConversationAgent] LLM åˆå§‹åŒ–æˆåŠŸ (langchain_openai)")
            except ImportError:
                # å¦‚æœ langchain_openai ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ litellm åˆ›å»ºå…¼å®¹çš„ ChatModel
                try:
                    import litellm
                    from langchain_core.language_models.chat_models import BaseChatModel
                    from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
                    from langchain_core.outputs import ChatGeneration, ChatResult
                    from langchain_core.callbacks.manager import CallbackManagerForLLMRun

                    class LiteLLMChatModel(BaseChatModel):
                        """LiteLLM ChatModel åŒ…è£…å™¨ï¼Œå…¼å®¹ LangChain ChatModel æ¥å£"""
                        api_key: str
                        api_base: Optional[str] = None
                        model: str = "gpt-3.5-turbo"
                        temperature: float = 0.3

                        @property
                        def _llm_type(self) -> str:
                            return "litellm"

                        def _generate(
                            self,
                            messages: List[BaseMessage],
                            stop: Optional[List[str]] = None,
                            run_manager: Optional[CallbackManagerForLLMRun] = None,
                            **kwargs: Any,
                        ) -> ChatResult:
                            # è½¬æ¢ LangChain messages ä¸º litellm æ ¼å¼
                            litellm_messages = []
                            for msg in messages:
                                if isinstance(msg, HumanMessage):
                                    litellm_messages.append({"role": "user", "content": msg.content})
                                elif isinstance(msg, AIMessage):
                                    litellm_messages.append({"role": "assistant", "content": msg.content})

                            response = litellm.completion(
                                model=f"openai/{self.model}",
                                messages=litellm_messages,
                                api_key=self.api_key,
                                api_base=self.api_base,
                                temperature=self.temperature,
                                **kwargs
                            )

                            content = response.choices[0].message.content
                            message = AIMessage(content=content)
                            generation = ChatGeneration(message=message)
                            return ChatResult(generations=[generation])

                        def _stream(self, messages, stop=None, run_manager=None, **kwargs):
                            # æµå¼è¾“å‡ºæš‚ä¸æ”¯æŒ
                            result = self._generate(messages, stop, run_manager, **kwargs)
                            yield result.generations[0].message

                    llm = LiteLLMChatModel(
                        api_key=llm_config.get('api_key'),
                        api_base=llm_config.get('api_base'),
                        model=llm_config.get('model', 'gpt-3.5-turbo'),
                        temperature=llm_config.get('temperature', 0.3),
                    )
                    print("[ConversationAgent] LLM åˆå§‹åŒ–æˆåŠŸ (litellm)")
                except (ImportError, Exception) as e:
                    print(f"[ConversationAgent] è­¦å‘Š: LLM åˆå§‹åŒ–å¤±è´¥ ({e})ï¼ŒLLM åŠŸèƒ½å°†ä¸å¯ç”¨")
                    llm = None

        except Exception as e:
            print(f"[ConversationAgent] åˆå§‹åŒ– LLM å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            llm = None

    # åˆå§‹åŒ– Orchestrator
    if use_orchestrator:
        try:
            from backend.orchestration.orchestrator import ToolOrchestrator
            from backend.orchestration.tools_bridge import register_all_financial_tools

            orchestrator = ToolOrchestrator()
            register_all_financial_tools(orchestrator)
        except Exception as e:
            print(f"[ConversationAgent] åˆå§‹åŒ– Orchestrator å¤±è´¥: {e}")

    # åˆå§‹åŒ– Report Agent
    if use_report_agent:
        try:
            from backend.langchain_agent import create_financial_agent
            report_agent = create_financial_agent()
            print("[ConversationAgent] Report Agent åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"[ConversationAgent] åˆå§‹åŒ– Report Agent å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    # åˆå§‹åŒ– Agent Supervisor (New in Phase 1)
    if llm and orchestrator:
        try:
            from backend.orchestration.supervisor import AgentSupervisor
            # éœ€è¦ä¼ å…¥ cache å’Œ circuit_breaker
            supervisor = AgentSupervisor(
                llm=llm,
                tools_module=orchestrator.tools_module, # Bridge æ³¨å†Œåçš„ module
                cache=orchestrator.cache,
                circuit_breaker=orchestrator.circuit_breaker
            )
            print("[ConversationAgent] Agent Supervisor åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"[ConversationAgent] åˆå§‹åŒ– Supervisor å¤±è´¥: {e}")

    return ConversationAgent(
        llm=llm,
        orchestrator=orchestrator,
        report_agent=report_agent,
        supervisor=supervisor
    )
