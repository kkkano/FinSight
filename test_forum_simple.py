"""
ç®€åŒ–çš„ Forum LLM æµ‹è¯•
"""
import asyncio
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç 
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

from dotenv import load_dotenv
load_dotenv()

async def test():
    print("=" * 60)
    print("Forum LLM ç®€åŒ–æµ‹è¯•")
    print("=" * 60)

    # 1. è¯»å–é…ç½®
    print("\n[1] è¯»å–é…ç½®...")
    from backend.llm_config import get_llm_config
    config = get_llm_config()
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    print(f"   - Provider: {config.get('provider')}")
    print(f"   - Model: {config.get('model')}")
    print(f"   - API Base: {config.get('api_base')}")
    print(f"   - API Key: {config.get('api_key')[:10]}...")

    # 2. åˆå§‹åŒ– LLM
    print("\n[2] åˆå§‹åŒ– LLM...")
    from langchain_openai import ChatOpenAI
    llm = ChatOpenAI(
        model=config["model"],
        api_key=config["api_key"],
        base_url=config.get("api_base"),
        temperature=config.get("temperature", 0.3)
    )
    print(f"âœ… LLM åˆå§‹åŒ–æˆåŠŸ: {type(llm).__name__}")

    # 3. æµ‹è¯•ç®€å•è°ƒç”¨
    print("\n[3] æµ‹è¯•ç®€å• LLM è°ƒç”¨...")
    from langchain_core.messages import HumanMessage
    try:
        response = await llm.ainvoke([HumanMessage(content="Say 'OK'")])
        print(f"âœ… LLM è°ƒç”¨æˆåŠŸ")
        print(f"   - Response: {response.content}")
    except Exception as e:
        print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # 4. æµ‹è¯• Forum
    print("\n[4] æµ‹è¯• ForumHost...")
    from backend.orchestration.forum import ForumHost
    from backend.agents.base_agent import AgentOutput

    forum = ForumHost(llm)

    mock_outputs = {
        "price": AgentOutput(
            agent_name="price",
            summary="AAPL å½“å‰ä»·æ ¼ $150.00ï¼Œä¸Šæ¶¨ 2.5%",
            confidence=0.9,
            evidence=[],
            data_sources=["mock"],
            as_of="2024-01-24"
        ),
        "news": AgentOutput(
            agent_name="news",
            summary="è‹¹æœå‘å¸ƒæ–°äº§å“ï¼Œå¸‚åœºååº”ç§¯æ",
            confidence=0.8,
            evidence=[],
            data_sources=["mock"],
            as_of="2024-01-24"
        )
    }

    print("   è°ƒç”¨ forum.synthesize()...")
    try:
        result = await forum.synthesize(mock_outputs)
        print(f"âœ… Forum è°ƒç”¨æˆåŠŸ")
        print(f"   - Consensus é•¿åº¦: {len(result.consensus)} å­—ç¬¦")
        print(f"   - Confidence: {result.confidence}")
        print(f"   - Recommendation: {result.recommendation}")

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† fallback
        if "### 1. ğŸ“Š æ‰§è¡Œæ‘˜è¦" in result.consensus and "HOLD (è§‚æœ›)" in result.consensus:
            print("\nâš ï¸  ä½¿ç”¨äº† fallback_synthesisï¼ˆLLM è°ƒç”¨å¤±è´¥ï¼‰")
            print("\nå‰ 500 å­—ç¬¦:")
            print(result.consensus[:500])
        else:
            print("\nâœ… ä½¿ç”¨äº† LLM ç”Ÿæˆçš„æŠ¥å‘Š")
            print("\nå‰ 500 å­—ç¬¦:")
            print(result.consensus[:500])

    except Exception as e:
        print(f"âŒ Forum è°ƒç”¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(test())
